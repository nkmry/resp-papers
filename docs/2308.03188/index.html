<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies</title>
<!--Generated on Thu Aug 10 19:11:16 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/dginev/ar5iv-css@0.7.6/css/ar5iv.min.css" type="text/css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Automatically Correcting Large Language Models: 
<br class="ltx_break"><span id="id1.id1" class="ltx_text ltx_font_italic">Surveying the landscape of diverse self-correction strategies</span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id2.1.id1" class="ltx_text ltx_font_bold">Liangming Pan,   Michael Saxon,   Wenda Xu,   
<br class="ltx_break">Deepak Nathani,   Xinyi Wang,   William Yang Wang 

<br class="ltx_break">University of California, Santa Barbara 

<br class="ltx_break"></span><span id="id3.2.id2" class="ltx_text ltx_font_typewriter">{liangmingpan, saxon, wendaxu, dnathani, xinyi_wang}@ucsb.edu</span><span id="id4.3.id3" class="ltx_text ltx_font_bold"> 
<br class="ltx_break"></span><span id="id5.4.id4" class="ltx_text ltx_font_typewriter">william@cs.ucsb.edu</span><span id="id6.5.id5" class="ltx_text ltx_font_bold">
</span>
</span></span>
</div>

<div id="table_of_contents" class="ltx_section">
    <ol>
        <li>
            <a href="#S1" class="ltx_ref ltx_ref_section">Introduction</a>
        </li>
        <li>
            <a href="#S2" class="ltx_ref ltx_ref_section">A Taxonomy for Correcting LLMs with Automated Feedback</a>
        </li>
        <li>
            <a href="#S3" class="ltx_ref ltx_ref_section">Training-Time Correction</a>
        </li>
        <li>
            <a href="#S4" class="ltx_ref ltx_ref_section">Generation-Time Correction</a>
        </li>
        <li>
            <a href="#S5" class="ltx_ref ltx_ref_section">Post-hoc Correction</a>
        </li>
        <li>
            <a href="#S6" class="ltx_ref ltx_ref_section">Applications</a>
        </li>
        <li>
            <a href="#S7" class="ltx_ref ltx_ref_section">Research Gaps and Future Directions</a>
        </li>
        <li>
            <a href="#S8" class="ltx_ref ltx_ref_section">Conclusion</a>
        </li>
        <li>
            <a href="#Sx1" class="ltx_ref ltx_ref_section">Acknowledgments</a>
        </li>
        <li>
            <a href="#bib" class="ltx_ref ltx_ref_section">References</a>
        </li>
    </ol>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.id1" class="ltx_p">Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks. However, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. A promising approach to rectify these flaws is <span id="id7.id1.1" class="ltx_text ltx_font_italic">self-correction</span>,
where the LLM itself is prompted or guided to fix problems in its own output.
Techniques leveraging <span id="id7.id1.2" class="ltx_text ltx_font_italic">automated feedback</span>—either produced by the LLM itself or some external system—are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback.
This paper presents a comprehensive review of this emerging class of techniques.
We analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. We also summarize the major applications of this strategy and conclude by discussing future directions and challenges.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recent years have seen striking empirical successes of large language models (LLMs), as they consistently obtain impressive results across a diverse range of NLP benchmarks <cite class="ltx_cite ltx_citemacro_cite">Guo et al. (<a href="#bib.bib39" title="" class="ltx_ref">2023</a>); Suzgun et al. (<a href="#bib.bib108" title="" class="ltx_ref">2023</a>); Qin et al. (<a href="#bib.bib93" title="" class="ltx_ref">2023</a>)</cite>, while also showcasing surprising abilities of language understanding <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a href="#bib.bib119" title="" class="ltx_ref">2022a</a>); Begus et al. (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite>, generation <cite class="ltx_cite ltx_citemacro_cite">Pu and Demberg (<a href="#bib.bib92" title="" class="ltx_ref">2023</a>); Lin and Chen (<a href="#bib.bib72" title="" class="ltx_ref">2023</a>); Lyu et al. (<a href="#bib.bib77" title="" class="ltx_ref">2023a</a>)</cite>, and reasoning <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a href="#bib.bib120" title="" class="ltx_ref">2022b</a>); Kojima et al. (<a href="#bib.bib56" title="" class="ltx_ref">2022</a>); Dasgupta et al. (<a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite>. However, these models are not without their flaws. LLMs are observed to intermittently display undesired and inconsistent behaviors such as producing seemingly convincing but inaccurate “hallucinations” <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib71" title="" class="ltx_ref">2022</a>); Zhang et al. (<a href="#bib.bib143" title="" class="ltx_ref">2023c</a>); Min et al. (<a href="#bib.bib81" title="" class="ltx_ref">2023</a>)</cite>, conducting unfaithful reasoning <cite class="ltx_cite ltx_citemacro_cite">Golovneva et al. (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>); Lyu et al. (<a href="#bib.bib78" title="" class="ltx_ref">2023b</a>); Wu et al. (<a href="#bib.bib125" title="" class="ltx_ref">2023b</a>)</cite>, generating inappropriate or harmful content <cite class="ltx_cite ltx_citemacro_cite">Gehman et al. (<a href="#bib.bib33" title="" class="ltx_ref">2020</a>); Levy et al. (<a href="#bib.bib63" title="" class="ltx_ref">2021</a>, <a href="#bib.bib62" title="" class="ltx_ref">2022</a>); Shaikh et al. (<a href="#bib.bib102" title="" class="ltx_ref">2023</a>)</cite>, and failing to trustfully follow rules and constraints <cite class="ltx_cite ltx_citemacro_cite">Zhuo et al. (<a href="#bib.bib146" title="" class="ltx_ref">2023</a>); Wang et al. (<a href="#bib.bib114" title="" class="ltx_ref">2023a</a>)</cite>. Such flawed behaviors hamper the trust in LLMs and pose hurdles to their real-world applications <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib86" title="" class="ltx_ref">2023</a>)</cite>.
</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">A prevailing strategy to rectify these undesired behaviors of LLMs is <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">learning from feedback</span>, mirroring a typical human learning strategy where individuals actively refine their behaviors through a cycle of trial, error, and correction. Humans, when making mistakes, often gather feedback either from others or through self-reflection. Such feedback offers valuable insights into missteps and proposes potential avenues for improvement. With feedback, humans can adapt and modify their behavior accordingly, learning to correct their mistakes over time.
Inspired by this natural learning mechanism, extensive research <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a href="#bib.bib45" title="" class="ltx_ref">2022</a>); Madaan et al. (<a href="#bib.bib79" title="" class="ltx_ref">2023</a>); Gero et al. (<a href="#bib.bib34" title="" class="ltx_ref">2023</a>); Jiang et al. (<a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite> has been undertaken to improve LLMs through the paradigm of learning from feedback.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">One popular line of research involves the use of <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">human feedback</span> to evaluate and refine models, as encapsulated in the survey by  <cite class="ltx_cite ltx_citemacro_citet">Fernandes et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite>. These methods typically involve direct optimization of LLMs against human feedback on their outputs <cite class="ltx_cite ltx_citemacro_cite">Kreutzer et al. (<a href="#bib.bib58" title="" class="ltx_ref">2018</a>); Glaese et al. (<a href="#bib.bib35" title="" class="ltx_ref">2022</a>); Ouyang et al. (<a href="#bib.bib87" title="" class="ltx_ref">2022</a>); Scheurer et al. (<a href="#bib.bib98" title="" class="ltx_ref">2023</a>)</cite>, wherein human evaluations of output quality serve as a reward signal for improving the model performance. However, this approach has two primary drawbacks: it can be costly due to the manual labor involved, and it lacks real-time capabilities as humans cannot provide instant feedback.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="872" height="375" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A conceptual framework for self-correcting LLMs with automated feedback. We identify three parties involved in the prototypical correction pipeline that are analogous to a patient, doctor, and treatment in medicine, respectively: a <span id="S1.F1.9.1" class="ltx_text ltx_font_italic">Language Model</span> produces initial output, a <span id="S1.F1.10.2" class="ltx_text ltx_font_italic">Critic Model</span> analyzes the output and provides feedback, and a <span id="S1.F1.11.3" class="ltx_text ltx_font_italic">Refine Model</span> provides treatment to either the output or the language model. We taxonomize existing works using this conceptualization along five key aspects: the <span id="S1.F1.12.4" class="ltx_text ltx_font_italic">problem</span> to be corrected, the <span id="S1.F1.13.5" class="ltx_text ltx_font_italic">source</span> and <span id="S1.F1.14.6" class="ltx_text ltx_font_italic">format</span> of the feedback, and the <span id="S1.F1.15.7" class="ltx_text ltx_font_italic">strategy</span> and <span id="S1.F1.16.8" class="ltx_text ltx_font_italic">learning</span> method of the refine model.</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To minimize the need for human intervention, another strategy is <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">self-correcting LLMs with automated feedback</span>, where the model (iteratively) learns from automatically generated feedback signals to understand the consequences of its actions and adapts its behaviors.
The source of automated feedback can be multifaceted, spanning from the LLM itself acting as the feedback model <cite class="ltx_cite ltx_citemacro_cite">Madaan et al. (<a href="#bib.bib79" title="" class="ltx_ref">2023</a>); Schick et al. (<a href="#bib.bib99" title="" class="ltx_ref">2023</a>)</cite>, a separately trained feedback model <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib134" title="" class="ltx_ref">2022b</a>); Paul et al. (<a href="#bib.bib90" title="" class="ltx_ref">2023</a>)</cite>, readily available external tools <cite class="ltx_cite ltx_citemacro_cite">Gou et al. (<a href="#bib.bib38" title="" class="ltx_ref">2023</a>); Chen et al. (<a href="#bib.bib13" title="" class="ltx_ref">2023c</a>)</cite>, to external knowledge sources such as Wikipedia or the internet <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib139" title="" class="ltx_ref">2023</a>); Li et al. (<a href="#bib.bib65" title="" class="ltx_ref">2023b</a>)</cite>. Different strategies have been proposed to correct LLM with automated feedback, including self-training <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a href="#bib.bib45" title="" class="ltx_ref">2022</a>); Bai et al. (<a href="#bib.bib4" title="" class="ltx_ref">2022b</a>)</cite>, generate-then-rank <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>); Weng et al. (<a href="#bib.bib122" title="" class="ltx_ref">2023</a>)</cite>, feedback-guided decoding <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib132" title="" class="ltx_ref">2022a</a>); Xie et al. (<a href="#bib.bib126" title="" class="ltx_ref">2023</a>)</cite>, iterative post-hoc revision <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib141" title="" class="ltx_ref">2023a</a>); Jiang et al. (<a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite>, etc. Recently, the incorporation of such strategies has demonstrated their effectiveness across a myriad of tasks, from question answering <cite class="ltx_cite ltx_citemacro_cite">Peng et al. (<a href="#bib.bib91" title="" class="ltx_ref">2023</a>)</cite> and reasoning <cite class="ltx_cite ltx_citemacro_cite">Pan et al. (<a href="#bib.bib88" title="" class="ltx_ref">2023</a>)</cite> to code generation <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib142" title="" class="ltx_ref">2023b</a>)</cite> and toxicity detection <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib76" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In light of these advancements, our paper aims to provide a comprehensive survey. We start by establishing the concept of <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">self-correcting LLMs with automated feedback</span> and creating a taxonomy of the different methods (§ <a href="#S2" title="2 A Taxonomy for Correcting LLMs with Automated Feedback ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). We then discuss the major techniques, categorized as training-time correction (§ <a href="#S3" title="3 Training-Time Correction ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), generation-time correction (§ <a href="#S4" title="4 Generation-Time Correction ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), and post-hoc correction (§ <a href="#S5" title="5 Post-hoc Correction ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>).
We then summarize the major application areas of this strategy (§ <a href="#S6" title="6 Applications ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). Finally, we discuss key future directions (§ <a href="#S7" title="7 Research Gaps and Future Directions ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>).
</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>A Taxonomy for Correcting LLMs with Automated Feedback</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">For the sake of clean exposition, we first present a conceptual framework outlining the overall process of correcting LLMs with feedback, thereby establishing the scope of this survey (§ <a href="#S2.SS1" title="2.1 Conceptual Framework ‣ 2 A Taxonomy for Correcting LLMs with Automated Feedback ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>).
We then proceed to identify five primary dimensions that serve as classification criteria for existing works: 1) What gets corrected, 2) What is the source of the feedback, 3) What is the format of the feedback, 4) When the feedback is used, and 5) How to correct the model with feedback (§ <a href="#S2.SS2" title="2.2 What gets corrected? ‣ 2 A Taxonomy for Correcting LLMs with Automated Feedback ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>–§ <a href="#S2.SS6" title="2.6 How to correct the model with feedback? ‣ 2 A Taxonomy for Correcting LLMs with Automated Feedback ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.6</span></a>). Finally, we summarize existing works in § <a href="#S2.SS7" title="2.7 Summary of existing works ‣ 2 A Taxonomy for Correcting LLMs with Automated Feedback ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.7</span></a>.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Conceptual Framework</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">We formulate the general process of correcting LLMs with automated feedback in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, using an analogy of medical treatment in our daily life. Three parties are involved in this process:</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.9" class="ltx_p"><math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mo id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><ci id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.1.m1.1d">∙</annotation></semantics></math> <span id="S2.SS1.p2.9.1" class="ltx_text ltx_font_bold">Language Model</span> <span id="S2.SS1.p2.9.2" class="ltx_text ltx_font_italic">(Patient)</span>. A language model <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{M}:\mathcal{X}\rightarrow\mathcal{Y}" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mrow id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p2.2.m2.1.1.2" xref="S2.SS1.p2.2.m2.1.1.2.cmml">ℳ</mi><mo lspace="0.278em" rspace="0.278em" id="S2.SS1.p2.2.m2.1.1.1" xref="S2.SS1.p2.2.m2.1.1.1.cmml">:</mo><mrow id="S2.SS1.p2.2.m2.1.1.3" xref="S2.SS1.p2.2.m2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p2.2.m2.1.1.3.2" xref="S2.SS1.p2.2.m2.1.1.3.2.cmml">𝒳</mi><mo stretchy="false" id="S2.SS1.p2.2.m2.1.1.3.1" xref="S2.SS1.p2.2.m2.1.1.3.1.cmml">→</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p2.2.m2.1.1.3.3" xref="S2.SS1.p2.2.m2.1.1.3.3.cmml">𝒴</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><apply id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1"><ci id="S2.SS1.p2.2.m2.1.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1.1">:</ci><ci id="S2.SS1.p2.2.m2.1.1.2.cmml" xref="S2.SS1.p2.2.m2.1.1.2">ℳ</ci><apply id="S2.SS1.p2.2.m2.1.1.3.cmml" xref="S2.SS1.p2.2.m2.1.1.3"><ci id="S2.SS1.p2.2.m2.1.1.3.1.cmml" xref="S2.SS1.p2.2.m2.1.1.3.1">→</ci><ci id="S2.SS1.p2.2.m2.1.1.3.2.cmml" xref="S2.SS1.p2.2.m2.1.1.3.2">𝒳</ci><ci id="S2.SS1.p2.2.m2.1.1.3.3.cmml" xref="S2.SS1.p2.2.m2.1.1.3.3">𝒴</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">\mathcal{M}:\mathcal{X}\rightarrow\mathcal{Y}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.2.m2.1d">caligraphic_M : caligraphic_X → caligraphic_Y</annotation></semantics></math> performs a specific task by mapping an input <math id="S2.SS1.p2.3.m3.1" class="ltx_Math" alttext="x\in\mathcal{X}" display="inline"><semantics id="S2.SS1.p2.3.m3.1a"><mrow id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml"><mi id="S2.SS1.p2.3.m3.1.1.2" xref="S2.SS1.p2.3.m3.1.1.2.cmml">x</mi><mo id="S2.SS1.p2.3.m3.1.1.1" xref="S2.SS1.p2.3.m3.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p2.3.m3.1.1.3" xref="S2.SS1.p2.3.m3.1.1.3.cmml">𝒳</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><apply id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1"><in id="S2.SS1.p2.3.m3.1.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1.1"></in><ci id="S2.SS1.p2.3.m3.1.1.2.cmml" xref="S2.SS1.p2.3.m3.1.1.2">𝑥</ci><ci id="S2.SS1.p2.3.m3.1.1.3.cmml" xref="S2.SS1.p2.3.m3.1.1.3">𝒳</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">x\in\mathcal{X}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.3.m3.1d">italic_x ∈ caligraphic_X</annotation></semantics></math> to an output text <math id="S2.SS1.p2.4.m4.1" class="ltx_Math" alttext="\hat{y}\in\mathcal{Y}" display="inline"><semantics id="S2.SS1.p2.4.m4.1a"><mrow id="S2.SS1.p2.4.m4.1.1" xref="S2.SS1.p2.4.m4.1.1.cmml"><mover accent="true" id="S2.SS1.p2.4.m4.1.1.2" xref="S2.SS1.p2.4.m4.1.1.2.cmml"><mi id="S2.SS1.p2.4.m4.1.1.2.2" xref="S2.SS1.p2.4.m4.1.1.2.2.cmml">y</mi><mo id="S2.SS1.p2.4.m4.1.1.2.1" xref="S2.SS1.p2.4.m4.1.1.2.1.cmml">^</mo></mover><mo id="S2.SS1.p2.4.m4.1.1.1" xref="S2.SS1.p2.4.m4.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p2.4.m4.1.1.3" xref="S2.SS1.p2.4.m4.1.1.3.cmml">𝒴</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.1b"><apply id="S2.SS1.p2.4.m4.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1"><in id="S2.SS1.p2.4.m4.1.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1.1"></in><apply id="S2.SS1.p2.4.m4.1.1.2.cmml" xref="S2.SS1.p2.4.m4.1.1.2"><ci id="S2.SS1.p2.4.m4.1.1.2.1.cmml" xref="S2.SS1.p2.4.m4.1.1.2.1">^</ci><ci id="S2.SS1.p2.4.m4.1.1.2.2.cmml" xref="S2.SS1.p2.4.m4.1.1.2.2">𝑦</ci></apply><ci id="S2.SS1.p2.4.m4.1.1.3.cmml" xref="S2.SS1.p2.4.m4.1.1.3">𝒴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.1c">\hat{y}\in\mathcal{Y}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.4.m4.1d">over^ start_ARG italic_y end_ARG ∈ caligraphic_Y</annotation></semantics></math>. This formulation encompasses a wide range of NLP tasks, for example, in summarization, <math id="S2.SS1.p2.5.m5.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.SS1.p2.5.m5.1a"><mi id="S2.SS1.p2.5.m5.1.1" xref="S2.SS1.p2.5.m5.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.m5.1b"><ci id="S2.SS1.p2.5.m5.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.5.m5.1c">x</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.5.m5.1d">italic_x</annotation></semantics></math> is a passage, <math id="S2.SS1.p2.6.m6.1" class="ltx_Math" alttext="\hat{y}" display="inline"><semantics id="S2.SS1.p2.6.m6.1a"><mover accent="true" id="S2.SS1.p2.6.m6.1.1" xref="S2.SS1.p2.6.m6.1.1.cmml"><mi id="S2.SS1.p2.6.m6.1.1.2" xref="S2.SS1.p2.6.m6.1.1.2.cmml">y</mi><mo id="S2.SS1.p2.6.m6.1.1.1" xref="S2.SS1.p2.6.m6.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.6.m6.1b"><apply id="S2.SS1.p2.6.m6.1.1.cmml" xref="S2.SS1.p2.6.m6.1.1"><ci id="S2.SS1.p2.6.m6.1.1.1.cmml" xref="S2.SS1.p2.6.m6.1.1.1">^</ci><ci id="S2.SS1.p2.6.m6.1.1.2.cmml" xref="S2.SS1.p2.6.m6.1.1.2">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.6.m6.1c">\hat{y}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.6.m6.1d">over^ start_ARG italic_y end_ARG</annotation></semantics></math> is the generated summary; for question-answering, <math id="S2.SS1.p2.7.m7.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.SS1.p2.7.m7.1a"><mi id="S2.SS1.p2.7.m7.1.1" xref="S2.SS1.p2.7.m7.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.7.m7.1b"><ci id="S2.SS1.p2.7.m7.1.1.cmml" xref="S2.SS1.p2.7.m7.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.7.m7.1c">x</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.7.m7.1d">italic_x</annotation></semantics></math> is a question and <math id="S2.SS1.p2.8.m8.1" class="ltx_Math" alttext="\hat{y}" display="inline"><semantics id="S2.SS1.p2.8.m8.1a"><mover accent="true" id="S2.SS1.p2.8.m8.1.1" xref="S2.SS1.p2.8.m8.1.1.cmml"><mi id="S2.SS1.p2.8.m8.1.1.2" xref="S2.SS1.p2.8.m8.1.1.2.cmml">y</mi><mo id="S2.SS1.p2.8.m8.1.1.1" xref="S2.SS1.p2.8.m8.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.8.m8.1b"><apply id="S2.SS1.p2.8.m8.1.1.cmml" xref="S2.SS1.p2.8.m8.1.1"><ci id="S2.SS1.p2.8.m8.1.1.1.cmml" xref="S2.SS1.p2.8.m8.1.1.1">^</ci><ci id="S2.SS1.p2.8.m8.1.1.2.cmml" xref="S2.SS1.p2.8.m8.1.1.2">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.8.m8.1c">\hat{y}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.8.m8.1d">over^ start_ARG italic_y end_ARG</annotation></semantics></math> is the predicted answer. The initial generation <math id="S2.SS1.p2.9.m9.1" class="ltx_Math" alttext="\hat{y}" display="inline"><semantics id="S2.SS1.p2.9.m9.1a"><mover accent="true" id="S2.SS1.p2.9.m9.1.1" xref="S2.SS1.p2.9.m9.1.1.cmml"><mi id="S2.SS1.p2.9.m9.1.1.2" xref="S2.SS1.p2.9.m9.1.1.2.cmml">y</mi><mo id="S2.SS1.p2.9.m9.1.1.1" xref="S2.SS1.p2.9.m9.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.9.m9.1b"><apply id="S2.SS1.p2.9.m9.1.1.cmml" xref="S2.SS1.p2.9.m9.1.1"><ci id="S2.SS1.p2.9.m9.1.1.1.cmml" xref="S2.SS1.p2.9.m9.1.1.1">^</ci><ci id="S2.SS1.p2.9.m9.1.1.2.cmml" xref="S2.SS1.p2.9.m9.1.1.2">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.9.m9.1c">\hat{y}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.9.m9.1d">over^ start_ARG italic_y end_ARG</annotation></semantics></math> may be imperfect and suffer from various problems such as hallucination and incorrect reasoning.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.p3.6" class="ltx_p"><math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><mo id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><ci id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p3.1.m1.1d">∙</annotation></semantics></math> <span id="S2.SS1.p3.6.1" class="ltx_text ltx_font_bold">Critic Model</span> <span id="S2.SS1.p3.6.2" class="ltx_text ltx_font_italic">(Doctor &amp; Diagnosis)</span>. A critic model <math id="S2.SS1.p3.2.m2.1" class="ltx_Math" alttext="\mathcal{C}:\mathcal{X}\times\mathcal{Y}\rightarrow\mathcal{F}" display="inline"><semantics id="S2.SS1.p3.2.m2.1a"><mrow id="S2.SS1.p3.2.m2.1.1" xref="S2.SS1.p3.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p3.2.m2.1.1.2" xref="S2.SS1.p3.2.m2.1.1.2.cmml">𝒞</mi><mo lspace="0.278em" rspace="0.278em" id="S2.SS1.p3.2.m2.1.1.1" xref="S2.SS1.p3.2.m2.1.1.1.cmml">:</mo><mrow id="S2.SS1.p3.2.m2.1.1.3" xref="S2.SS1.p3.2.m2.1.1.3.cmml"><mrow id="S2.SS1.p3.2.m2.1.1.3.2" xref="S2.SS1.p3.2.m2.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p3.2.m2.1.1.3.2.2" xref="S2.SS1.p3.2.m2.1.1.3.2.2.cmml">𝒳</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p3.2.m2.1.1.3.2.1" xref="S2.SS1.p3.2.m2.1.1.3.2.1.cmml">×</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p3.2.m2.1.1.3.2.3" xref="S2.SS1.p3.2.m2.1.1.3.2.3.cmml">𝒴</mi></mrow><mo stretchy="false" id="S2.SS1.p3.2.m2.1.1.3.1" xref="S2.SS1.p3.2.m2.1.1.3.1.cmml">→</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p3.2.m2.1.1.3.3" xref="S2.SS1.p3.2.m2.1.1.3.3.cmml">ℱ</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.2.m2.1b"><apply id="S2.SS1.p3.2.m2.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1"><ci id="S2.SS1.p3.2.m2.1.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1.1">:</ci><ci id="S2.SS1.p3.2.m2.1.1.2.cmml" xref="S2.SS1.p3.2.m2.1.1.2">𝒞</ci><apply id="S2.SS1.p3.2.m2.1.1.3.cmml" xref="S2.SS1.p3.2.m2.1.1.3"><ci id="S2.SS1.p3.2.m2.1.1.3.1.cmml" xref="S2.SS1.p3.2.m2.1.1.3.1">→</ci><apply id="S2.SS1.p3.2.m2.1.1.3.2.cmml" xref="S2.SS1.p3.2.m2.1.1.3.2"><times id="S2.SS1.p3.2.m2.1.1.3.2.1.cmml" xref="S2.SS1.p3.2.m2.1.1.3.2.1"></times><ci id="S2.SS1.p3.2.m2.1.1.3.2.2.cmml" xref="S2.SS1.p3.2.m2.1.1.3.2.2">𝒳</ci><ci id="S2.SS1.p3.2.m2.1.1.3.2.3.cmml" xref="S2.SS1.p3.2.m2.1.1.3.2.3">𝒴</ci></apply><ci id="S2.SS1.p3.2.m2.1.1.3.3.cmml" xref="S2.SS1.p3.2.m2.1.1.3.3">ℱ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.2.m2.1c">\mathcal{C}:\mathcal{X}\times\mathcal{Y}\rightarrow\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p3.2.m2.1d">caligraphic_C : caligraphic_X × caligraphic_Y → caligraphic_F</annotation></semantics></math> learns to generate feedback <math id="S2.SS1.p3.3.m3.2" class="ltx_Math" alttext="x,\hat{y}\rightarrow c" display="inline"><semantics id="S2.SS1.p3.3.m3.2a"><mrow id="S2.SS1.p3.3.m3.2.3" xref="S2.SS1.p3.3.m3.2.3.cmml"><mrow id="S2.SS1.p3.3.m3.2.3.2.2" xref="S2.SS1.p3.3.m3.2.3.2.1.cmml"><mi id="S2.SS1.p3.3.m3.1.1" xref="S2.SS1.p3.3.m3.1.1.cmml">x</mi><mo id="S2.SS1.p3.3.m3.2.3.2.2.1" xref="S2.SS1.p3.3.m3.2.3.2.1.cmml">,</mo><mover accent="true" id="S2.SS1.p3.3.m3.2.2" xref="S2.SS1.p3.3.m3.2.2.cmml"><mi id="S2.SS1.p3.3.m3.2.2.2" xref="S2.SS1.p3.3.m3.2.2.2.cmml">y</mi><mo id="S2.SS1.p3.3.m3.2.2.1" xref="S2.SS1.p3.3.m3.2.2.1.cmml">^</mo></mover></mrow><mo stretchy="false" id="S2.SS1.p3.3.m3.2.3.1" xref="S2.SS1.p3.3.m3.2.3.1.cmml">→</mo><mi id="S2.SS1.p3.3.m3.2.3.3" xref="S2.SS1.p3.3.m3.2.3.3.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.3.m3.2b"><apply id="S2.SS1.p3.3.m3.2.3.cmml" xref="S2.SS1.p3.3.m3.2.3"><ci id="S2.SS1.p3.3.m3.2.3.1.cmml" xref="S2.SS1.p3.3.m3.2.3.1">→</ci><list id="S2.SS1.p3.3.m3.2.3.2.1.cmml" xref="S2.SS1.p3.3.m3.2.3.2.2"><ci id="S2.SS1.p3.3.m3.1.1.cmml" xref="S2.SS1.p3.3.m3.1.1">𝑥</ci><apply id="S2.SS1.p3.3.m3.2.2.cmml" xref="S2.SS1.p3.3.m3.2.2"><ci id="S2.SS1.p3.3.m3.2.2.1.cmml" xref="S2.SS1.p3.3.m3.2.2.1">^</ci><ci id="S2.SS1.p3.3.m3.2.2.2.cmml" xref="S2.SS1.p3.3.m3.2.2.2">𝑦</ci></apply></list><ci id="S2.SS1.p3.3.m3.2.3.3.cmml" xref="S2.SS1.p3.3.m3.2.3.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.3.m3.2c">x,\hat{y}\rightarrow c</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p3.3.m3.2d">italic_x , over^ start_ARG italic_y end_ARG → italic_c</annotation></semantics></math> where <math id="S2.SS1.p3.4.m4.1" class="ltx_Math" alttext="\hat{y}\sim\mathcal{M}(x)" display="inline"><semantics id="S2.SS1.p3.4.m4.1a"><mrow id="S2.SS1.p3.4.m4.1.2" xref="S2.SS1.p3.4.m4.1.2.cmml"><mover accent="true" id="S2.SS1.p3.4.m4.1.2.2" xref="S2.SS1.p3.4.m4.1.2.2.cmml"><mi id="S2.SS1.p3.4.m4.1.2.2.2" xref="S2.SS1.p3.4.m4.1.2.2.2.cmml">y</mi><mo id="S2.SS1.p3.4.m4.1.2.2.1" xref="S2.SS1.p3.4.m4.1.2.2.1.cmml">^</mo></mover><mo id="S2.SS1.p3.4.m4.1.2.1" xref="S2.SS1.p3.4.m4.1.2.1.cmml">∼</mo><mrow id="S2.SS1.p3.4.m4.1.2.3" xref="S2.SS1.p3.4.m4.1.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p3.4.m4.1.2.3.2" xref="S2.SS1.p3.4.m4.1.2.3.2.cmml">ℳ</mi><mo id="S2.SS1.p3.4.m4.1.2.3.1" xref="S2.SS1.p3.4.m4.1.2.3.1.cmml">⁢</mo><mrow id="S2.SS1.p3.4.m4.1.2.3.3.2" xref="S2.SS1.p3.4.m4.1.2.3.cmml"><mo stretchy="false" id="S2.SS1.p3.4.m4.1.2.3.3.2.1" xref="S2.SS1.p3.4.m4.1.2.3.cmml">(</mo><mi id="S2.SS1.p3.4.m4.1.1" xref="S2.SS1.p3.4.m4.1.1.cmml">x</mi><mo stretchy="false" id="S2.SS1.p3.4.m4.1.2.3.3.2.2" xref="S2.SS1.p3.4.m4.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.4.m4.1b"><apply id="S2.SS1.p3.4.m4.1.2.cmml" xref="S2.SS1.p3.4.m4.1.2"><csymbol cd="latexml" id="S2.SS1.p3.4.m4.1.2.1.cmml" xref="S2.SS1.p3.4.m4.1.2.1">similar-to</csymbol><apply id="S2.SS1.p3.4.m4.1.2.2.cmml" xref="S2.SS1.p3.4.m4.1.2.2"><ci id="S2.SS1.p3.4.m4.1.2.2.1.cmml" xref="S2.SS1.p3.4.m4.1.2.2.1">^</ci><ci id="S2.SS1.p3.4.m4.1.2.2.2.cmml" xref="S2.SS1.p3.4.m4.1.2.2.2">𝑦</ci></apply><apply id="S2.SS1.p3.4.m4.1.2.3.cmml" xref="S2.SS1.p3.4.m4.1.2.3"><times id="S2.SS1.p3.4.m4.1.2.3.1.cmml" xref="S2.SS1.p3.4.m4.1.2.3.1"></times><ci id="S2.SS1.p3.4.m4.1.2.3.2.cmml" xref="S2.SS1.p3.4.m4.1.2.3.2">ℳ</ci><ci id="S2.SS1.p3.4.m4.1.1.cmml" xref="S2.SS1.p3.4.m4.1.1">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.4.m4.1c">\hat{y}\sim\mathcal{M}(x)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p3.4.m4.1d">over^ start_ARG italic_y end_ARG ∼ caligraphic_M ( italic_x )</annotation></semantics></math> is the output or partial output of the language model, and <math id="S2.SS1.p3.5.m5.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S2.SS1.p3.5.m5.1a"><mi id="S2.SS1.p3.5.m5.1.1" xref="S2.SS1.p3.5.m5.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.5.m5.1b"><ci id="S2.SS1.p3.5.m5.1.1.cmml" xref="S2.SS1.p3.5.m5.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.5.m5.1c">c</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p3.5.m5.1d">italic_c</annotation></semantics></math> is the feedback of some format, <span id="S2.SS1.p3.6.3" class="ltx_text ltx_font_italic">e.g.</span>, scalar value, or natural language. A simple example is binary feedback of whether the output is good or bad given the input (<math id="S2.SS1.p3.6.m6.2" class="ltx_Math" alttext="\mathcal{C}:\mathcal{X}\times\mathcal{Y}\rightarrow\{0,1\}" display="inline"><semantics id="S2.SS1.p3.6.m6.2a"><mrow id="S2.SS1.p3.6.m6.2.3" xref="S2.SS1.p3.6.m6.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p3.6.m6.2.3.2" xref="S2.SS1.p3.6.m6.2.3.2.cmml">𝒞</mi><mo lspace="0.278em" rspace="0.278em" id="S2.SS1.p3.6.m6.2.3.1" xref="S2.SS1.p3.6.m6.2.3.1.cmml">:</mo><mrow id="S2.SS1.p3.6.m6.2.3.3" xref="S2.SS1.p3.6.m6.2.3.3.cmml"><mrow id="S2.SS1.p3.6.m6.2.3.3.2" xref="S2.SS1.p3.6.m6.2.3.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p3.6.m6.2.3.3.2.2" xref="S2.SS1.p3.6.m6.2.3.3.2.2.cmml">𝒳</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p3.6.m6.2.3.3.2.1" xref="S2.SS1.p3.6.m6.2.3.3.2.1.cmml">×</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p3.6.m6.2.3.3.2.3" xref="S2.SS1.p3.6.m6.2.3.3.2.3.cmml">𝒴</mi></mrow><mo stretchy="false" id="S2.SS1.p3.6.m6.2.3.3.1" xref="S2.SS1.p3.6.m6.2.3.3.1.cmml">→</mo><mrow id="S2.SS1.p3.6.m6.2.3.3.3.2" xref="S2.SS1.p3.6.m6.2.3.3.3.1.cmml"><mo stretchy="false" id="S2.SS1.p3.6.m6.2.3.3.3.2.1" xref="S2.SS1.p3.6.m6.2.3.3.3.1.cmml">{</mo><mn id="S2.SS1.p3.6.m6.1.1" xref="S2.SS1.p3.6.m6.1.1.cmml">0</mn><mo id="S2.SS1.p3.6.m6.2.3.3.3.2.2" xref="S2.SS1.p3.6.m6.2.3.3.3.1.cmml">,</mo><mn id="S2.SS1.p3.6.m6.2.2" xref="S2.SS1.p3.6.m6.2.2.cmml">1</mn><mo stretchy="false" id="S2.SS1.p3.6.m6.2.3.3.3.2.3" xref="S2.SS1.p3.6.m6.2.3.3.3.1.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.6.m6.2b"><apply id="S2.SS1.p3.6.m6.2.3.cmml" xref="S2.SS1.p3.6.m6.2.3"><ci id="S2.SS1.p3.6.m6.2.3.1.cmml" xref="S2.SS1.p3.6.m6.2.3.1">:</ci><ci id="S2.SS1.p3.6.m6.2.3.2.cmml" xref="S2.SS1.p3.6.m6.2.3.2">𝒞</ci><apply id="S2.SS1.p3.6.m6.2.3.3.cmml" xref="S2.SS1.p3.6.m6.2.3.3"><ci id="S2.SS1.p3.6.m6.2.3.3.1.cmml" xref="S2.SS1.p3.6.m6.2.3.3.1">→</ci><apply id="S2.SS1.p3.6.m6.2.3.3.2.cmml" xref="S2.SS1.p3.6.m6.2.3.3.2"><times id="S2.SS1.p3.6.m6.2.3.3.2.1.cmml" xref="S2.SS1.p3.6.m6.2.3.3.2.1"></times><ci id="S2.SS1.p3.6.m6.2.3.3.2.2.cmml" xref="S2.SS1.p3.6.m6.2.3.3.2.2">𝒳</ci><ci id="S2.SS1.p3.6.m6.2.3.3.2.3.cmml" xref="S2.SS1.p3.6.m6.2.3.3.2.3">𝒴</ci></apply><set id="S2.SS1.p3.6.m6.2.3.3.3.1.cmml" xref="S2.SS1.p3.6.m6.2.3.3.3.2"><cn type="integer" id="S2.SS1.p3.6.m6.1.1.cmml" xref="S2.SS1.p3.6.m6.1.1">0</cn><cn type="integer" id="S2.SS1.p3.6.m6.2.2.cmml" xref="S2.SS1.p3.6.m6.2.2">1</cn></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.6.m6.2c">\mathcal{C}:\mathcal{X}\times\mathcal{Y}\rightarrow\{0,1\}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p3.6.m6.2d">caligraphic_C : caligraphic_X × caligraphic_Y → { 0 , 1 }</annotation></semantics></math>).</p>
</div>
<div id="S2.SS1.p4" class="ltx_para ltx_noindent">
<p id="S2.SS1.p4.6" class="ltx_p"><math id="S2.SS1.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.p4.1.m1.1a"><mo id="S2.SS1.p4.1.m1.1.1" xref="S2.SS1.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.1.m1.1b"><ci id="S2.SS1.p4.1.m1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.1.m1.1d">∙</annotation></semantics></math> <span id="S2.SS1.p4.6.1" class="ltx_text ltx_font_bold">Refine Model</span> <span id="S2.SS1.p4.6.2" class="ltx_text ltx_font_italic">(Treatment)</span>. A refine model <math id="S2.SS1.p4.2.m2.1" class="ltx_Math" alttext="\mathcal{R}:\mathcal{X}\times\mathcal{Y}\times\mathcal{F}\rightarrow\mathcal{Y}" display="inline"><semantics id="S2.SS1.p4.2.m2.1a"><mrow id="S2.SS1.p4.2.m2.1.1" xref="S2.SS1.p4.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p4.2.m2.1.1.2" xref="S2.SS1.p4.2.m2.1.1.2.cmml">ℛ</mi><mo lspace="0.278em" rspace="0.278em" id="S2.SS1.p4.2.m2.1.1.1" xref="S2.SS1.p4.2.m2.1.1.1.cmml">:</mo><mrow id="S2.SS1.p4.2.m2.1.1.3" xref="S2.SS1.p4.2.m2.1.1.3.cmml"><mrow id="S2.SS1.p4.2.m2.1.1.3.2" xref="S2.SS1.p4.2.m2.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p4.2.m2.1.1.3.2.2" xref="S2.SS1.p4.2.m2.1.1.3.2.2.cmml">𝒳</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p4.2.m2.1.1.3.2.1" xref="S2.SS1.p4.2.m2.1.1.3.2.1.cmml">×</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p4.2.m2.1.1.3.2.3" xref="S2.SS1.p4.2.m2.1.1.3.2.3.cmml">𝒴</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p4.2.m2.1.1.3.2.1a" xref="S2.SS1.p4.2.m2.1.1.3.2.1.cmml">×</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p4.2.m2.1.1.3.2.4" xref="S2.SS1.p4.2.m2.1.1.3.2.4.cmml">ℱ</mi></mrow><mo stretchy="false" id="S2.SS1.p4.2.m2.1.1.3.1" xref="S2.SS1.p4.2.m2.1.1.3.1.cmml">→</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p4.2.m2.1.1.3.3" xref="S2.SS1.p4.2.m2.1.1.3.3.cmml">𝒴</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.2.m2.1b"><apply id="S2.SS1.p4.2.m2.1.1.cmml" xref="S2.SS1.p4.2.m2.1.1"><ci id="S2.SS1.p4.2.m2.1.1.1.cmml" xref="S2.SS1.p4.2.m2.1.1.1">:</ci><ci id="S2.SS1.p4.2.m2.1.1.2.cmml" xref="S2.SS1.p4.2.m2.1.1.2">ℛ</ci><apply id="S2.SS1.p4.2.m2.1.1.3.cmml" xref="S2.SS1.p4.2.m2.1.1.3"><ci id="S2.SS1.p4.2.m2.1.1.3.1.cmml" xref="S2.SS1.p4.2.m2.1.1.3.1">→</ci><apply id="S2.SS1.p4.2.m2.1.1.3.2.cmml" xref="S2.SS1.p4.2.m2.1.1.3.2"><times id="S2.SS1.p4.2.m2.1.1.3.2.1.cmml" xref="S2.SS1.p4.2.m2.1.1.3.2.1"></times><ci id="S2.SS1.p4.2.m2.1.1.3.2.2.cmml" xref="S2.SS1.p4.2.m2.1.1.3.2.2">𝒳</ci><ci id="S2.SS1.p4.2.m2.1.1.3.2.3.cmml" xref="S2.SS1.p4.2.m2.1.1.3.2.3">𝒴</ci><ci id="S2.SS1.p4.2.m2.1.1.3.2.4.cmml" xref="S2.SS1.p4.2.m2.1.1.3.2.4">ℱ</ci></apply><ci id="S2.SS1.p4.2.m2.1.1.3.3.cmml" xref="S2.SS1.p4.2.m2.1.1.3.3">𝒴</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.2.m2.1c">\mathcal{R}:\mathcal{X}\times\mathcal{Y}\times\mathcal{F}\rightarrow\mathcal{Y}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.2.m2.1d">caligraphic_R : caligraphic_X × caligraphic_Y × caligraphic_F → caligraphic_Y</annotation></semantics></math> learns to repair an output <math id="S2.SS1.p4.3.m3.3" class="ltx_Math" alttext="x,\hat{y},c\rightarrow y_{new}" display="inline"><semantics id="S2.SS1.p4.3.m3.3a"><mrow id="S2.SS1.p4.3.m3.3.4" xref="S2.SS1.p4.3.m3.3.4.cmml"><mrow id="S2.SS1.p4.3.m3.3.4.2.2" xref="S2.SS1.p4.3.m3.3.4.2.1.cmml"><mi id="S2.SS1.p4.3.m3.1.1" xref="S2.SS1.p4.3.m3.1.1.cmml">x</mi><mo id="S2.SS1.p4.3.m3.3.4.2.2.1" xref="S2.SS1.p4.3.m3.3.4.2.1.cmml">,</mo><mover accent="true" id="S2.SS1.p4.3.m3.2.2" xref="S2.SS1.p4.3.m3.2.2.cmml"><mi id="S2.SS1.p4.3.m3.2.2.2" xref="S2.SS1.p4.3.m3.2.2.2.cmml">y</mi><mo id="S2.SS1.p4.3.m3.2.2.1" xref="S2.SS1.p4.3.m3.2.2.1.cmml">^</mo></mover><mo id="S2.SS1.p4.3.m3.3.4.2.2.2" xref="S2.SS1.p4.3.m3.3.4.2.1.cmml">,</mo><mi id="S2.SS1.p4.3.m3.3.3" xref="S2.SS1.p4.3.m3.3.3.cmml">c</mi></mrow><mo stretchy="false" id="S2.SS1.p4.3.m3.3.4.1" xref="S2.SS1.p4.3.m3.3.4.1.cmml">→</mo><msub id="S2.SS1.p4.3.m3.3.4.3" xref="S2.SS1.p4.3.m3.3.4.3.cmml"><mi id="S2.SS1.p4.3.m3.3.4.3.2" xref="S2.SS1.p4.3.m3.3.4.3.2.cmml">y</mi><mrow id="S2.SS1.p4.3.m3.3.4.3.3" xref="S2.SS1.p4.3.m3.3.4.3.3.cmml"><mi id="S2.SS1.p4.3.m3.3.4.3.3.2" xref="S2.SS1.p4.3.m3.3.4.3.3.2.cmml">n</mi><mo id="S2.SS1.p4.3.m3.3.4.3.3.1" xref="S2.SS1.p4.3.m3.3.4.3.3.1.cmml">⁢</mo><mi id="S2.SS1.p4.3.m3.3.4.3.3.3" xref="S2.SS1.p4.3.m3.3.4.3.3.3.cmml">e</mi><mo id="S2.SS1.p4.3.m3.3.4.3.3.1a" xref="S2.SS1.p4.3.m3.3.4.3.3.1.cmml">⁢</mo><mi id="S2.SS1.p4.3.m3.3.4.3.3.4" xref="S2.SS1.p4.3.m3.3.4.3.3.4.cmml">w</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.3.m3.3b"><apply id="S2.SS1.p4.3.m3.3.4.cmml" xref="S2.SS1.p4.3.m3.3.4"><ci id="S2.SS1.p4.3.m3.3.4.1.cmml" xref="S2.SS1.p4.3.m3.3.4.1">→</ci><list id="S2.SS1.p4.3.m3.3.4.2.1.cmml" xref="S2.SS1.p4.3.m3.3.4.2.2"><ci id="S2.SS1.p4.3.m3.1.1.cmml" xref="S2.SS1.p4.3.m3.1.1">𝑥</ci><apply id="S2.SS1.p4.3.m3.2.2.cmml" xref="S2.SS1.p4.3.m3.2.2"><ci id="S2.SS1.p4.3.m3.2.2.1.cmml" xref="S2.SS1.p4.3.m3.2.2.1">^</ci><ci id="S2.SS1.p4.3.m3.2.2.2.cmml" xref="S2.SS1.p4.3.m3.2.2.2">𝑦</ci></apply><ci id="S2.SS1.p4.3.m3.3.3.cmml" xref="S2.SS1.p4.3.m3.3.3">𝑐</ci></list><apply id="S2.SS1.p4.3.m3.3.4.3.cmml" xref="S2.SS1.p4.3.m3.3.4.3"><csymbol cd="ambiguous" id="S2.SS1.p4.3.m3.3.4.3.1.cmml" xref="S2.SS1.p4.3.m3.3.4.3">subscript</csymbol><ci id="S2.SS1.p4.3.m3.3.4.3.2.cmml" xref="S2.SS1.p4.3.m3.3.4.3.2">𝑦</ci><apply id="S2.SS1.p4.3.m3.3.4.3.3.cmml" xref="S2.SS1.p4.3.m3.3.4.3.3"><times id="S2.SS1.p4.3.m3.3.4.3.3.1.cmml" xref="S2.SS1.p4.3.m3.3.4.3.3.1"></times><ci id="S2.SS1.p4.3.m3.3.4.3.3.2.cmml" xref="S2.SS1.p4.3.m3.3.4.3.3.2">𝑛</ci><ci id="S2.SS1.p4.3.m3.3.4.3.3.3.cmml" xref="S2.SS1.p4.3.m3.3.4.3.3.3">𝑒</ci><ci id="S2.SS1.p4.3.m3.3.4.3.3.4.cmml" xref="S2.SS1.p4.3.m3.3.4.3.3.4">𝑤</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.3.m3.3c">x,\hat{y},c\rightarrow y_{new}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.3.m3.3d">italic_x , over^ start_ARG italic_y end_ARG , italic_c → italic_y start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT</annotation></semantics></math> based on the feedback <math id="S2.SS1.p4.4.m4.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S2.SS1.p4.4.m4.1a"><mi id="S2.SS1.p4.4.m4.1.1" xref="S2.SS1.p4.4.m4.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.4.m4.1b"><ci id="S2.SS1.p4.4.m4.1.1.cmml" xref="S2.SS1.p4.4.m4.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.4.m4.1c">c</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.4.m4.1d">italic_c</annotation></semantics></math>, where <math id="S2.SS1.p4.5.m5.1" class="ltx_Math" alttext="y_{new}" display="inline"><semantics id="S2.SS1.p4.5.m5.1a"><msub id="S2.SS1.p4.5.m5.1.1" xref="S2.SS1.p4.5.m5.1.1.cmml"><mi id="S2.SS1.p4.5.m5.1.1.2" xref="S2.SS1.p4.5.m5.1.1.2.cmml">y</mi><mrow id="S2.SS1.p4.5.m5.1.1.3" xref="S2.SS1.p4.5.m5.1.1.3.cmml"><mi id="S2.SS1.p4.5.m5.1.1.3.2" xref="S2.SS1.p4.5.m5.1.1.3.2.cmml">n</mi><mo id="S2.SS1.p4.5.m5.1.1.3.1" xref="S2.SS1.p4.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S2.SS1.p4.5.m5.1.1.3.3" xref="S2.SS1.p4.5.m5.1.1.3.3.cmml">e</mi><mo id="S2.SS1.p4.5.m5.1.1.3.1a" xref="S2.SS1.p4.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S2.SS1.p4.5.m5.1.1.3.4" xref="S2.SS1.p4.5.m5.1.1.3.4.cmml">w</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.5.m5.1b"><apply id="S2.SS1.p4.5.m5.1.1.cmml" xref="S2.SS1.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.5.m5.1.1.1.cmml" xref="S2.SS1.p4.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p4.5.m5.1.1.2.cmml" xref="S2.SS1.p4.5.m5.1.1.2">𝑦</ci><apply id="S2.SS1.p4.5.m5.1.1.3.cmml" xref="S2.SS1.p4.5.m5.1.1.3"><times id="S2.SS1.p4.5.m5.1.1.3.1.cmml" xref="S2.SS1.p4.5.m5.1.1.3.1"></times><ci id="S2.SS1.p4.5.m5.1.1.3.2.cmml" xref="S2.SS1.p4.5.m5.1.1.3.2">𝑛</ci><ci id="S2.SS1.p4.5.m5.1.1.3.3.cmml" xref="S2.SS1.p4.5.m5.1.1.3.3">𝑒</ci><ci id="S2.SS1.p4.5.m5.1.1.3.4.cmml" xref="S2.SS1.p4.5.m5.1.1.3.4">𝑤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.5.m5.1c">y_{new}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.5.m5.1d">italic_y start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT</annotation></semantics></math> is the revised output. Besides repairing output, some refine models directly repair the language model <math id="S2.SS1.p4.6.m6.1" class="ltx_Math" alttext="\mathcal{M}" display="inline"><semantics id="S2.SS1.p4.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p4.6.m6.1.1" xref="S2.SS1.p4.6.m6.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.6.m6.1b"><ci id="S2.SS1.p4.6.m6.1.1.cmml" xref="S2.SS1.p4.6.m6.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.6.m6.1c">\mathcal{M}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.6.m6.1d">caligraphic_M</annotation></semantics></math> through fine-tuning or reinforcement learning.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.3" class="ltx_p">Based on the above formulation, Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the fundamental interaction among the language model <math id="S2.SS1.p5.1.m1.1" class="ltx_Math" alttext="\mathcal{M}" display="inline"><semantics id="S2.SS1.p5.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p5.1.m1.1.1" xref="S2.SS1.p5.1.m1.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.1.m1.1b"><ci id="S2.SS1.p5.1.m1.1.1.cmml" xref="S2.SS1.p5.1.m1.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.1.m1.1c">\mathcal{M}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p5.1.m1.1d">caligraphic_M</annotation></semantics></math>, the critic model <math id="S2.SS1.p5.2.m2.1" class="ltx_Math" alttext="\mathcal{C}" display="inline"><semantics id="S2.SS1.p5.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p5.2.m2.1.1" xref="S2.SS1.p5.2.m2.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.2.m2.1b"><ci id="S2.SS1.p5.2.m2.1.1.cmml" xref="S2.SS1.p5.2.m2.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.2.m2.1c">\mathcal{C}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p5.2.m2.1d">caligraphic_C</annotation></semantics></math>, and the refine model <math id="S2.SS1.p5.3.m3.1" class="ltx_Math" alttext="\mathcal{R}" display="inline"><semantics id="S2.SS1.p5.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p5.3.m3.1.1" xref="S2.SS1.p5.3.m3.1.1.cmml">ℛ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.3.m3.1b"><ci id="S2.SS1.p5.3.m3.1.1.cmml" xref="S2.SS1.p5.3.m3.1.1">ℛ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.3.m3.1c">\mathcal{R}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p5.3.m3.1d">caligraphic_R</annotation></semantics></math>. However, the specific model design in existing works varies along five crucial axes, which we will elaborate on in the following sections.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>What gets corrected?</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">LLM-based natural language systems can exhibit a variety of errors. We summarize the four major types of errors that are targeted for correction in existing works through automated feedback.
</p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p"><math id="S2.SS2.p2.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS2.p2.1.m1.1a"><mo id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><ci id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.1.m1.1d">∙</annotation></semantics></math> <span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">Hallucination.</span> An open challenge for LLMs is that they often hallucinate by making up facts or citing sources that do not exist <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib64" title="" class="ltx_ref">2023a</a>); Zhang et al. (<a href="#bib.bib143" title="" class="ltx_ref">2023c</a>)</cite>. These hallucinated contents are often quite plausible-sounding, making it difficult even for humans to detect <cite class="ltx_cite ltx_citemacro_cite">Clark et al. (<a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>. To address this, several studies have proposed the collection of automated feedback on potential factual inaccuracies by cross-referencing the output generated by the model with credible knowledge sources. The gathered feedback can then be utilized by a subsequent refinement model to correct hallucinations <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a href="#bib.bib32" title="" class="ltx_ref">2023b</a>); Peng et al. (<a href="#bib.bib91" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.p3.1" class="ltx_p"><math id="S2.SS2.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS2.p3.1.m1.1a"><mo id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><ci id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.1.m1.1d">∙</annotation></semantics></math> <span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold">Unfaithful Reasoning.</span> LLMs have exhibited a strong ability in solving complex reasoning tasks with improved reasoning strategies, such as Chain-of-Thought prompting <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a href="#bib.bib120" title="" class="ltx_ref">2022b</a>)</cite>. However, recent studies <cite class="ltx_cite ltx_citemacro_cite">Golovneva et al. (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>); Ribeiro et al. (<a href="#bib.bib95" title="" class="ltx_ref">2023</a>); Lyu et al. (<a href="#bib.bib78" title="" class="ltx_ref">2023b</a>)</cite> found that LLMs occasionally make <span id="S2.SS2.p3.1.2" class="ltx_text ltx_font_italic">unfaithful</span> reasoning, <span id="S2.SS2.p3.1.3" class="ltx_text ltx_font_italic">i.e.</span>, the derived conclusion does not follow the previously generated reasoning chain. To address this, existing works have proposed the use of automated feedback from external tools or models for guiding the reasoning process <cite class="ltx_cite ltx_citemacro_cite">Xie et al. (<a href="#bib.bib126" title="" class="ltx_ref">2023</a>); Yao et al. (<a href="#bib.bib136" title="" class="ltx_ref">2023a</a>)</cite>, verifying the reasoning process and rectifying errors <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>); Pan et al. (<a href="#bib.bib88" title="" class="ltx_ref">2023</a>)</cite>, or fine-tuning LLMs with process-based feedback <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a href="#bib.bib45" title="" class="ltx_ref">2022</a>); Lightman et al. (<a href="#bib.bib70" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para ltx_noindent">
<p id="S2.SS2.p4.1" class="ltx_p"><math id="S2.SS2.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS2.p4.1.m1.1a"><mo id="S2.SS2.p4.1.m1.1.1" xref="S2.SS2.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.1.m1.1b"><ci id="S2.SS2.p4.1.m1.1.1.cmml" xref="S2.SS2.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p4.1.m1.1d">∙</annotation></semantics></math> <span id="S2.SS2.p4.1.1" class="ltx_text ltx_font_bold">Toxic, Biased, and Harmful Contents.</span>
LLMs have been observed to occasionally generate content that is toxic, biased, or harmful due to biases present in the training data <cite class="ltx_cite ltx_citemacro_cite">Shaikh et al. (<a href="#bib.bib102" title="" class="ltx_ref">2023</a>)</cite>. To rectify this, reinforcement learning from human feedback (RLHF) <cite class="ltx_cite ltx_citemacro_cite">Ouyang et al. (<a href="#bib.bib87" title="" class="ltx_ref">2022</a>); Bai et al. (<a href="#bib.bib3" title="" class="ltx_ref">2022a</a>)</cite> has been extensively employed to train LLMs to align more closely with human values, such as being helpful, honest, and harmless. However, RLHF is heavily dependent on high-quality human feedback, the collection of which can be resource-intensive. To alleviate this, recent works <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib76" title="" class="ltx_ref">2022</a>); Gou et al. (<a href="#bib.bib38" title="" class="ltx_ref">2023</a>)</cite> have also explored collecting automated feedback to identify and correct potentially harmful outputs.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para ltx_noindent">
<p id="S2.SS2.p5.1" class="ltx_p"><math id="S2.SS2.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS2.p5.1.m1.1a"><mo id="S2.SS2.p5.1.m1.1.1" xref="S2.SS2.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.1.m1.1b"><ci id="S2.SS2.p5.1.m1.1.1.cmml" xref="S2.SS2.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p5.1.m1.1d">∙</annotation></semantics></math> <span id="S2.SS2.p5.1.1" class="ltx_text ltx_font_bold">Flawed Code.</span> Besides generating natural language text, LLMs also show strong abilities to generate computer programs (<span id="S2.SS2.p5.1.2" class="ltx_text ltx_font_italic">i.e.</span>, code) <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib11" title="" class="ltx_ref">2023a</a>)</cite>. However, the generated code can sometimes be flawed or incorrect. To fix this, the approach of learning from automated feedback has been extensively applied in code generation <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib13" title="" class="ltx_ref">2023c</a>); Olausson et al. (<a href="#bib.bib84" title="" class="ltx_ref">2023</a>)</cite>, largely facilitated by the ease of obtaining such feedback through the execution of generated code with the corresponding compilers or interpreters.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>What is the <span id="S2.SS3.1.1" class="ltx_text ltx_font_italic">source</span> of the feedback?</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Feedback can be broadly divided into two categories: <span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_italic">human feedback</span> and <span id="S2.SS3.p1.1.2" class="ltx_text ltx_font_italic">automated feedback</span>. <cite class="ltx_cite ltx_citemacro_citet">Fernandes et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite> provided a survey on integrating human feedback for language generation. In our survey, we focus on the emerging research area of automated feedback, which explores the possibility of LLMs to self-correct without constant human intervention. Automated feedback typically originates from two sources, distinguished by their relationship with the LLM: <span id="S2.SS3.p1.1.3" class="ltx_text ltx_font_italic">self-feedback</span> (<span id="S2.SS3.p1.1.4" class="ltx_text ltx_font_italic">i.e.</span>, the feedback originates from the LLM itself) and <span id="S2.SS3.p1.1.5" class="ltx_text ltx_font_italic">external feedback</span> (<span id="S2.SS3.p1.1.6" class="ltx_text ltx_font_italic">i.e.</span>, the feedback is derived from external models, tools, or knowledge sources).</p>
</div>
<div id="S2.SS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.p2.1" class="ltx_p"><math id="S2.SS3.p2.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS3.p2.1.m1.1a"><mo id="S2.SS3.p2.1.m1.1.1" xref="S2.SS3.p2.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.1.m1.1b"><ci id="S2.SS3.p2.1.m1.1.1.cmml" xref="S2.SS3.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.1.m1.1d">∙</annotation></semantics></math> <span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_bold">Self-Feedback.</span>
The LLM itself can be utilized as a feedback provider. One straightforward way is to directly evaluate the quality of the generated outputs through prompting and subsequently use this feedback to refine the results <cite class="ltx_cite ltx_citemacro_cite">Madaan et al. (<a href="#bib.bib79" title="" class="ltx_ref">2023</a>); Shinn et al. (<a href="#bib.bib104" title="" class="ltx_ref">2023</a>)</cite>. This process can be iterative, with the model continually refining its output until it meets a certain standard. This continuous self-improvement strategy has been found particularly useful by numerous studies <cite class="ltx_cite ltx_citemacro_cite">Ye et al. (<a href="#bib.bib138" title="" class="ltx_ref">2023</a>); Yan et al. (<a href="#bib.bib130" title="" class="ltx_ref">2023a</a>)</cite>, especially in scenarios where external feedback is unavailable or limited.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para ltx_noindent">
<p id="S2.SS3.p3.1" class="ltx_p"><math id="S2.SS3.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS3.p3.1.m1.1a"><mo id="S2.SS3.p3.1.m1.1.1" xref="S2.SS3.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.1.m1.1b"><ci id="S2.SS3.p3.1.m1.1.1.cmml" xref="S2.SS3.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p3.1.m1.1d">∙</annotation></semantics></math> <span id="S2.SS3.p3.1.1" class="ltx_text ltx_font_bold">External Feedback.</span> Feedback can originate from sources external to the LLM, typically including 1) other trained models <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib134" title="" class="ltx_ref">2022b</a>); Lightman et al. (<a href="#bib.bib70" title="" class="ltx_ref">2023</a>)</cite>, 2) external tools <cite class="ltx_cite ltx_citemacro_cite">Gou et al. (<a href="#bib.bib38" title="" class="ltx_ref">2023</a>); Charalambous et al. (<a href="#bib.bib10" title="" class="ltx_ref">2023</a>)</cite>, 3) external knowledge sources <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a href="#bib.bib32" title="" class="ltx_ref">2023b</a>); Yu et al. (<a href="#bib.bib139" title="" class="ltx_ref">2023</a>)</cite>, and 4) external evaluation metrics <cite class="ltx_cite ltx_citemacro_cite">Jung et al. (<a href="#bib.bib48" title="" class="ltx_ref">2022</a>); Welleck et al. (<a href="#bib.bib121" title="" class="ltx_ref">2023</a>)</cite>. External feedback provides a valuable outside perspective which is particularly useful in identifying errors that the LLM might not recognize on its own. For example, code interpreters are widely used in programming tasks to provide real-time error messages; while external knowledge sources can be utilized to verify the factual accuracy of the LLM’s output.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>What is the <span id="S2.SS4.1.1" class="ltx_text ltx_font_italic">format</span> of the feedback?</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">The selection of feedback format requires the consideration of its expressivity, the ease of its collection, and its potential to improve systems <cite class="ltx_cite ltx_citemacro_cite">Fernandes et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite>. In existing works, automated feedback is typically in the form of a <span id="S2.SS4.p1.1.1" class="ltx_text ltx_font_italic">scalar value</span> signal or in <span id="S2.SS4.p1.1.2" class="ltx_text ltx_font_italic">natural language</span>.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para ltx_noindent">
<p id="S2.SS4.p2.2" class="ltx_p"><math id="S2.SS4.p2.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS4.p2.1.m1.1a"><mo id="S2.SS4.p2.1.m1.1.1" xref="S2.SS4.p2.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.1.m1.1b"><ci id="S2.SS4.p2.1.m1.1.1.cmml" xref="S2.SS4.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.1.m1.1d">∙</annotation></semantics></math> <span id="S2.SS4.p2.2.1" class="ltx_text ltx_font_bold">Scalar Value Feedback.</span>
In this scenario, the critic model maps the input and output to a single score (<math id="S2.SS4.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{C}:\mathcal{X}\times\mathcal{Y}\rightarrow\mathcal{N}\subseteq\mathbb{R}" display="inline"><semantics id="S2.SS4.p2.2.m2.1a"><mrow id="S2.SS4.p2.2.m2.1.1" xref="S2.SS4.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS4.p2.2.m2.1.1.2" xref="S2.SS4.p2.2.m2.1.1.2.cmml">𝒞</mi><mo lspace="0.278em" rspace="0.278em" id="S2.SS4.p2.2.m2.1.1.1" xref="S2.SS4.p2.2.m2.1.1.1.cmml">:</mo><mrow id="S2.SS4.p2.2.m2.1.1.3" xref="S2.SS4.p2.2.m2.1.1.3.cmml"><mrow id="S2.SS4.p2.2.m2.1.1.3.2" xref="S2.SS4.p2.2.m2.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS4.p2.2.m2.1.1.3.2.2" xref="S2.SS4.p2.2.m2.1.1.3.2.2.cmml">𝒳</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS4.p2.2.m2.1.1.3.2.1" xref="S2.SS4.p2.2.m2.1.1.3.2.1.cmml">×</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS4.p2.2.m2.1.1.3.2.3" xref="S2.SS4.p2.2.m2.1.1.3.2.3.cmml">𝒴</mi></mrow><mo stretchy="false" id="S2.SS4.p2.2.m2.1.1.3.3" xref="S2.SS4.p2.2.m2.1.1.3.3.cmml">→</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS4.p2.2.m2.1.1.3.4" xref="S2.SS4.p2.2.m2.1.1.3.4.cmml">𝒩</mi><mo id="S2.SS4.p2.2.m2.1.1.3.5" xref="S2.SS4.p2.2.m2.1.1.3.5.cmml">⊆</mo><mi id="S2.SS4.p2.2.m2.1.1.3.6" xref="S2.SS4.p2.2.m2.1.1.3.6.cmml">ℝ</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.2.m2.1b"><apply id="S2.SS4.p2.2.m2.1.1.cmml" xref="S2.SS4.p2.2.m2.1.1"><ci id="S2.SS4.p2.2.m2.1.1.1.cmml" xref="S2.SS4.p2.2.m2.1.1.1">:</ci><ci id="S2.SS4.p2.2.m2.1.1.2.cmml" xref="S2.SS4.p2.2.m2.1.1.2">𝒞</ci><apply id="S2.SS4.p2.2.m2.1.1.3.cmml" xref="S2.SS4.p2.2.m2.1.1.3"><and id="S2.SS4.p2.2.m2.1.1.3a.cmml" xref="S2.SS4.p2.2.m2.1.1.3"></and><apply id="S2.SS4.p2.2.m2.1.1.3b.cmml" xref="S2.SS4.p2.2.m2.1.1.3"><ci id="S2.SS4.p2.2.m2.1.1.3.3.cmml" xref="S2.SS4.p2.2.m2.1.1.3.3">→</ci><apply id="S2.SS4.p2.2.m2.1.1.3.2.cmml" xref="S2.SS4.p2.2.m2.1.1.3.2"><times id="S2.SS4.p2.2.m2.1.1.3.2.1.cmml" xref="S2.SS4.p2.2.m2.1.1.3.2.1"></times><ci id="S2.SS4.p2.2.m2.1.1.3.2.2.cmml" xref="S2.SS4.p2.2.m2.1.1.3.2.2">𝒳</ci><ci id="S2.SS4.p2.2.m2.1.1.3.2.3.cmml" xref="S2.SS4.p2.2.m2.1.1.3.2.3">𝒴</ci></apply><ci id="S2.SS4.p2.2.m2.1.1.3.4.cmml" xref="S2.SS4.p2.2.m2.1.1.3.4">𝒩</ci></apply><apply id="S2.SS4.p2.2.m2.1.1.3c.cmml" xref="S2.SS4.p2.2.m2.1.1.3"><subset id="S2.SS4.p2.2.m2.1.1.3.5.cmml" xref="S2.SS4.p2.2.m2.1.1.3.5"></subset><share href="#S2.SS4.p2.2.m2.1.1.3.4.cmml" id="S2.SS4.p2.2.m2.1.1.3d.cmml" xref="S2.SS4.p2.2.m2.1.1.3"></share><ci id="S2.SS4.p2.2.m2.1.1.3.6.cmml" xref="S2.SS4.p2.2.m2.1.1.3.6">ℝ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.2.m2.1c">\mathcal{C}:\mathcal{X}\times\mathcal{Y}\rightarrow\mathcal{N}\subseteq\mathbb{R}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.2.m2.1d">caligraphic_C : caligraphic_X × caligraphic_Y → caligraphic_N ⊆ blackboard_R</annotation></semantics></math>). Scalar value feedback can be easily integrated into the training/decoding process of LLMs. For example, Self-Verification <cite class="ltx_cite ltx_citemacro_cite">Weng et al. (<a href="#bib.bib122" title="" class="ltx_ref">2023</a>)</cite> ranks candidate outputs to find the optimal one based on the real-value feedback score assigned by the critic model to each candidate. Similarly, <cite class="ltx_cite ltx_citemacro_citet">Xie et al. (<a href="#bib.bib126" title="" class="ltx_ref">2023</a>)</cite> use real-value feedback for each intermediate reasoning step to guide the model in performing a stochastic beam search for the optimal solution. However, despite its flexibility, scalar value feedback is often less informative to capture detailed information necessary for model correction.</p>
</div>
<div id="S2.SS4.p3" class="ltx_para ltx_noindent">
<p id="S2.SS4.p3.1" class="ltx_p"><math id="S2.SS4.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS4.p3.1.m1.1a"><mo id="S2.SS4.p3.1.m1.1.1" xref="S2.SS4.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS4.p3.1.m1.1b"><ci id="S2.SS4.p3.1.m1.1.1.cmml" xref="S2.SS4.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p3.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p3.1.m1.1d">∙</annotation></semantics></math> <span id="S2.SS4.p3.1.1" class="ltx_text ltx_font_bold">Natural Language Feedback.</span>
Natural language feedback offers greater expressivity than scalar value feedback, providing richer information that can highlight the shortcomings of the current output or suggest specific improvements. This form of feedback is particularly crucial for certain applications, such as text editing and code generation. For text editing, PEER <cite class="ltx_cite ltx_citemacro_cite">Schick et al. (<a href="#bib.bib99" title="" class="ltx_ref">2023</a>)</cite> trains an LLM to generate detailed suggestions for edits to the initial generated text, such as “remove unsourced claim” or “rewrote the guacamole question for clarity”.
For code generation, Self-Debug <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib13" title="" class="ltx_ref">2023c</a>)</cite> uses LLMs to generate explanations for the produced code and utilize both the explanation and the execution results as feedback to enhance coding solutions.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<div id="S2.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:658.7pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-216.0pt,327.9pt) scale(0.500941809142604,0.500941809142604) ;">
<table id="S2.T1.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.2.1" class="ltx_tr">
<td id="S2.T1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2"><span id="S2.T1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S2.T1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">
<span id="S2.T1.1.1.2.1.2.1" class="ltx_text ltx_font_bold">Feedback</span></td>
<td id="S2.T1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="3">
<span id="S2.T1.1.1.2.1.3.1" class="ltx_text ltx_font_bold">Model Refinement</span></td>
<td id="S2.T1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S2.T1.1.1.2.1.4.1" class="ltx_text ltx_font_bold">Application</span></td>
</tr>
<tr id="S2.T1.1.1.3.2" class="ltx_tr">
<td id="S2.T1.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.3.2.1.1" class="ltx_text ltx_font_bold">Source</span></td>
<td id="S2.T1.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.3.2.2.1" class="ltx_text ltx_font_bold">Format</span></td>
<td id="S2.T1.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.3.2.3.1" class="ltx_text ltx_font_bold">Strategy</span></td>
<td id="S2.T1.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.3.2.4.1" class="ltx_text ltx_font_bold">Learning</span></td>
<td id="S2.T1.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.3.2.5.1" class="ltx_text ltx_font_bold">Iter.</span></td>
</tr>
<tr id="S2.T1.1.1.4.3" class="ltx_tr" style="background-color:#E0FFFF;">
<td id="S2.T1.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_t" colspan="7"><span id="S2.T1.1.1.4.3.1.1" class="ltx_text ltx_font_bold" style="background-color:#E0FFFF;">Training-Time Correction<span id="S2.T1.1.1.4.3.1.1.1" class="ltx_text ltx_font_medium"></span></span></td>
</tr>
<tr id="S2.T1.1.1.5.4" class="ltx_tr">
<td id="S2.T1.1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_t">RLHF <cite class="ltx_cite ltx_citemacro_cite">Ouyang et al. (<a href="#bib.bib87" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S2.T1.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_t">Reward Model</td>
<td id="S2.T1.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t">Scalar</td>
<td id="S2.T1.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_t">RLHF</td>
<td id="S2.T1.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_t">RL</td>
<td id="S2.T1.1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_t">
<span id="S2.T1.1.1.5.4.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.5.4.7" class="ltx_td ltx_align_center ltx_border_t">Multiple Tasks</td>
</tr>
<tr id="S2.T1.1.1.6.5" class="ltx_tr">
<td id="S2.T1.1.1.6.5.1" class="ltx_td ltx_align_left">Fine-Grained RLHF <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib124" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S2.T1.1.1.6.5.2" class="ltx_td ltx_align_center">Reward Model</td>
<td id="S2.T1.1.1.6.5.3" class="ltx_td ltx_align_center">Scalar</td>
<td id="S2.T1.1.1.6.5.4" class="ltx_td ltx_align_center">RLHF</td>
<td id="S2.T1.1.1.6.5.5" class="ltx_td ltx_align_center">RL</td>
<td id="S2.T1.1.1.6.5.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.6.5.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.6.5.7" class="ltx_td ltx_align_center">Detoxification, Long-form QA</td>
</tr>
<tr id="S2.T1.1.1.7.6" class="ltx_tr">
<td id="S2.T1.1.1.7.6.1" class="ltx_td ltx_align_left">HH-RLHF <cite class="ltx_cite ltx_citemacro_cite">Bai et al. (<a href="#bib.bib3" title="" class="ltx_ref">2022a</a>)</cite>
</td>
<td id="S2.T1.1.1.7.6.2" class="ltx_td ltx_align_center">Reward Model</td>
<td id="S2.T1.1.1.7.6.3" class="ltx_td ltx_align_center">Scalar</td>
<td id="S2.T1.1.1.7.6.4" class="ltx_td ltx_align_center">RLHF</td>
<td id="S2.T1.1.1.7.6.5" class="ltx_td ltx_align_center">SL &amp; RL</td>
<td id="S2.T1.1.1.7.6.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.7.6.6.1" class="ltx_ERROR undefined">\usym</span>2713</td>
<td id="S2.T1.1.1.7.6.7" class="ltx_td ltx_align_center">Helpfulness, Harmlessness</td>
</tr>
<tr id="S2.T1.1.1.8.7" class="ltx_tr">
<td id="S2.T1.1.1.8.7.1" class="ltx_td ltx_align_left">Sparrow <cite class="ltx_cite ltx_citemacro_cite">Glaese et al. (<a href="#bib.bib35" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S2.T1.1.1.8.7.2" class="ltx_td ltx_align_center">Reward Model</td>
<td id="S2.T1.1.1.8.7.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.8.7.4" class="ltx_td ltx_align_center">RLHF</td>
<td id="S2.T1.1.1.8.7.5" class="ltx_td ltx_align_center">SL &amp; RL</td>
<td id="S2.T1.1.1.8.7.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.8.7.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.8.7.7" class="ltx_td ltx_align_center">Dialogue</td>
</tr>
<tr id="S2.T1.1.1.9.8" class="ltx_tr">
<td id="S2.T1.1.1.9.8.1" class="ltx_td ltx_align_left">ILF <cite class="ltx_cite ltx_citemacro_cite">Scheurer et al. (<a href="#bib.bib98" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.9.8.2" class="ltx_td ltx_align_center">Human Feedback</td>
<td id="S2.T1.1.1.9.8.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.9.8.4" class="ltx_td ltx_align_center">Fine-tuning</td>
<td id="S2.T1.1.1.9.8.5" class="ltx_td ltx_align_center">SL</td>
<td id="S2.T1.1.1.9.8.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.9.8.6.1" class="ltx_ERROR undefined">\usym</span>2713</td>
<td id="S2.T1.1.1.9.8.7" class="ltx_td ltx_align_center">Summarization</td>
</tr>
<tr id="S2.T1.1.1.10.9" class="ltx_tr">
<td id="S2.T1.1.1.10.9.1" class="ltx_td ltx_align_left"><cite class="ltx_cite ltx_citemacro_citet">Gao et al. (<a href="#bib.bib31" title="" class="ltx_ref">2023a</a>)</cite></td>
<td id="S2.T1.1.1.10.9.2" class="ltx_td ltx_align_center">Human Feedback</td>
<td id="S2.T1.1.1.10.9.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.10.9.4" class="ltx_td ltx_align_center">Fine-tuning</td>
<td id="S2.T1.1.1.10.9.5" class="ltx_td ltx_align_center">SL &amp; RL</td>
<td id="S2.T1.1.1.10.9.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.10.9.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.10.9.7" class="ltx_td ltx_align_center">Extractive QA</td>
</tr>
<tr id="S2.T1.1.1.11.10" class="ltx_tr">
<td id="S2.T1.1.1.11.10.1" class="ltx_td ltx_align_left">Chain-of-Hindsight <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib74" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.11.10.2" class="ltx_td ltx_align_center">Human Feedback</td>
<td id="S2.T1.1.1.11.10.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.11.10.4" class="ltx_td ltx_align_center">Fine-tuning</td>
<td id="S2.T1.1.1.11.10.5" class="ltx_td ltx_align_center">SL</td>
<td id="S2.T1.1.1.11.10.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.11.10.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.11.10.7" class="ltx_td ltx_align_center">Multiple Tasks</td>
</tr>
<tr id="S2.T1.1.1.12.11" class="ltx_tr">
<td id="S2.T1.1.1.12.11.1" class="ltx_td ltx_align_left">Quark <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib76" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S2.T1.1.1.12.11.2" class="ltx_td ltx_align_center">External Metrics</td>
<td id="S2.T1.1.1.12.11.3" class="ltx_td ltx_align_center">Scalar</td>
<td id="S2.T1.1.1.12.11.4" class="ltx_td ltx_align_center">Fine-tuning</td>
<td id="S2.T1.1.1.12.11.5" class="ltx_td ltx_align_center">RL</td>
<td id="S2.T1.1.1.12.11.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.12.11.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.12.11.7" class="ltx_td ltx_align_center">Toxicity, Repetition, Sentiment</td>
</tr>
<tr id="S2.T1.1.1.13.12" class="ltx_tr">
<td id="S2.T1.1.1.13.12.1" class="ltx_td ltx_align_left">SimCLS <cite class="ltx_cite ltx_citemacro_cite">Liu and Liu (<a href="#bib.bib75" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S2.T1.1.1.13.12.2" class="ltx_td ltx_align_center">External Metrics</td>
<td id="S2.T1.1.1.13.12.3" class="ltx_td ltx_align_center">Scalar</td>
<td id="S2.T1.1.1.13.12.4" class="ltx_td ltx_align_center">Fine-tuning</td>
<td id="S2.T1.1.1.13.12.5" class="ltx_td ltx_align_center">SL</td>
<td id="S2.T1.1.1.13.12.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.13.12.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.13.12.7" class="ltx_td ltx_align_center">Summarization</td>
</tr>
<tr id="S2.T1.1.1.14.13" class="ltx_tr">
<td id="S2.T1.1.1.14.13.1" class="ltx_td ltx_align_left">BERTTune <cite class="ltx_cite ltx_citemacro_cite">Unanue et al. (<a href="#bib.bib112" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S2.T1.1.1.14.13.2" class="ltx_td ltx_align_center">External Metrics</td>
<td id="S2.T1.1.1.14.13.3" class="ltx_td ltx_align_center">Scalar</td>
<td id="S2.T1.1.1.14.13.4" class="ltx_td ltx_align_center">Fine-tuning</td>
<td id="S2.T1.1.1.14.13.5" class="ltx_td ltx_align_center">RL</td>
<td id="S2.T1.1.1.14.13.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.14.13.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.14.13.7" class="ltx_td ltx_align_center">Machine Translation</td>
</tr>
<tr id="S2.T1.1.1.15.14" class="ltx_tr">
<td id="S2.T1.1.1.15.14.1" class="ltx_td ltx_align_left">STaR <cite class="ltx_cite ltx_citemacro_cite">Zelikman et al. (<a href="#bib.bib140" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S2.T1.1.1.15.14.2" class="ltx_td ltx_align_center">Language Model</td>
<td id="S2.T1.1.1.15.14.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.15.14.4" class="ltx_td ltx_align_center">Self-Training</td>
<td id="S2.T1.1.1.15.14.5" class="ltx_td ltx_align_center">SL</td>
<td id="S2.T1.1.1.15.14.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.15.14.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.15.14.7" class="ltx_td ltx_align_center">QA, Reasoning</td>
</tr>
<tr id="S2.T1.1.1.16.15" class="ltx_tr">
<td id="S2.T1.1.1.16.15.1" class="ltx_td ltx_align_left">Self-Instruct <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib117" title="" class="ltx_ref">2023d</a>)</cite>
</td>
<td id="S2.T1.1.1.16.15.2" class="ltx_td ltx_align_center">Language Model</td>
<td id="S2.T1.1.1.16.15.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.16.15.4" class="ltx_td ltx_align_center">Self-Training</td>
<td id="S2.T1.1.1.16.15.5" class="ltx_td ltx_align_center">SL</td>
<td id="S2.T1.1.1.16.15.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.16.15.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.16.15.7" class="ltx_td ltx_align_center">Multiple Tasks</td>
</tr>
<tr id="S2.T1.1.1.17.16" class="ltx_tr">
<td id="S2.T1.1.1.17.16.1" class="ltx_td ltx_align_left">RLAIF <cite class="ltx_cite ltx_citemacro_cite">Bai et al. (<a href="#bib.bib4" title="" class="ltx_ref">2022b</a>)</cite>
</td>
<td id="S2.T1.1.1.17.16.2" class="ltx_td ltx_align_center">Language Model</td>
<td id="S2.T1.1.1.17.16.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.17.16.4" class="ltx_td ltx_align_center">Self-Training</td>
<td id="S2.T1.1.1.17.16.5" class="ltx_td ltx_align_center">SL &amp; RL</td>
<td id="S2.T1.1.1.17.16.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.17.16.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.17.16.7" class="ltx_td ltx_align_center">Dialogue</td>
</tr>
<tr id="S2.T1.1.1.18.17" class="ltx_tr">
<td id="S2.T1.1.1.18.17.1" class="ltx_td ltx_align_left">SIRLC <cite class="ltx_cite ltx_citemacro_cite">Pang et al. (<a href="#bib.bib89" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.18.17.2" class="ltx_td ltx_align_center">Language Model</td>
<td id="S2.T1.1.1.18.17.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.18.17.4" class="ltx_td ltx_align_center">Self-Training</td>
<td id="S2.T1.1.1.18.17.5" class="ltx_td ltx_align_center">RL</td>
<td id="S2.T1.1.1.18.17.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.18.17.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.18.17.7" class="ltx_td ltx_align_center">Reasoning, Trans., Summ.</td>
</tr>
<tr id="S2.T1.1.1.19.18" class="ltx_tr">
<td id="S2.T1.1.1.19.18.1" class="ltx_td ltx_align_left">Self-Improve <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a href="#bib.bib45" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S2.T1.1.1.19.18.2" class="ltx_td ltx_align_center">Language Model</td>
<td id="S2.T1.1.1.19.18.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.19.18.4" class="ltx_td ltx_align_center">Self-Training</td>
<td id="S2.T1.1.1.19.18.5" class="ltx_td ltx_align_center">SL</td>
<td id="S2.T1.1.1.19.18.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.19.18.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.19.18.7" class="ltx_td ltx_align_center">QA, Reasoning, NLI</td>
</tr>
<tr id="S2.T1.1.1.20.19" class="ltx_tr">
<td id="S2.T1.1.1.20.19.1" class="ltx_td ltx_align_left">AlpacaFarm <cite class="ltx_cite ltx_citemacro_cite">Dubois et al. (<a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.20.19.2" class="ltx_td ltx_align_center">Language Model</td>
<td id="S2.T1.1.1.20.19.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.20.19.4" class="ltx_td ltx_align_center">Self-Training</td>
<td id="S2.T1.1.1.20.19.5" class="ltx_td ltx_align_center">SL &amp; RL</td>
<td id="S2.T1.1.1.20.19.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.20.19.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.20.19.7" class="ltx_td ltx_align_center">None (Intrinsic Evaluation)</td>
</tr>
<tr id="S2.T1.1.1.21.20" class="ltx_tr" style="background-color:#E0FFFF;">
<td id="S2.T1.1.1.21.20.1" class="ltx_td ltx_align_center ltx_border_t" colspan="7"><span id="S2.T1.1.1.21.20.1.1" class="ltx_text ltx_font_bold" style="background-color:#E0FFFF;">Generation-Time Correction<span id="S2.T1.1.1.21.20.1.1.1" class="ltx_text ltx_font_medium"></span></span></td>
</tr>
<tr id="S2.T1.1.1.22.21" class="ltx_tr">
<td id="S2.T1.1.1.22.21.1" class="ltx_td ltx_align_left ltx_border_t">Self-Verification <cite class="ltx_cite ltx_citemacro_cite">Weng et al. (<a href="#bib.bib122" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.22.21.2" class="ltx_td ltx_align_center ltx_border_t">Language Model</td>
<td id="S2.T1.1.1.22.21.3" class="ltx_td ltx_align_center ltx_border_t">Scalar</td>
<td id="S2.T1.1.1.22.21.4" class="ltx_td ltx_align_center ltx_border_t">Re-Ranking</td>
<td id="S2.T1.1.1.22.21.5" class="ltx_td ltx_align_center ltx_border_t">ICL</td>
<td id="S2.T1.1.1.22.21.6" class="ltx_td ltx_align_center ltx_border_t">
<span id="S2.T1.1.1.22.21.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.22.21.7" class="ltx_td ltx_align_center ltx_border_t">Arithmetic Reasoning</td>
</tr>
<tr id="S2.T1.1.1.23.22" class="ltx_tr">
<td id="S2.T1.1.1.23.22.1" class="ltx_td ltx_align_left">CodeT <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib11" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S2.T1.1.1.23.22.2" class="ltx_td ltx_align_center">Program Executor</td>
<td id="S2.T1.1.1.23.22.3" class="ltx_td ltx_align_center">Scalar</td>
<td id="S2.T1.1.1.23.22.4" class="ltx_td ltx_align_center">Re-Ranking</td>
<td id="S2.T1.1.1.23.22.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.23.22.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.23.22.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.23.22.7" class="ltx_td ltx_align_center">Code Generation</td>
</tr>
<tr id="S2.T1.1.1.24.23" class="ltx_tr">
<td id="S2.T1.1.1.24.23.1" class="ltx_td ltx_align_left">LEVER <cite class="ltx_cite ltx_citemacro_cite">Ni et al. (<a href="#bib.bib83" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.24.23.2" class="ltx_td ltx_align_center">Program Executor</td>
<td id="S2.T1.1.1.24.23.3" class="ltx_td ltx_align_center">Scalar</td>
<td id="S2.T1.1.1.24.23.4" class="ltx_td ltx_align_center">Re-Ranking</td>
<td id="S2.T1.1.1.24.23.5" class="ltx_td ltx_align_center">SL</td>
<td id="S2.T1.1.1.24.23.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.24.23.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.24.23.7" class="ltx_td ltx_align_center">Table QA, Math QA, Program</td>
</tr>
<tr id="S2.T1.1.1.25.24" class="ltx_tr">
<td id="S2.T1.1.1.25.24.1" class="ltx_td ltx_align_left">RR <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.25.24.2" class="ltx_td ltx_align_center">External Knowledge</td>
<td id="S2.T1.1.1.25.24.3" class="ltx_td ltx_align_center">Scalar</td>
<td id="S2.T1.1.1.25.24.4" class="ltx_td ltx_align_center">Re-Ranking</td>
<td id="S2.T1.1.1.25.24.5" class="ltx_td ltx_align_center">—</td>
<td id="S2.T1.1.1.25.24.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.25.24.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.25.24.7" class="ltx_td ltx_align_center">Reasoning</td>
</tr>
<tr id="S2.T1.1.1.26.25" class="ltx_tr">
<td id="S2.T1.1.1.26.25.1" class="ltx_td ltx_align_left">InstructScore <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a href="#bib.bib129" title="" class="ltx_ref">2023b</a>)</cite>
</td>
<td id="S2.T1.1.1.26.25.2" class="ltx_td ltx_align_center">Language Model</td>
<td id="S2.T1.1.1.26.25.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.26.25.4" class="ltx_td ltx_align_center">Re-Ranking</td>
<td id="S2.T1.1.1.26.25.5" class="ltx_td ltx_align_center">SL</td>
<td id="S2.T1.1.1.26.25.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.26.25.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.26.25.7" class="ltx_td ltx_align_center">Generation Evaluation</td>
</tr>
<tr id="S2.T1.1.1.27.26" class="ltx_tr">
<td id="S2.T1.1.1.27.26.1" class="ltx_td ltx_align_left">MBR Decoding <cite class="ltx_cite ltx_citemacro_cite">Freitag et al. (<a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S2.T1.1.1.27.26.2" class="ltx_td ltx_align_center">External Metrics</td>
<td id="S2.T1.1.1.27.26.3" class="ltx_td ltx_align_center">Scalar</td>
<td id="S2.T1.1.1.27.26.4" class="ltx_td ltx_align_center">Re-Ranking</td>
<td id="S2.T1.1.1.27.26.5" class="ltx_td ltx_align_center">SL</td>
<td id="S2.T1.1.1.27.26.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.27.26.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.27.26.7" class="ltx_td ltx_align_center">Machine Translation</td>
</tr>
<tr id="S2.T1.1.1.28.27" class="ltx_tr">
<td id="S2.T1.1.1.28.27.1" class="ltx_td ltx_align_left">DIVERSE <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib69" title="" class="ltx_ref">2023d</a>)</cite>
</td>
<td id="S2.T1.1.1.28.27.2" class="ltx_td ltx_align_center">Trained Model</td>
<td id="S2.T1.1.1.28.27.3" class="ltx_td ltx_align_center">Scalar</td>
<td id="S2.T1.1.1.28.27.4" class="ltx_td ltx_align_center">Re-Ranking</td>
<td id="S2.T1.1.1.28.27.5" class="ltx_td ltx_align_center">SL</td>
<td id="S2.T1.1.1.28.27.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.28.27.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.28.27.7" class="ltx_td ltx_align_center">Arithmetic Reasoning</td>
</tr>
<tr id="S2.T1.1.1.29.28" class="ltx_tr">
<td id="S2.T1.1.1.29.28.1" class="ltx_td ltx_align_left">PRM <cite class="ltx_cite ltx_citemacro_cite">Lightman et al. (<a href="#bib.bib70" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.29.28.2" class="ltx_td ltx_align_center">Reward Model</td>
<td id="S2.T1.1.1.29.28.3" class="ltx_td ltx_align_center">Scalar</td>
<td id="S2.T1.1.1.29.28.4" class="ltx_td ltx_align_center">Feedback-guided</td>
<td id="S2.T1.1.1.29.28.5" class="ltx_td ltx_align_center">SL</td>
<td id="S2.T1.1.1.29.28.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.29.28.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.29.28.7" class="ltx_td ltx_align_center">Arithmetic Reasoning</td>
</tr>
<tr id="S2.T1.1.1.30.29" class="ltx_tr">
<td id="S2.T1.1.1.30.29.1" class="ltx_td ltx_align_left">DiffusionLM <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib68" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S2.T1.1.1.30.29.2" class="ltx_td ltx_align_center">Trained Model</td>
<td id="S2.T1.1.1.30.29.3" class="ltx_td ltx_align_center">Scalar</td>
<td id="S2.T1.1.1.30.29.4" class="ltx_td ltx_align_center">Feedback-guided</td>
<td id="S2.T1.1.1.30.29.5" class="ltx_td ltx_align_center">SL</td>
<td id="S2.T1.1.1.30.29.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.30.29.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.30.29.7" class="ltx_td ltx_align_center">Controlled Text Generation</td>
</tr>
<tr id="S2.T1.1.1.31.30" class="ltx_tr">
<td id="S2.T1.1.1.31.30.1" class="ltx_td ltx_align_left">Fudge <cite class="ltx_cite ltx_citemacro_cite">Yang and Klein (<a href="#bib.bib133" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S2.T1.1.1.31.30.2" class="ltx_td ltx_align_center">Trained Model</td>
<td id="S2.T1.1.1.31.30.3" class="ltx_td ltx_align_center">Scalar</td>
<td id="S2.T1.1.1.31.30.4" class="ltx_td ltx_align_center">Feedback-guided</td>
<td id="S2.T1.1.1.31.30.5" class="ltx_td ltx_align_center">SL</td>
<td id="S2.T1.1.1.31.30.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.31.30.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.31.30.7" class="ltx_td ltx_align_center">Controlled Text Generation</td>
</tr>
<tr id="S2.T1.1.1.32.31" class="ltx_tr">
<td id="S2.T1.1.1.32.31.1" class="ltx_td ltx_align_left">Entailer <cite class="ltx_cite ltx_citemacro_cite">Tafjord et al. (<a href="#bib.bib109" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S2.T1.1.1.32.31.2" class="ltx_td ltx_align_center">Trained Model</td>
<td id="S2.T1.1.1.32.31.3" class="ltx_td ltx_align_center">Scalar</td>
<td id="S2.T1.1.1.32.31.4" class="ltx_td ltx_align_center">Feedback-guided</td>
<td id="S2.T1.1.1.32.31.5" class="ltx_td ltx_align_center">SL</td>
<td id="S2.T1.1.1.32.31.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.32.31.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.32.31.7" class="ltx_td ltx_align_center">Proof Generation</td>
</tr>
<tr id="S2.T1.1.1.33.32" class="ltx_tr">
<td id="S2.T1.1.1.33.32.1" class="ltx_td ltx_align_left">NLProofS <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib132" title="" class="ltx_ref">2022a</a>)</cite>
</td>
<td id="S2.T1.1.1.33.32.2" class="ltx_td ltx_align_center">Trained Model</td>
<td id="S2.T1.1.1.33.32.3" class="ltx_td ltx_align_center">Scalar</td>
<td id="S2.T1.1.1.33.32.4" class="ltx_td ltx_align_center">Feedback-guided</td>
<td id="S2.T1.1.1.33.32.5" class="ltx_td ltx_align_center">SL</td>
<td id="S2.T1.1.1.33.32.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.33.32.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.33.32.7" class="ltx_td ltx_align_center">Proof Generation</td>
</tr>
<tr id="S2.T1.1.1.34.33" class="ltx_tr">
<td id="S2.T1.1.1.34.33.1" class="ltx_td ltx_align_left">GRACE <cite class="ltx_cite ltx_citemacro_cite">Khalifa et al. (<a href="#bib.bib53" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.34.33.2" class="ltx_td ltx_align_center">Trained Model</td>
<td id="S2.T1.1.1.34.33.3" class="ltx_td ltx_align_center">Scalar</td>
<td id="S2.T1.1.1.34.33.4" class="ltx_td ltx_align_center">Feedback-guided</td>
<td id="S2.T1.1.1.34.33.5" class="ltx_td ltx_align_center">SL</td>
<td id="S2.T1.1.1.34.33.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.34.33.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.34.33.7" class="ltx_td ltx_align_center">Arithmetic Reasoning</td>
</tr>
<tr id="S2.T1.1.1.35.34" class="ltx_tr">
<td id="S2.T1.1.1.35.34.1" class="ltx_td ltx_align_left">CoRe <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib145" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.35.34.2" class="ltx_td ltx_align_center">Trained Model</td>
<td id="S2.T1.1.1.35.34.3" class="ltx_td ltx_align_center">Scalar</td>
<td id="S2.T1.1.1.35.34.4" class="ltx_td ltx_align_center">Feedback-guided</td>
<td id="S2.T1.1.1.35.34.5" class="ltx_td ltx_align_center">SL</td>
<td id="S2.T1.1.1.35.34.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.35.34.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.35.34.7" class="ltx_td ltx_align_center">Arithmetic Reasoning</td>
</tr>
<tr id="S2.T1.1.1.36.35" class="ltx_tr">
<td id="S2.T1.1.1.36.35.1" class="ltx_td ltx_align_left">Maieutic Prompting <cite class="ltx_cite ltx_citemacro_cite">Jung et al. (<a href="#bib.bib48" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S2.T1.1.1.36.35.2" class="ltx_td ltx_align_center">External Metrics</td>
<td id="S2.T1.1.1.36.35.3" class="ltx_td ltx_align_center">Scalar</td>
<td id="S2.T1.1.1.36.35.4" class="ltx_td ltx_align_center">Feedback-guided</td>
<td id="S2.T1.1.1.36.35.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.36.35.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.36.35.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.36.35.7" class="ltx_td ltx_align_center">Commonsense Reasoning</td>
</tr>
<tr id="S2.T1.1.1.37.36" class="ltx_tr">
<td id="S2.T1.1.1.37.36.1" class="ltx_td ltx_align_left">SI <cite class="ltx_cite ltx_citemacro_cite">Creswell and Shanahan (<a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S2.T1.1.1.37.36.2" class="ltx_td ltx_align_center">Language Model</td>
<td id="S2.T1.1.1.37.36.3" class="ltx_td ltx_align_center">Scalar</td>
<td id="S2.T1.1.1.37.36.4" class="ltx_td ltx_align_center">Feedback-guided</td>
<td id="S2.T1.1.1.37.36.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.37.36.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.37.36.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.37.36.7" class="ltx_td ltx_align_center">Proof Generation</td>
</tr>
<tr id="S2.T1.1.1.38.37" class="ltx_tr">
<td id="S2.T1.1.1.38.37.1" class="ltx_td ltx_align_left">RAP <cite class="ltx_cite ltx_citemacro_cite">Hao et al. (<a href="#bib.bib40" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.38.37.2" class="ltx_td ltx_align_center">Language Model</td>
<td id="S2.T1.1.1.38.37.3" class="ltx_td ltx_align_center">Scalar</td>
<td id="S2.T1.1.1.38.37.4" class="ltx_td ltx_align_center">Feedback-guided</td>
<td id="S2.T1.1.1.38.37.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.38.37.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.38.37.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.38.37.7" class="ltx_td ltx_align_center">Planning, Reasoning</td>
</tr>
<tr id="S2.T1.1.1.39.38" class="ltx_tr">
<td id="S2.T1.1.1.39.38.1" class="ltx_td ltx_align_left">SelfEval-Decoding <cite class="ltx_cite ltx_citemacro_cite">Xie et al. (<a href="#bib.bib126" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.39.38.2" class="ltx_td ltx_align_center">Language Model</td>
<td id="S2.T1.1.1.39.38.3" class="ltx_td ltx_align_center">Scalar</td>
<td id="S2.T1.1.1.39.38.4" class="ltx_td ltx_align_center">Feedback-guided</td>
<td id="S2.T1.1.1.39.38.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.39.38.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.39.38.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.39.38.7" class="ltx_td ltx_align_center">Arithmetic / Symbolic Reasoning</td>
</tr>
<tr id="S2.T1.1.1.40.39" class="ltx_tr">
<td id="S2.T1.1.1.40.39.1" class="ltx_td ltx_align_left">Tree of Thoughts <cite class="ltx_cite ltx_citemacro_cite">Yao et al. (<a href="#bib.bib136" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S2.T1.1.1.40.39.2" class="ltx_td ltx_align_center">Language Model</td>
<td id="S2.T1.1.1.40.39.3" class="ltx_td ltx_align_center">NL / Scalar</td>
<td id="S2.T1.1.1.40.39.4" class="ltx_td ltx_align_center">Feedback-guided</td>
<td id="S2.T1.1.1.40.39.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.40.39.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.40.39.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.40.39.7" class="ltx_td ltx_align_center">Games, Writing</td>
</tr>
<tr id="S2.T1.1.1.41.40" class="ltx_tr" style="background-color:#E0FFFF;">
<td id="S2.T1.1.1.41.40.1" class="ltx_td ltx_align_center ltx_border_t" colspan="7"><span id="S2.T1.1.1.41.40.1.1" class="ltx_text ltx_font_bold" style="background-color:#E0FFFF;">Post-hoc Correction<span id="S2.T1.1.1.41.40.1.1.1" class="ltx_text ltx_font_medium"></span></span></td>
</tr>
<tr id="S2.T1.1.1.42.41" class="ltx_tr">
<td id="S2.T1.1.1.42.41.1" class="ltx_td ltx_align_left ltx_border_t">Self-Refine <cite class="ltx_cite ltx_citemacro_cite">Madaan et al. (<a href="#bib.bib79" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.42.41.2" class="ltx_td ltx_align_center ltx_border_t">Language Model</td>
<td id="S2.T1.1.1.42.41.3" class="ltx_td ltx_align_center ltx_border_t">NL</td>
<td id="S2.T1.1.1.42.41.4" class="ltx_td ltx_align_center ltx_border_t">Self-Refine</td>
<td id="S2.T1.1.1.42.41.5" class="ltx_td ltx_align_center ltx_border_t">ICL</td>
<td id="S2.T1.1.1.42.41.6" class="ltx_td ltx_align_center ltx_border_t">
<span id="S2.T1.1.1.42.41.6.1" class="ltx_ERROR undefined">\usym</span>2713</td>
<td id="S2.T1.1.1.42.41.7" class="ltx_td ltx_align_center ltx_border_t">Multiple Tasks</td>
</tr>
<tr id="S2.T1.1.1.43.42" class="ltx_tr">
<td id="S2.T1.1.1.43.42.1" class="ltx_td ltx_align_left">Clinical SV <cite class="ltx_cite ltx_citemacro_cite">Gero et al. (<a href="#bib.bib34" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.43.42.2" class="ltx_td ltx_align_center">Language Model</td>
<td id="S2.T1.1.1.43.42.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.43.42.4" class="ltx_td ltx_align_center">Self-Refine</td>
<td id="S2.T1.1.1.43.42.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.43.42.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.43.42.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.43.42.7" class="ltx_td ltx_align_center">Information Extraction</td>
</tr>
<tr id="S2.T1.1.1.44.43" class="ltx_tr">
<td id="S2.T1.1.1.44.43.1" class="ltx_td ltx_align_left">Reflexion <cite class="ltx_cite ltx_citemacro_cite">Shinn et al. (<a href="#bib.bib104" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.44.43.2" class="ltx_td ltx_align_center">Language Model</td>
<td id="S2.T1.1.1.44.43.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.44.43.4" class="ltx_td ltx_align_center">Self-Refine</td>
<td id="S2.T1.1.1.44.43.5" class="ltx_td ltx_align_center">RL</td>
<td id="S2.T1.1.1.44.43.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.44.43.6.1" class="ltx_ERROR undefined">\usym</span>2713</td>
<td id="S2.T1.1.1.44.43.7" class="ltx_td ltx_align_center">QA, Code Generation</td>
</tr>
<tr id="S2.T1.1.1.45.44" class="ltx_tr">
<td id="S2.T1.1.1.45.44.1" class="ltx_td ltx_align_left">IterRefinement <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib12" title="" class="ltx_ref">2023b</a>)</cite>
</td>
<td id="S2.T1.1.1.45.44.2" class="ltx_td ltx_align_center">Language Model</td>
<td id="S2.T1.1.1.45.44.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.45.44.4" class="ltx_td ltx_align_center">Self-Refine</td>
<td id="S2.T1.1.1.45.44.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.45.44.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.45.44.6.1" class="ltx_ERROR undefined">\usym</span>2713</td>
<td id="S2.T1.1.1.45.44.7" class="ltx_td ltx_align_center">Machine Translation</td>
</tr>
<tr id="S2.T1.1.1.46.45" class="ltx_tr">
<td id="S2.T1.1.1.46.45.1" class="ltx_td ltx_align_left">Auto-Post-Editing <cite class="ltx_cite ltx_citemacro_cite">Raunak et al. (<a href="#bib.bib94" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.46.45.2" class="ltx_td ltx_align_center">Language Model</td>
<td id="S2.T1.1.1.46.45.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.46.45.4" class="ltx_td ltx_align_center">Self-Refine</td>
<td id="S2.T1.1.1.46.45.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.46.45.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.46.45.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.46.45.7" class="ltx_td ltx_align_center">Machine Translation</td>
</tr>
<tr id="S2.T1.1.1.47.46" class="ltx_tr">
<td id="S2.T1.1.1.47.46.1" class="ltx_td ltx_align_left">RCI <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a href="#bib.bib54" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.47.46.2" class="ltx_td ltx_align_center">Language Model</td>
<td id="S2.T1.1.1.47.46.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.47.46.4" class="ltx_td ltx_align_center">Self-Refine</td>
<td id="S2.T1.1.1.47.46.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.47.46.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.47.46.6.1" class="ltx_ERROR undefined">\usym</span>2713</td>
<td id="S2.T1.1.1.47.46.7" class="ltx_td ltx_align_center">Computer Tasks</td>
</tr>
<tr id="S2.T1.1.1.48.47" class="ltx_tr">
<td id="S2.T1.1.1.48.47.1" class="ltx_td ltx_align_left">SelFee <cite class="ltx_cite ltx_citemacro_cite">Ye et al. (<a href="#bib.bib138" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.48.47.2" class="ltx_td ltx_align_center">Language Model</td>
<td id="S2.T1.1.1.48.47.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.48.47.4" class="ltx_td ltx_align_center">Self-Refine</td>
<td id="S2.T1.1.1.48.47.5" class="ltx_td ltx_align_center">SL</td>
<td id="S2.T1.1.1.48.47.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.48.47.6.1" class="ltx_ERROR undefined">\usym</span>2713</td>
<td id="S2.T1.1.1.48.47.7" class="ltx_td ltx_align_center">Dialogue</td>
</tr>
<tr id="S2.T1.1.1.49.48" class="ltx_tr">
<td id="S2.T1.1.1.49.48.1" class="ltx_td ltx_align_left">SelfCheckGPT <cite class="ltx_cite ltx_citemacro_cite">Manakul et al. (<a href="#bib.bib80" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.49.48.2" class="ltx_td ltx_align_center">Language Model</td>
<td id="S2.T1.1.1.49.48.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.49.48.4" class="ltx_td ltx_align_center">Self-Refine</td>
<td id="S2.T1.1.1.49.48.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.49.48.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.49.48.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.49.48.7" class="ltx_td ltx_align_center">Hallucination Detection</td>
</tr>
<tr id="S2.T1.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.1" class="ltx_td ltx_align_left">Re<math id="S2.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="{}^{3}" display="inline"><semantics id="S2.T1.1.1.1.1.m1.1a"><msup id="S2.T1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.m1.1.1.cmml"><mi id="S2.T1.1.1.1.1.m1.1.1a" xref="S2.T1.1.1.1.1.m1.1.1.cmml"></mi><mn id="S2.T1.1.1.1.1.m1.1.1.1" xref="S2.T1.1.1.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.m1.1b"><apply id="S2.T1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.m1.1.1"><cn type="integer" id="S2.T1.1.1.1.1.m1.1.1.1.cmml" xref="S2.T1.1.1.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib134" title="" class="ltx_ref">2022b</a>)</cite>
</td>
<td id="S2.T1.1.1.1.2" class="ltx_td ltx_align_center">Trained Model</td>
<td id="S2.T1.1.1.1.3" class="ltx_td ltx_align_center">Scalar</td>
<td id="S2.T1.1.1.1.4" class="ltx_td ltx_align_center">External Feedback</td>
<td id="S2.T1.1.1.1.5" class="ltx_td ltx_align_center">SL &amp; ICL</td>
<td id="S2.T1.1.1.1.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.1.6.1" class="ltx_ERROR undefined">\usym</span>2713</td>
<td id="S2.T1.1.1.1.7" class="ltx_td ltx_align_center">Story Generation</td>
</tr>
<tr id="S2.T1.1.1.50.49" class="ltx_tr">
<td id="S2.T1.1.1.50.49.1" class="ltx_td ltx_align_left">CodeRL <cite class="ltx_cite ltx_citemacro_cite">Le et al. (<a href="#bib.bib59" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S2.T1.1.1.50.49.2" class="ltx_td ltx_align_center">Trained Model</td>
<td id="S2.T1.1.1.50.49.3" class="ltx_td ltx_align_center">Scalar</td>
<td id="S2.T1.1.1.50.49.4" class="ltx_td ltx_align_center">External Feedback</td>
<td id="S2.T1.1.1.50.49.5" class="ltx_td ltx_align_center">RL</td>
<td id="S2.T1.1.1.50.49.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.50.49.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.50.49.7" class="ltx_td ltx_align_center">Code Generation</td>
</tr>
<tr id="S2.T1.1.1.51.50" class="ltx_tr">
<td id="S2.T1.1.1.51.50.1" class="ltx_td ltx_align_left">REFINER <cite class="ltx_cite ltx_citemacro_cite">Paul et al. (<a href="#bib.bib90" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.51.50.2" class="ltx_td ltx_align_center">Trained Model</td>
<td id="S2.T1.1.1.51.50.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.51.50.4" class="ltx_td ltx_align_center">External Feedback</td>
<td id="S2.T1.1.1.51.50.5" class="ltx_td ltx_align_center">SL &amp; ICL</td>
<td id="S2.T1.1.1.51.50.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.51.50.6.1" class="ltx_ERROR undefined">\usym</span>2713</td>
<td id="S2.T1.1.1.51.50.7" class="ltx_td ltx_align_center">Reasoning, Moral Story</td>
</tr>
<tr id="S2.T1.1.1.52.51" class="ltx_tr">
<td id="S2.T1.1.1.52.51.1" class="ltx_td ltx_align_left">RL4F <cite class="ltx_cite ltx_citemacro_cite">Akyürek et al. (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.52.51.2" class="ltx_td ltx_align_center">Trained Model</td>
<td id="S2.T1.1.1.52.51.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.52.51.4" class="ltx_td ltx_align_center">External Feedback</td>
<td id="S2.T1.1.1.52.51.5" class="ltx_td ltx_align_center">SL &amp; RL</td>
<td id="S2.T1.1.1.52.51.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.52.51.6.1" class="ltx_ERROR undefined">\usym</span>2713</td>
<td id="S2.T1.1.1.52.51.7" class="ltx_td ltx_align_center">Planning, Summarization</td>
</tr>
<tr id="S2.T1.1.1.53.52" class="ltx_tr">
<td id="S2.T1.1.1.53.52.1" class="ltx_td ltx_align_left"><cite class="ltx_cite ltx_citemacro_citet">Yan et al. (<a href="#bib.bib130" title="" class="ltx_ref">2023a</a>)</cite></td>
<td id="S2.T1.1.1.53.52.2" class="ltx_td ltx_align_center">Trained Model</td>
<td id="S2.T1.1.1.53.52.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.53.52.4" class="ltx_td ltx_align_center">External Feedback</td>
<td id="S2.T1.1.1.53.52.5" class="ltx_td ltx_align_center">SL</td>
<td id="S2.T1.1.1.53.52.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.53.52.6.1" class="ltx_ERROR undefined">\usym</span>2713</td>
<td id="S2.T1.1.1.53.52.7" class="ltx_td ltx_align_center">Semantic Parsing</td>
</tr>
<tr id="S2.T1.1.1.54.53" class="ltx_tr">
<td id="S2.T1.1.1.54.53.1" class="ltx_td ltx_align_left">Baldur <cite class="ltx_cite ltx_citemacro_cite">First et al. (<a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.54.53.2" class="ltx_td ltx_align_center">Trained Model</td>
<td id="S2.T1.1.1.54.53.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.54.53.4" class="ltx_td ltx_align_center">External Feedback</td>
<td id="S2.T1.1.1.54.53.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.54.53.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.54.53.6.1" class="ltx_ERROR undefined">\usym</span>2713</td>
<td id="S2.T1.1.1.54.53.7" class="ltx_td ltx_align_center">Proof Generation</td>
</tr>
<tr id="S2.T1.1.1.55.54" class="ltx_tr">
<td id="S2.T1.1.1.55.54.1" class="ltx_td ltx_align_left">CRITIC <cite class="ltx_cite ltx_citemacro_cite">Gou et al. (<a href="#bib.bib38" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.55.54.2" class="ltx_td ltx_align_center">External Tools</td>
<td id="S2.T1.1.1.55.54.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.55.54.4" class="ltx_td ltx_align_center">External Feedback</td>
<td id="S2.T1.1.1.55.54.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.55.54.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.55.54.6.1" class="ltx_ERROR undefined">\usym</span>2713</td>
<td id="S2.T1.1.1.55.54.7" class="ltx_td ltx_align_center">QA, Program, Toxicity</td>
</tr>
<tr id="S2.T1.1.1.56.55" class="ltx_tr">
<td id="S2.T1.1.1.56.55.1" class="ltx_td ltx_align_left">FacTool <cite class="ltx_cite ltx_citemacro_cite">Chern et al. (<a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.56.55.2" class="ltx_td ltx_align_center">External Tools</td>
<td id="S2.T1.1.1.56.55.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.56.55.4" class="ltx_td ltx_align_center">External Feedback</td>
<td id="S2.T1.1.1.56.55.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.56.55.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.56.55.6.1" class="ltx_ERROR undefined">\usym</span>2713</td>
<td id="S2.T1.1.1.56.55.7" class="ltx_td ltx_align_center">QA, Reasoning, Generation</td>
</tr>
<tr id="S2.T1.1.1.57.56" class="ltx_tr">
<td id="S2.T1.1.1.57.56.1" class="ltx_td ltx_align_left">RARR <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a href="#bib.bib32" title="" class="ltx_ref">2023b</a>)</cite>
</td>
<td id="S2.T1.1.1.57.56.2" class="ltx_td ltx_align_center">External Knowledge</td>
<td id="S2.T1.1.1.57.56.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.57.56.4" class="ltx_td ltx_align_center">External Feedback</td>
<td id="S2.T1.1.1.57.56.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.57.56.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.57.56.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.57.56.7" class="ltx_td ltx_align_center">Open-Domain QA</td>
</tr>
<tr id="S2.T1.1.1.58.57" class="ltx_tr">
<td id="S2.T1.1.1.58.57.1" class="ltx_td ltx_align_left">LLM-Augmenter <cite class="ltx_cite ltx_citemacro_cite">Peng et al. (<a href="#bib.bib91" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.58.57.2" class="ltx_td ltx_align_center">External Knowledge</td>
<td id="S2.T1.1.1.58.57.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.58.57.4" class="ltx_td ltx_align_center">External Feedback</td>
<td id="S2.T1.1.1.58.57.5" class="ltx_td ltx_align_center">RL</td>
<td id="S2.T1.1.1.58.57.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.58.57.6.1" class="ltx_ERROR undefined">\usym</span>2713</td>
<td id="S2.T1.1.1.58.57.7" class="ltx_td ltx_align_center">Open-Domain QA</td>
</tr>
<tr id="S2.T1.1.1.59.58" class="ltx_tr">
<td id="S2.T1.1.1.59.58.1" class="ltx_td ltx_align_left">Self-Checker <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib65" title="" class="ltx_ref">2023b</a>)</cite>
</td>
<td id="S2.T1.1.1.59.58.2" class="ltx_td ltx_align_center">External Knowledge</td>
<td id="S2.T1.1.1.59.58.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.59.58.4" class="ltx_td ltx_align_center">External Feedback</td>
<td id="S2.T1.1.1.59.58.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.59.58.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.59.58.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.59.58.7" class="ltx_td ltx_align_center">Fact-Checking</td>
</tr>
<tr id="S2.T1.1.1.60.59" class="ltx_tr">
<td id="S2.T1.1.1.60.59.1" class="ltx_td ltx_align_left">REFEED <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib139" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.60.59.2" class="ltx_td ltx_align_center">External Knowledge</td>
<td id="S2.T1.1.1.60.59.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.60.59.4" class="ltx_td ltx_align_center">External Feedback</td>
<td id="S2.T1.1.1.60.59.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.60.59.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.60.59.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.60.59.7" class="ltx_td ltx_align_center">QA, Dialogue</td>
</tr>
<tr id="S2.T1.1.1.61.60" class="ltx_tr">
<td id="S2.T1.1.1.61.60.1" class="ltx_td ltx_align_left"><cite class="ltx_cite ltx_citemacro_citet">Olausson et al. (<a href="#bib.bib84" title="" class="ltx_ref">2023</a>)</cite></td>
<td id="S2.T1.1.1.61.60.2" class="ltx_td ltx_align_center">Program Executor</td>
<td id="S2.T1.1.1.61.60.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.61.60.4" class="ltx_td ltx_align_center">External Feedback</td>
<td id="S2.T1.1.1.61.60.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.61.60.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.61.60.6.1" class="ltx_ERROR undefined">\usym</span>2713</td>
<td id="S2.T1.1.1.61.60.7" class="ltx_td ltx_align_center">Code Generation</td>
</tr>
<tr id="S2.T1.1.1.62.61" class="ltx_tr">
<td id="S2.T1.1.1.62.61.1" class="ltx_td ltx_align_left">Self-Edit <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib141" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S2.T1.1.1.62.61.2" class="ltx_td ltx_align_center">Program Executor</td>
<td id="S2.T1.1.1.62.61.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.62.61.4" class="ltx_td ltx_align_center">External Feedback</td>
<td id="S2.T1.1.1.62.61.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.62.61.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.62.61.6.1" class="ltx_ERROR undefined">\usym</span>2713</td>
<td id="S2.T1.1.1.62.61.7" class="ltx_td ltx_align_center">Code Generation</td>
</tr>
<tr id="S2.T1.1.1.63.62" class="ltx_tr">
<td id="S2.T1.1.1.63.62.1" class="ltx_td ltx_align_left">Self-Debug <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib13" title="" class="ltx_ref">2023c</a>)</cite>
</td>
<td id="S2.T1.1.1.63.62.2" class="ltx_td ltx_align_center">Program Executor</td>
<td id="S2.T1.1.1.63.62.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.63.62.4" class="ltx_td ltx_align_center">External Feedback</td>
<td id="S2.T1.1.1.63.62.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.63.62.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.63.62.6.1" class="ltx_ERROR undefined">\usym</span>2713</td>
<td id="S2.T1.1.1.63.62.7" class="ltx_td ltx_align_center">Code Generation</td>
</tr>
<tr id="S2.T1.1.1.64.63" class="ltx_tr">
<td id="S2.T1.1.1.64.63.1" class="ltx_td ltx_align_left">Self-Evolve <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.64.63.2" class="ltx_td ltx_align_center">Program Executor</td>
<td id="S2.T1.1.1.64.63.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.64.63.4" class="ltx_td ltx_align_center">External Feedback</td>
<td id="S2.T1.1.1.64.63.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.64.63.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.64.63.6.1" class="ltx_ERROR undefined">\usym</span>2713</td>
<td id="S2.T1.1.1.64.63.7" class="ltx_td ltx_align_center">Code Generation</td>
</tr>
<tr id="S2.T1.1.1.65.64" class="ltx_tr">
<td id="S2.T1.1.1.65.64.1" class="ltx_td ltx_align_left">Logic-LM <cite class="ltx_cite ltx_citemacro_cite">Pan et al. (<a href="#bib.bib88" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.65.64.2" class="ltx_td ltx_align_center">Symbolic Solver</td>
<td id="S2.T1.1.1.65.64.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.65.64.4" class="ltx_td ltx_align_center">External Feedback</td>
<td id="S2.T1.1.1.65.64.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.65.64.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.65.64.6.1" class="ltx_ERROR undefined">\usym</span>2713</td>
<td id="S2.T1.1.1.65.64.7" class="ltx_td ltx_align_center">Logical Reasoning</td>
</tr>
<tr id="S2.T1.1.1.66.65" class="ltx_tr">
<td id="S2.T1.1.1.66.65.1" class="ltx_td ltx_align_left">Self-Critique <cite class="ltx_cite ltx_citemacro_cite">Saunders et al. (<a href="#bib.bib96" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S2.T1.1.1.66.65.2" class="ltx_td ltx_align_center">LLMs + Human</td>
<td id="S2.T1.1.1.66.65.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.66.65.4" class="ltx_td ltx_align_center">External Feedback</td>
<td id="S2.T1.1.1.66.65.5" class="ltx_td ltx_align_center">SL</td>
<td id="S2.T1.1.1.66.65.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.66.65.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.66.65.7" class="ltx_td ltx_align_center">Summarization</td>
</tr>
<tr id="S2.T1.1.1.67.66" class="ltx_tr">
<td id="S2.T1.1.1.67.66.1" class="ltx_td ltx_align_left">ALGO <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib142" title="" class="ltx_ref">2023b</a>)</cite>
</td>
<td id="S2.T1.1.1.67.66.2" class="ltx_td ltx_align_center">Oracle Verifier</td>
<td id="S2.T1.1.1.67.66.3" class="ltx_td ltx_align_center">Scalar</td>
<td id="S2.T1.1.1.67.66.4" class="ltx_td ltx_align_center">External Feedback</td>
<td id="S2.T1.1.1.67.66.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.67.66.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.67.66.6.1" class="ltx_ERROR undefined">\usym</span>2713</td>
<td id="S2.T1.1.1.67.66.7" class="ltx_td ltx_align_center">Code Generation</td>
</tr>
<tr id="S2.T1.1.1.68.67" class="ltx_tr">
<td id="S2.T1.1.1.68.67.1" class="ltx_td ltx_align_left"><cite class="ltx_cite ltx_citemacro_citet">Charalambous et al. (<a href="#bib.bib10" title="" class="ltx_ref">2023</a>)</cite></td>
<td id="S2.T1.1.1.68.67.2" class="ltx_td ltx_align_center">Bounded Model Checker</td>
<td id="S2.T1.1.1.68.67.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.68.67.4" class="ltx_td ltx_align_center">External Feedback</td>
<td id="S2.T1.1.1.68.67.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.68.67.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.68.67.6.1" class="ltx_ERROR undefined">\usym</span>2717</td>
<td id="S2.T1.1.1.68.67.7" class="ltx_td ltx_align_center">Software Verification</td>
</tr>
<tr id="S2.T1.1.1.69.68" class="ltx_tr">
<td id="S2.T1.1.1.69.68.1" class="ltx_td ltx_align_left">Self-Correction <cite class="ltx_cite ltx_citemacro_cite">Welleck et al. (<a href="#bib.bib121" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.69.68.2" class="ltx_td ltx_align_center">External Metrics</td>
<td id="S2.T1.1.1.69.68.3" class="ltx_td ltx_align_center">NL / Scalar</td>
<td id="S2.T1.1.1.69.68.4" class="ltx_td ltx_align_center">External Feedback</td>
<td id="S2.T1.1.1.69.68.5" class="ltx_td ltx_align_center">SL</td>
<td id="S2.T1.1.1.69.68.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.69.68.6.1" class="ltx_ERROR undefined">\usym</span>2713</td>
<td id="S2.T1.1.1.69.68.7" class="ltx_td ltx_align_center">Reasoning, Generation, Toxicity</td>
</tr>
<tr id="S2.T1.1.1.70.69" class="ltx_tr">
<td id="S2.T1.1.1.70.69.1" class="ltx_td ltx_align_left">Multiagent Debate <cite class="ltx_cite ltx_citemacro_cite">Du et al. (<a href="#bib.bib24" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.70.69.2" class="ltx_td ltx_align_center">Language Model</td>
<td id="S2.T1.1.1.70.69.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.70.69.4" class="ltx_td ltx_align_center">Model Debate</td>
<td id="S2.T1.1.1.70.69.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.70.69.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.70.69.6.1" class="ltx_ERROR undefined">\usym</span>2713</td>
<td id="S2.T1.1.1.70.69.7" class="ltx_td ltx_align_center">Reasoning, Factuality</td>
</tr>
<tr id="S2.T1.1.1.71.70" class="ltx_tr">
<td id="S2.T1.1.1.71.70.1" class="ltx_td ltx_align_left">LM vs LM <cite class="ltx_cite ltx_citemacro_cite">Cohen et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.71.70.2" class="ltx_td ltx_align_center">Language Model</td>
<td id="S2.T1.1.1.71.70.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.71.70.4" class="ltx_td ltx_align_center">Model Debate</td>
<td id="S2.T1.1.1.71.70.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.71.70.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.71.70.6.1" class="ltx_ERROR undefined">\usym</span>2713</td>
<td id="S2.T1.1.1.71.70.7" class="ltx_td ltx_align_center">Factual Error Detection</td>
</tr>
<tr id="S2.T1.1.1.72.71" class="ltx_tr">
<td id="S2.T1.1.1.72.71.1" class="ltx_td ltx_align_left">ICL-AIF <cite class="ltx_cite ltx_citemacro_cite">Fu et al. (<a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.72.71.2" class="ltx_td ltx_align_center">Language Model</td>
<td id="S2.T1.1.1.72.71.3" class="ltx_td ltx_align_center">NL</td>
<td id="S2.T1.1.1.72.71.4" class="ltx_td ltx_align_center">Model Debate</td>
<td id="S2.T1.1.1.72.71.5" class="ltx_td ltx_align_center">ICL</td>
<td id="S2.T1.1.1.72.71.6" class="ltx_td ltx_align_center">
<span id="S2.T1.1.1.72.71.6.1" class="ltx_ERROR undefined">\usym</span>2713</td>
<td id="S2.T1.1.1.72.71.7" class="ltx_td ltx_align_center">Bargaining Game</td>
</tr>
<tr id="S2.T1.1.1.73.72" class="ltx_tr">
<td id="S2.T1.1.1.73.72.1" class="ltx_td ltx_align_left ltx_border_bb">PRD <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib66" title="" class="ltx_ref">2023c</a>)</cite>
</td>
<td id="S2.T1.1.1.73.72.2" class="ltx_td ltx_align_center ltx_border_bb">Language Model</td>
<td id="S2.T1.1.1.73.72.3" class="ltx_td ltx_align_center ltx_border_bb">NL</td>
<td id="S2.T1.1.1.73.72.4" class="ltx_td ltx_align_center ltx_border_bb">Model Debate</td>
<td id="S2.T1.1.1.73.72.5" class="ltx_td ltx_align_center ltx_border_bb">ICL</td>
<td id="S2.T1.1.1.73.72.6" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S2.T1.1.1.73.72.6.1" class="ltx_ERROR undefined">\usym</span>2713</td>
<td id="S2.T1.1.1.73.72.7" class="ltx_td ltx_align_center ltx_border_bb">Open-ended QA</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>A summary of representative works on correcting large language models with automated feedback. We summarize the key features for each work: the <span id="S2.T1.8.1" class="ltx_text ltx_font_bold">source</span> and <span id="S2.T1.9.2" class="ltx_text ltx_font_bold">format</span> of the feedback, the <span id="S2.T1.10.3" class="ltx_text ltx_font_bold">strategy</span> and <span id="S2.T1.11.4" class="ltx_text ltx_font_bold">learning</span> method of the refinement process, whether the refinement is iterative <span id="S2.T1.12.5" class="ltx_text ltx_font_bold">(Iter.)</span>, and the <span id="S2.T1.13.6" class="ltx_text ltx_font_bold">application</span> of the method.</figcaption>
</figure>
<figure id="S2.F2" class="ltx_figure"><img src="x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="872" height="283" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Three typical strategies of <span id="S2.F2.2.1" class="ltx_text ltx_font_italic">training-time correction</span>: directly optimization with human feedback (a), training a reward model that approximates human feedback (b), and self-training with automated feedback (c).</figcaption>
</figure>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span><span id="S2.SS5.1.1" class="ltx_text ltx_font_italic">When</span> to correct the model with feedback?</h3>

<div id="S2.SS5.p1" class="ltx_para">
<p id="S2.SS5.p1.1" class="ltx_p">Depending on the timing of using automated feedback to correct the model, existing works can be divided into three major categories.</p>
</div>
<div id="S2.SS5.p2" class="ltx_para ltx_noindent">
<p id="S2.SS5.p2.1" class="ltx_p"><math id="S2.SS5.p2.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS5.p2.1.m1.1a"><mo id="S2.SS5.p2.1.m1.1.1" xref="S2.SS5.p2.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS5.p2.1.m1.1b"><ci id="S2.SS5.p2.1.m1.1.1.cmml" xref="S2.SS5.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p2.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS5.p2.1.m1.1d">∙</annotation></semantics></math> <span id="S2.SS5.p2.1.1" class="ltx_text ltx_font_bold">Training-time Correction.</span>
The ideal scenario is to rectify a flawed model during training, prior to its deployment for use. Once feedback has been collected, it is directly used to optimize the model parameters. Human feedback is typically used for training-time correction, as exemplified by the widely adopted RLHF approach <cite class="ltx_cite ltx_citemacro_cite">Ouyang et al. (<a href="#bib.bib87" title="" class="ltx_ref">2022</a>)</cite>. For leveraging automated feedback, a common strategy is <span id="S2.SS5.p2.1.2" class="ltx_text ltx_font_italic">self-training</span> <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a href="#bib.bib45" title="" class="ltx_ref">2022</a>)</cite>, where the model is trained with its own generated high-quality output filtered out by the critic model.
While training-time correction is a pre-hoc strategy that addresses problems during training, its practical application may be hindered by three issues: 1) the infeasibility of fine-tuning gaint closed-source LLMs, such as GPT-4 <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib86" title="" class="ltx_ref">2023</a>)</cite>, 2) the potential unavailability of feedback during model training, and 3) the requirement for the feedback to be “optimizable”, <span id="S2.SS5.p2.1.3" class="ltx_text ltx_font_italic">e.g.</span>, a numerical score severing as the basis for model optimization.
</p>
</div>
<div id="S2.SS5.p3" class="ltx_para ltx_noindent">
<p id="S2.SS5.p3.1" class="ltx_p"><math id="S2.SS5.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS5.p3.1.m1.1a"><mo id="S2.SS5.p3.1.m1.1.1" xref="S2.SS5.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS5.p3.1.m1.1b"><ci id="S2.SS5.p3.1.m1.1.1.cmml" xref="S2.SS5.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p3.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS5.p3.1.m1.1d">∙</annotation></semantics></math> <span id="S2.SS5.p3.1.1" class="ltx_text ltx_font_bold">Generation-time Correction.</span> This strategy utilizes automated feedback to guide the language model <span id="S2.SS5.p3.1.2" class="ltx_text ltx_font_italic">during</span> generation, which allows the model to correct errors in its outputs as it is being generated. For example, for proof generation, several works utilize the automated feedback of the intermediate reasoning steps to guide the model to recover from incorrect generation and search for the optimal solution in a more efficient way <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib132" title="" class="ltx_ref">2022a</a>); Lightman et al. (<a href="#bib.bib70" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S2.SS5.p4" class="ltx_para ltx_noindent">
<p id="S2.SS5.p4.1" class="ltx_p"><math id="S2.SS5.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS5.p4.1.m1.1a"><mo id="S2.SS5.p4.1.m1.1.1" xref="S2.SS5.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS5.p4.1.m1.1b"><ci id="S2.SS5.p4.1.m1.1.1.cmml" xref="S2.SS5.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p4.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS5.p4.1.m1.1d">∙</annotation></semantics></math> <span id="S2.SS5.p4.1.1" class="ltx_text ltx_font_bold">Post-hoc Correction.</span>
Finally, post-hoc correction involves refining the model output <span id="S2.SS5.p4.1.2" class="ltx_text ltx_font_italic">after it has been generated</span>, without updating the model parameters. This typically involves an iterative process of generating output, receiving feedback, and refining output. Post-hoc correction provides more flexibility than the previous two strategies as it does not require training the LLM or accessing its parameters. Furthermore, post-hoc correction enhances explainability as it facilitates the incorporation of more informative natural language feedback. This allows for a more transparent visualization and interpretation of the self-correction process.</p>
</div>
</section>
<section id="S2.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.6 </span><span id="S2.SS6.1.1" class="ltx_text ltx_font_italic">How</span> to correct the model with feedback?</h3>

<div id="S2.SS6.p1" class="ltx_para">
<p id="S2.SS6.p1.1" class="ltx_p">Various concrete strategies have been proposed to correct LLMs with automated feedback, which are tailored to the different dimensions we mentioned in previous sections. For example, <span id="S2.SS6.p1.1.1" class="ltx_text ltx_font_italic">self-training</span> is often used for training-time correction. <span id="S2.SS6.p1.1.2" class="ltx_text ltx_font_italic">Generate-then-rank</span> often comes with scalar value feedback. We will cover typical existing strategies for model correction through Section <a href="#S3" title="3 Training-Time Correction ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> to Section <a href="#S5" title="5 Post-hoc Correction ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
<section id="S2.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.7 </span>Summary of existing works</h3>

<div id="S2.SS7.p1" class="ltx_para">
<p id="S2.SS7.p1.1" class="ltx_p">Building upon the taxonomy established in the preceding sections, we collate existing works on correcting LLMs with (automated) feedback in Table <a href="#S2.T1" title="Table 1 ‣ 2.4 What is the format of the feedback? ‣ 2 A Taxonomy for Correcting LLMs with Automated Feedback ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We have two major selection criteria for a work to be included in this survey:</p>
</div>
<div id="S2.SS7.p2" class="ltx_para">
<p id="S2.SS7.p2.1" class="ltx_p">1. <span id="S2.SS7.p2.1.1" class="ltx_text ltx_font_bold">Automated Feedback</span>: Explicit feedback is involved to assess the quality of the model output. We focus on automated feedback which originates from external models, metrics, knowledge, etc. However, we will cover some representative works of human feedback for completeness.</p>
</div>
<div id="S2.SS7.p3" class="ltx_para">
<p id="S2.SS7.p3.1" class="ltx_p">2. <span id="S2.SS7.p3.1.1" class="ltx_text ltx_font_bold">Model Refinement</span>: The feedback should act as a directive to enhance the LLM, either by: 1) updating model parameters, or 2) altering the model’s output during or post the generation process.</p>
</div>
<div id="S2.SS7.p4" class="ltx_para">
<p id="S2.SS7.p4.1" class="ltx_p">These works are categorized based on the three strategies introduced in Section <a href="#S2.SS5" title="2.5 When to correct the model with feedback? ‣ 2 A Taxonomy for Correcting LLMs with Automated Feedback ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.5</span></a>. We also summarize key features of each work, including: 1) the source of feedback, 2) the format of feedback, 3) the strategy and learning method employed for the refinement, 4) whether the refinement process is iterative, and 5) the application of the method. Subsequently, we will delve into a detailed review of each method type, encompassing <span id="S2.SS7.p4.1.1" class="ltx_text ltx_font_italic">Training-Time Correction</span> (§ <a href="#S3" title="3 Training-Time Correction ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), <span id="S2.SS7.p4.1.2" class="ltx_text ltx_font_italic">Generation-Time Correction</span> (§ <a href="#S4" title="4 Generation-Time Correction ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), and <span id="S2.SS7.p4.1.3" class="ltx_text ltx_font_italic">Post-hoc Correction</span> (§ <a href="#S5" title="5 Post-hoc Correction ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Training-Time Correction</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we delve into methodologies that rectify model behavior during the training phase. As depicted in Figure <a href="#S2.F2" title="Figure 2 ‣ 2.4 What is the format of the feedback? ‣ 2 A Taxonomy for Correcting LLMs with Automated Feedback ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we identify three typical strategies for training-time correction. Each strategy utilizes different forms of feedback to modify the model parameters during training: human feedback (a), a reward model that approximates human feedback (b), and automated feedback (c).</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Learning from Human Feedback</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The next-word prediction objective of LLM pre-training is not inherently designed to encapsulate human values or preferences. This misalignment can lead to unintended consequences, such as the generation of harmful, misleading, or biased content <cite class="ltx_cite ltx_citemacro_cite">Kenton et al. (<a href="#bib.bib51" title="" class="ltx_ref">2021</a>)</cite>. To mitigate these issues, many research efforts have explored the integration of human feedback to better align LLMs with human values and expectations, which are extensively reviewed by <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a href="#bib.bib118" title="" class="ltx_ref">2023e</a>)</cite> and  <cite class="ltx_cite ltx_citemacro_citet">Fernandes et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite>. Our survey, however, focuses on automated feedback, thus we will only touch upon representative works in this direction.
</p>
</div>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Direct Optimization with Human Feedback.</h4>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">In an ideal scenario, we would directly leverage human feedback to optimize the model parameters. Typically, this approach follows the framework depicted in Figure <a href="#prehoc_methods" title="Figure 2 ‣ Figure 2 ‣ 2.4 What is the format of the feedback? ‣ 2 A Taxonomy for Correcting LLMs with Automated Feedback ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></span>(a)</a>: 1) Candidate outputs are generated by LLMs, 2) Humans provide feedback or refinements on these outputs, and 3) LLMs are then directly optimized on the collected (outputs, feedback) to better align with human preferences. A simple strategy is to fine-tune the model on the outputs with positively-labeled feedback. For example, <span id="S3.SS1.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">Sparrow</span> <cite class="ltx_cite ltx_citemacro_cite">Glaese et al. (<a href="#bib.bib35" title="" class="ltx_ref">2022</a>)</cite> fine-tunes LLMs on the collected dialogues rated as preferred and rule compliant (concerning correctness, harmfulness, and helpfulness), according to humans. Similarly, <cite class="ltx_cite ltx_citemacro_citet">Scheurer et al. (<a href="#bib.bib98" title="" class="ltx_ref">2023</a>)</cite> utilizes an LLM to generate multiple refinements of the original output based on human feedback, and then the best refinement is picked up to finetune the original LLM. However, relying solely on positive-rated data may constrain the model’s ability to identify and correct negative attributes or errors. To address this, <span id="S3.SS1.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_italic">Chain-of-Hindsight</span> <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib74" title="" class="ltx_ref">2023</a>)</cite> fine-tunes the LLM on model outputs paired with both positive and negative feedback. <cite class="ltx_cite ltx_citemacro_citet">Gao et al. (<a href="#bib.bib31" title="" class="ltx_ref">2023a</a>)</cite> utilizes human feedback as the reward signal and optimizes the model with contextual bandit learning.
</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Reward Modeling and RLHF.</h4>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">Employing human feedback directly to rectify model behavior may not always be practical. The collection of human feedback can be both labor-intensive and time-consuming. An efficient alternative is to train a <span id="S3.SS1.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">reward model</span> that emulates human feedback. Once trained, this reward model can provide consistent, real-time feedback for every model output, thereby circumventing the need for constant human involvement. A prominent example of this approach is Reinforcement Learning from Human Feedback (RLHF) <cite class="ltx_cite ltx_citemacro_cite">Ouyang et al. (<a href="#bib.bib87" title="" class="ltx_ref">2022</a>)</cite>, as illustrated in Figure <a href="#prehoc_methods" title="Figure 2 ‣ Figure 2 ‣ 2.4 What is the format of the feedback? ‣ 2 A Taxonomy for Correcting LLMs with Automated Feedback ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></span>(b)</a>. It first asks human annotators to label the preference for different LLM outputs and then train the reward model to predict the human preference. Afterward, reinforcement learning (RL) algorithms (<span id="S3.SS1.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">e.g.</span>, Proximal Policy Optimization (PPO) <cite class="ltx_cite ltx_citemacro_cite">Schulman et al. (<a href="#bib.bib100" title="" class="ltx_ref">2017</a>)</cite>) are employed to optimize the model. RLHF and its variants have proven effective in correcting LLMs to become more beneficial and less harmful <cite class="ltx_cite ltx_citemacro_cite">Bai et al. (<a href="#bib.bib3" title="" class="ltx_ref">2022a</a>)</cite>, as well as instilling moral correctness <cite class="ltx_cite ltx_citemacro_cite">Ganguli et al. (<a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="409" height="456" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The illustrations of the two typical strategies of <span id="S3.F3.2.1" class="ltx_text ltx_font_italic">generation-time correction</span>: (a) Generate-then-Rank, and (b) Feedback-Guided Decoding.</figcaption>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Learning with Automated Feedback</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Since collecting human feedback is quite resource-intensive, numerous studies have explored the use of automated feedback to minimize the demand for human intervention.
To differentiate between human and automated feedback, we define human feedback as a quality assessment performed by human evaluators on the outputs generated by the base model. This feedback is then used for either direct optimization or reward model learning (Section <a href="#S3.SS1" title="3.1 Learning from Human Feedback ‣ 3 Training-Time Correction ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>). On the other hand, automated feedback is collected in an offline environment, without the need for human assessment of model outputs. We mainly discuss training time strategies utilizing two types of automated feedback: <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">extrinsic feedback</span> from external metrics/models, and <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_italic">intrinsic feedback</span> from the language model itself.</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">External Metric Guidance.</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">Feedback provided by external metrics has been frequently used for training-time correction. Due to the discrete nature of metric signals, most approaches focus on non-differentiable training techniques. Minimum risk training <cite class="ltx_cite ltx_citemacro_cite">Shen et al. (<a href="#bib.bib103" title="" class="ltx_ref">2016</a>)</cite> optimizes model parameters with external evaluation metrics <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a href="#bib.bib128" title="" class="ltx_ref">2022</a>, <a href="#bib.bib127" title="" class="ltx_ref">2023a</a>)</cite>, by incorporating metric score with maximum log-likelihood in the loss function. It can optimize metric scores during training time. However, it can lead the model to the robustness deficiencies of some metrics <cite class="ltx_cite ltx_citemacro_cite">Yan et al. (<a href="#bib.bib131" title="" class="ltx_ref">2023b</a>)</cite>, such as BLEURT <cite class="ltx_cite ltx_citemacro_cite">Sellam et al. (<a href="#bib.bib101" title="" class="ltx_ref">2020</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Liu and Liu (<a href="#bib.bib75" title="" class="ltx_ref">2021</a>)</cite> leverages a contrastive learning framework to rerank candidates based on metric scores, which bridges the gap between training and inference objectives. <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a href="#bib.bib67" title="" class="ltx_ref">2019</a>)</cite> employs deep RL algorithm and <cite class="ltx_cite ltx_citemacro_citet">Unanue et al. (<a href="#bib.bib112" title="" class="ltx_ref">2021</a>)</cite> leverage Gumbel softmax <cite class="ltx_cite ltx_citemacro_cite">Jang et al. (<a href="#bib.bib46" title="" class="ltx_ref">2017</a>)</cite> to build distributional semantic reward from BERTScore <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib144" title="" class="ltx_ref">2020</a>)</cite> and mitigate exposure bias. To stabilize gradients, <cite class="ltx_cite ltx_citemacro_citet">Wu et al. (<a href="#bib.bib123" title="" class="ltx_ref">2021</a>)</cite> utilizes contrastive discriminator and PPO to imitate human texts. Recently, <cite class="ltx_cite ltx_citemacro_citet">Chang et al. (<a href="#bib.bib9" title="" class="ltx_ref">2023</a>)</cite> propose a more efficient RL algorithm, RLGF, than PPO <cite class="ltx_cite ltx_citemacro_cite">Schulman et al. (<a href="#bib.bib100" title="" class="ltx_ref">2017</a>)</cite> to finetune LLM with pre-defined reward. They integrate a reasonable but incomplete guide policy into a policy gradient framework and learn a near-optimal strategy. Different from leveraging feedback solely at fine-tuning, <cite class="ltx_cite ltx_citemacro_citet">Korbak et al. (<a href="#bib.bib57" title="" class="ltx_ref">2023</a>)</cite> employs conditional training <cite class="ltx_cite ltx_citemacro_cite">Keskar et al. (<a href="#bib.bib52" title="" class="ltx_ref">2019</a>)</cite> and an automated classifier to tag undesirable contents at the pretraining stage.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Self-Training.</h4>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.2" class="ltx_p">Instead of leveraging external metrics as feedback, the language model itself can be used to provide feedback for its own output. This gives rise to the <span id="S3.SS2.SSS0.Px2.p1.2.1" class="ltx_text ltx_font_italic">self-training</span> strategy of self-improving LLM by bootstrapping its original outputs, as depicted in Figure <a href="#prehoc_methods" title="Figure 2 ‣ Figure 2 ‣ 2.4 What is the format of the feedback? ‣ 2 A Taxonomy for Correcting LLMs with Automated Feedback ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></span>(c)</a>. STaR <cite class="ltx_cite ltx_citemacro_cite">Zelikman et al. (<a href="#bib.bib140" title="" class="ltx_ref">2022</a>)</cite> leverages the idea of CoT by prompting LLM to generate answers with rationales. By selecting rationales leading to the correct answer to further finetune LLM, the performance of LLM is improved. This process can be iterated with further performance gains. <cite class="ltx_cite ltx_citemacro_citet">Huang et al. (<a href="#bib.bib45" title="" class="ltx_ref">2022</a>)</cite> follows this idea by applying self-consistency <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib116" title="" class="ltx_ref">2023c</a>)</cite> to majority vote reasoning paths (the paths which lead to the most voted answers). LLM is finetuned over selected reasoning-answer data with augmented prompts. This strategy has also been used to reduce the harmful responses of LLMs. RLAIF <cite class="ltx_cite ltx_citemacro_cite">Bai et al. (<a href="#bib.bib4" title="" class="ltx_ref">2022b</a>)</cite> adopted the strategy of critique <math id="S3.SS2.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.1.m1.1a"><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.1.m1.1d">→</annotation></semantics></math> revision <math id="S3.SS2.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.2.m2.1a"><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.2.m2.1b"><ci id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.2.m2.1d">→</annotation></semantics></math> supervised learning. The initial toxic responses are criticized and revised by the LLM itself following a set of human-defined principles. Afterward, the LLM is fine-tuned on the revised responses. AlpacaFarm <cite class="ltx_cite ltx_citemacro_cite">Dubois et al. (<a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite> further shows that LLMs can self-improve with RL. It designs LLM prompts to simulate human feedback in RLHF and shows that the feedback is effective and it greatly reduces the cost.
</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Generation-Time Correction</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Correcting LLMs at training time appears to be the ideal solution given the principle of “an ounce of prevention is worth a pound of cure”. However, there is no guarantee that all undesired behavior can be solved at training time. Moreover, training-time correction might be excessively resource-intensive or even impractical for many LLMs, <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">e.g.</span>, closed-source LLMs where weights are inaccessible, and colossal LLMs with billions of parameters. This motivates the exploration of methods that seek to correct LLMs <span id="S4.p1.1.2" class="ltx_text ltx_font_italic">during</span> the generation time or <span id="S4.p1.1.3" class="ltx_text ltx_font_italic">after</span> the output is generated. This section focuses on generation-time correction techniques, in which automated feedback serves as a guiding mechanism for LLM generation. Such a strategy allows LLMs to rectify errors during generation without modifying the model parameters. We identify two primary strategies for generation-time correction: <span id="S4.p1.1.4" class="ltx_text ltx_font_italic">Generate-then-Rank</span>, and <span id="S4.p1.1.5" class="ltx_text ltx_font_italic">Feedback-Guided Decoding</span>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Generate-then-Rank</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.5" class="ltx_p">The most immediate strategy involves sampling a large number of candidate generations and subsequently picking up the best generation based on the feedback provided by the critic model, as illustrated in Figure <a href="#in_generation_methods" title="Figure 3 ‣ Figure 3 ‣ Reward Modeling and RLHF. ‣ 3.1 Learning from Human Feedback ‣ 3 Training-Time Correction ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></span>(a)</a>. Here, the critic model <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{C}" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\mathcal{C}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">caligraphic_C</annotation></semantics></math> aims to learn the mapping <math id="S4.SS1.p1.2.m2.4" class="ltx_Math" alttext="x,\hat{y}_{1},\cdots,\hat{y}_{N}\rightarrow y_{best}" display="inline"><semantics id="S4.SS1.p1.2.m2.4a"><mrow id="S4.SS1.p1.2.m2.4.4" xref="S4.SS1.p1.2.m2.4.4.cmml"><mrow id="S4.SS1.p1.2.m2.4.4.2.2" xref="S4.SS1.p1.2.m2.4.4.2.3.cmml"><mi id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">x</mi><mo id="S4.SS1.p1.2.m2.4.4.2.2.3" xref="S4.SS1.p1.2.m2.4.4.2.3.cmml">,</mo><msub id="S4.SS1.p1.2.m2.3.3.1.1.1" xref="S4.SS1.p1.2.m2.3.3.1.1.1.cmml"><mover accent="true" id="S4.SS1.p1.2.m2.3.3.1.1.1.2" xref="S4.SS1.p1.2.m2.3.3.1.1.1.2.cmml"><mi id="S4.SS1.p1.2.m2.3.3.1.1.1.2.2" xref="S4.SS1.p1.2.m2.3.3.1.1.1.2.2.cmml">y</mi><mo id="S4.SS1.p1.2.m2.3.3.1.1.1.2.1" xref="S4.SS1.p1.2.m2.3.3.1.1.1.2.1.cmml">^</mo></mover><mn id="S4.SS1.p1.2.m2.3.3.1.1.1.3" xref="S4.SS1.p1.2.m2.3.3.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS1.p1.2.m2.4.4.2.2.4" xref="S4.SS1.p1.2.m2.4.4.2.3.cmml">,</mo><mi mathvariant="normal" id="S4.SS1.p1.2.m2.2.2" xref="S4.SS1.p1.2.m2.2.2.cmml">⋯</mi><mo id="S4.SS1.p1.2.m2.4.4.2.2.5" xref="S4.SS1.p1.2.m2.4.4.2.3.cmml">,</mo><msub id="S4.SS1.p1.2.m2.4.4.2.2.2" xref="S4.SS1.p1.2.m2.4.4.2.2.2.cmml"><mover accent="true" id="S4.SS1.p1.2.m2.4.4.2.2.2.2" xref="S4.SS1.p1.2.m2.4.4.2.2.2.2.cmml"><mi id="S4.SS1.p1.2.m2.4.4.2.2.2.2.2" xref="S4.SS1.p1.2.m2.4.4.2.2.2.2.2.cmml">y</mi><mo id="S4.SS1.p1.2.m2.4.4.2.2.2.2.1" xref="S4.SS1.p1.2.m2.4.4.2.2.2.2.1.cmml">^</mo></mover><mi id="S4.SS1.p1.2.m2.4.4.2.2.2.3" xref="S4.SS1.p1.2.m2.4.4.2.2.2.3.cmml">N</mi></msub></mrow><mo stretchy="false" id="S4.SS1.p1.2.m2.4.4.3" xref="S4.SS1.p1.2.m2.4.4.3.cmml">→</mo><msub id="S4.SS1.p1.2.m2.4.4.4" xref="S4.SS1.p1.2.m2.4.4.4.cmml"><mi id="S4.SS1.p1.2.m2.4.4.4.2" xref="S4.SS1.p1.2.m2.4.4.4.2.cmml">y</mi><mrow id="S4.SS1.p1.2.m2.4.4.4.3" xref="S4.SS1.p1.2.m2.4.4.4.3.cmml"><mi id="S4.SS1.p1.2.m2.4.4.4.3.2" xref="S4.SS1.p1.2.m2.4.4.4.3.2.cmml">b</mi><mo id="S4.SS1.p1.2.m2.4.4.4.3.1" xref="S4.SS1.p1.2.m2.4.4.4.3.1.cmml">⁢</mo><mi id="S4.SS1.p1.2.m2.4.4.4.3.3" xref="S4.SS1.p1.2.m2.4.4.4.3.3.cmml">e</mi><mo id="S4.SS1.p1.2.m2.4.4.4.3.1a" xref="S4.SS1.p1.2.m2.4.4.4.3.1.cmml">⁢</mo><mi id="S4.SS1.p1.2.m2.4.4.4.3.4" xref="S4.SS1.p1.2.m2.4.4.4.3.4.cmml">s</mi><mo id="S4.SS1.p1.2.m2.4.4.4.3.1b" xref="S4.SS1.p1.2.m2.4.4.4.3.1.cmml">⁢</mo><mi id="S4.SS1.p1.2.m2.4.4.4.3.5" xref="S4.SS1.p1.2.m2.4.4.4.3.5.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.4b"><apply id="S4.SS1.p1.2.m2.4.4.cmml" xref="S4.SS1.p1.2.m2.4.4"><ci id="S4.SS1.p1.2.m2.4.4.3.cmml" xref="S4.SS1.p1.2.m2.4.4.3">→</ci><list id="S4.SS1.p1.2.m2.4.4.2.3.cmml" xref="S4.SS1.p1.2.m2.4.4.2.2"><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">𝑥</ci><apply id="S4.SS1.p1.2.m2.3.3.1.1.1.cmml" xref="S4.SS1.p1.2.m2.3.3.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m2.3.3.1.1.1.1.cmml" xref="S4.SS1.p1.2.m2.3.3.1.1.1">subscript</csymbol><apply id="S4.SS1.p1.2.m2.3.3.1.1.1.2.cmml" xref="S4.SS1.p1.2.m2.3.3.1.1.1.2"><ci id="S4.SS1.p1.2.m2.3.3.1.1.1.2.1.cmml" xref="S4.SS1.p1.2.m2.3.3.1.1.1.2.1">^</ci><ci id="S4.SS1.p1.2.m2.3.3.1.1.1.2.2.cmml" xref="S4.SS1.p1.2.m2.3.3.1.1.1.2.2">𝑦</ci></apply><cn type="integer" id="S4.SS1.p1.2.m2.3.3.1.1.1.3.cmml" xref="S4.SS1.p1.2.m2.3.3.1.1.1.3">1</cn></apply><ci id="S4.SS1.p1.2.m2.2.2.cmml" xref="S4.SS1.p1.2.m2.2.2">⋯</ci><apply id="S4.SS1.p1.2.m2.4.4.2.2.2.cmml" xref="S4.SS1.p1.2.m2.4.4.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m2.4.4.2.2.2.1.cmml" xref="S4.SS1.p1.2.m2.4.4.2.2.2">subscript</csymbol><apply id="S4.SS1.p1.2.m2.4.4.2.2.2.2.cmml" xref="S4.SS1.p1.2.m2.4.4.2.2.2.2"><ci id="S4.SS1.p1.2.m2.4.4.2.2.2.2.1.cmml" xref="S4.SS1.p1.2.m2.4.4.2.2.2.2.1">^</ci><ci id="S4.SS1.p1.2.m2.4.4.2.2.2.2.2.cmml" xref="S4.SS1.p1.2.m2.4.4.2.2.2.2.2">𝑦</ci></apply><ci id="S4.SS1.p1.2.m2.4.4.2.2.2.3.cmml" xref="S4.SS1.p1.2.m2.4.4.2.2.2.3">𝑁</ci></apply></list><apply id="S4.SS1.p1.2.m2.4.4.4.cmml" xref="S4.SS1.p1.2.m2.4.4.4"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m2.4.4.4.1.cmml" xref="S4.SS1.p1.2.m2.4.4.4">subscript</csymbol><ci id="S4.SS1.p1.2.m2.4.4.4.2.cmml" xref="S4.SS1.p1.2.m2.4.4.4.2">𝑦</ci><apply id="S4.SS1.p1.2.m2.4.4.4.3.cmml" xref="S4.SS1.p1.2.m2.4.4.4.3"><times id="S4.SS1.p1.2.m2.4.4.4.3.1.cmml" xref="S4.SS1.p1.2.m2.4.4.4.3.1"></times><ci id="S4.SS1.p1.2.m2.4.4.4.3.2.cmml" xref="S4.SS1.p1.2.m2.4.4.4.3.2">𝑏</ci><ci id="S4.SS1.p1.2.m2.4.4.4.3.3.cmml" xref="S4.SS1.p1.2.m2.4.4.4.3.3">𝑒</ci><ci id="S4.SS1.p1.2.m2.4.4.4.3.4.cmml" xref="S4.SS1.p1.2.m2.4.4.4.3.4">𝑠</ci><ci id="S4.SS1.p1.2.m2.4.4.4.3.5.cmml" xref="S4.SS1.p1.2.m2.4.4.4.3.5">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.4c">x,\hat{y}_{1},\cdots,\hat{y}_{N}\rightarrow y_{best}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m2.4d">italic_x , over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ⋯ , over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT → italic_y start_POSTSUBSCRIPT italic_b italic_e italic_s italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, where <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="y_{best}" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><msub id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml"><mi id="S4.SS1.p1.3.m3.1.1.2" xref="S4.SS1.p1.3.m3.1.1.2.cmml">y</mi><mrow id="S4.SS1.p1.3.m3.1.1.3" xref="S4.SS1.p1.3.m3.1.1.3.cmml"><mi id="S4.SS1.p1.3.m3.1.1.3.2" xref="S4.SS1.p1.3.m3.1.1.3.2.cmml">b</mi><mo id="S4.SS1.p1.3.m3.1.1.3.1" xref="S4.SS1.p1.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p1.3.m3.1.1.3.3" xref="S4.SS1.p1.3.m3.1.1.3.3.cmml">e</mi><mo id="S4.SS1.p1.3.m3.1.1.3.1a" xref="S4.SS1.p1.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p1.3.m3.1.1.3.4" xref="S4.SS1.p1.3.m3.1.1.3.4.cmml">s</mi><mo id="S4.SS1.p1.3.m3.1.1.3.1b" xref="S4.SS1.p1.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p1.3.m3.1.1.3.5" xref="S4.SS1.p1.3.m3.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><apply id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.3.m3.1.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.p1.3.m3.1.1.2.cmml" xref="S4.SS1.p1.3.m3.1.1.2">𝑦</ci><apply id="S4.SS1.p1.3.m3.1.1.3.cmml" xref="S4.SS1.p1.3.m3.1.1.3"><times id="S4.SS1.p1.3.m3.1.1.3.1.cmml" xref="S4.SS1.p1.3.m3.1.1.3.1"></times><ci id="S4.SS1.p1.3.m3.1.1.3.2.cmml" xref="S4.SS1.p1.3.m3.1.1.3.2">𝑏</ci><ci id="S4.SS1.p1.3.m3.1.1.3.3.cmml" xref="S4.SS1.p1.3.m3.1.1.3.3">𝑒</ci><ci id="S4.SS1.p1.3.m3.1.1.3.4.cmml" xref="S4.SS1.p1.3.m3.1.1.3.4">𝑠</ci><ci id="S4.SS1.p1.3.m3.1.1.3.5.cmml" xref="S4.SS1.p1.3.m3.1.1.3.5">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">y_{best}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.3.m3.1d">italic_y start_POSTSUBSCRIPT italic_b italic_e italic_s italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is the best output among the <math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><mi id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><ci id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">N</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.4.m4.1d">italic_N</annotation></semantics></math> candidate outputs <math id="S4.SS1.p1.5.m5.4" class="ltx_Math" alttext="\hat{y}_{1},\cdots,\hat{y}_{N}\sim\mathcal{M}(x)" display="inline"><semantics id="S4.SS1.p1.5.m5.4a"><mrow id="S4.SS1.p1.5.m5.4.4" xref="S4.SS1.p1.5.m5.4.4.cmml"><mrow id="S4.SS1.p1.5.m5.4.4.2.2" xref="S4.SS1.p1.5.m5.4.4.2.3.cmml"><msub id="S4.SS1.p1.5.m5.3.3.1.1.1" xref="S4.SS1.p1.5.m5.3.3.1.1.1.cmml"><mover accent="true" id="S4.SS1.p1.5.m5.3.3.1.1.1.2" xref="S4.SS1.p1.5.m5.3.3.1.1.1.2.cmml"><mi id="S4.SS1.p1.5.m5.3.3.1.1.1.2.2" xref="S4.SS1.p1.5.m5.3.3.1.1.1.2.2.cmml">y</mi><mo id="S4.SS1.p1.5.m5.3.3.1.1.1.2.1" xref="S4.SS1.p1.5.m5.3.3.1.1.1.2.1.cmml">^</mo></mover><mn id="S4.SS1.p1.5.m5.3.3.1.1.1.3" xref="S4.SS1.p1.5.m5.3.3.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS1.p1.5.m5.4.4.2.2.3" xref="S4.SS1.p1.5.m5.4.4.2.3.cmml">,</mo><mi mathvariant="normal" id="S4.SS1.p1.5.m5.2.2" xref="S4.SS1.p1.5.m5.2.2.cmml">⋯</mi><mo id="S4.SS1.p1.5.m5.4.4.2.2.4" xref="S4.SS1.p1.5.m5.4.4.2.3.cmml">,</mo><msub id="S4.SS1.p1.5.m5.4.4.2.2.2" xref="S4.SS1.p1.5.m5.4.4.2.2.2.cmml"><mover accent="true" id="S4.SS1.p1.5.m5.4.4.2.2.2.2" xref="S4.SS1.p1.5.m5.4.4.2.2.2.2.cmml"><mi id="S4.SS1.p1.5.m5.4.4.2.2.2.2.2" xref="S4.SS1.p1.5.m5.4.4.2.2.2.2.2.cmml">y</mi><mo id="S4.SS1.p1.5.m5.4.4.2.2.2.2.1" xref="S4.SS1.p1.5.m5.4.4.2.2.2.2.1.cmml">^</mo></mover><mi id="S4.SS1.p1.5.m5.4.4.2.2.2.3" xref="S4.SS1.p1.5.m5.4.4.2.2.2.3.cmml">N</mi></msub></mrow><mo id="S4.SS1.p1.5.m5.4.4.3" xref="S4.SS1.p1.5.m5.4.4.3.cmml">∼</mo><mrow id="S4.SS1.p1.5.m5.4.4.4" xref="S4.SS1.p1.5.m5.4.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.5.m5.4.4.4.2" xref="S4.SS1.p1.5.m5.4.4.4.2.cmml">ℳ</mi><mo id="S4.SS1.p1.5.m5.4.4.4.1" xref="S4.SS1.p1.5.m5.4.4.4.1.cmml">⁢</mo><mrow id="S4.SS1.p1.5.m5.4.4.4.3.2" xref="S4.SS1.p1.5.m5.4.4.4.cmml"><mo stretchy="false" id="S4.SS1.p1.5.m5.4.4.4.3.2.1" xref="S4.SS1.p1.5.m5.4.4.4.cmml">(</mo><mi id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml">x</mi><mo stretchy="false" id="S4.SS1.p1.5.m5.4.4.4.3.2.2" xref="S4.SS1.p1.5.m5.4.4.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.4b"><apply id="S4.SS1.p1.5.m5.4.4.cmml" xref="S4.SS1.p1.5.m5.4.4"><csymbol cd="latexml" id="S4.SS1.p1.5.m5.4.4.3.cmml" xref="S4.SS1.p1.5.m5.4.4.3">similar-to</csymbol><list id="S4.SS1.p1.5.m5.4.4.2.3.cmml" xref="S4.SS1.p1.5.m5.4.4.2.2"><apply id="S4.SS1.p1.5.m5.3.3.1.1.1.cmml" xref="S4.SS1.p1.5.m5.3.3.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.5.m5.3.3.1.1.1.1.cmml" xref="S4.SS1.p1.5.m5.3.3.1.1.1">subscript</csymbol><apply id="S4.SS1.p1.5.m5.3.3.1.1.1.2.cmml" xref="S4.SS1.p1.5.m5.3.3.1.1.1.2"><ci id="S4.SS1.p1.5.m5.3.3.1.1.1.2.1.cmml" xref="S4.SS1.p1.5.m5.3.3.1.1.1.2.1">^</ci><ci id="S4.SS1.p1.5.m5.3.3.1.1.1.2.2.cmml" xref="S4.SS1.p1.5.m5.3.3.1.1.1.2.2">𝑦</ci></apply><cn type="integer" id="S4.SS1.p1.5.m5.3.3.1.1.1.3.cmml" xref="S4.SS1.p1.5.m5.3.3.1.1.1.3">1</cn></apply><ci id="S4.SS1.p1.5.m5.2.2.cmml" xref="S4.SS1.p1.5.m5.2.2">⋯</ci><apply id="S4.SS1.p1.5.m5.4.4.2.2.2.cmml" xref="S4.SS1.p1.5.m5.4.4.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p1.5.m5.4.4.2.2.2.1.cmml" xref="S4.SS1.p1.5.m5.4.4.2.2.2">subscript</csymbol><apply id="S4.SS1.p1.5.m5.4.4.2.2.2.2.cmml" xref="S4.SS1.p1.5.m5.4.4.2.2.2.2"><ci id="S4.SS1.p1.5.m5.4.4.2.2.2.2.1.cmml" xref="S4.SS1.p1.5.m5.4.4.2.2.2.2.1">^</ci><ci id="S4.SS1.p1.5.m5.4.4.2.2.2.2.2.cmml" xref="S4.SS1.p1.5.m5.4.4.2.2.2.2.2">𝑦</ci></apply><ci id="S4.SS1.p1.5.m5.4.4.2.2.2.3.cmml" xref="S4.SS1.p1.5.m5.4.4.2.2.2.3">𝑁</ci></apply></list><apply id="S4.SS1.p1.5.m5.4.4.4.cmml" xref="S4.SS1.p1.5.m5.4.4.4"><times id="S4.SS1.p1.5.m5.4.4.4.1.cmml" xref="S4.SS1.p1.5.m5.4.4.4.1"></times><ci id="S4.SS1.p1.5.m5.4.4.4.2.cmml" xref="S4.SS1.p1.5.m5.4.4.4.2">ℳ</ci><ci id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.4c">\hat{y}_{1},\cdots,\hat{y}_{N}\sim\mathcal{M}(x)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.5.m5.4d">over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ⋯ , over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ∼ caligraphic_M ( italic_x )</annotation></semantics></math>.
</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.9" class="ltx_p">This approach is often integrated with the Chain-of-Thought (CoT) prompting method <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a href="#bib.bib120" title="" class="ltx_ref">2022b</a>)</cite> to tackle complex reasoning tasks, such as solving math word problems as in GSM8K <cite class="ltx_cite ltx_citemacro_cite">Cobbe et al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>. Given an input problem <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mi id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><ci id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">italic_x</annotation></semantics></math>, the LLM initially generates multiple candidate solutions <math id="S4.SS1.p2.2.m2.3" class="ltx_Math" alttext="{y_{1},\cdots,y_{n}}" display="inline"><semantics id="S4.SS1.p2.2.m2.3a"><mrow id="S4.SS1.p2.2.m2.3.3.2" xref="S4.SS1.p2.2.m2.3.3.3.cmml"><msub id="S4.SS1.p2.2.m2.2.2.1.1" xref="S4.SS1.p2.2.m2.2.2.1.1.cmml"><mi id="S4.SS1.p2.2.m2.2.2.1.1.2" xref="S4.SS1.p2.2.m2.2.2.1.1.2.cmml">y</mi><mn id="S4.SS1.p2.2.m2.2.2.1.1.3" xref="S4.SS1.p2.2.m2.2.2.1.1.3.cmml">1</mn></msub><mo id="S4.SS1.p2.2.m2.3.3.2.3" xref="S4.SS1.p2.2.m2.3.3.3.cmml">,</mo><mi mathvariant="normal" id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">⋯</mi><mo id="S4.SS1.p2.2.m2.3.3.2.4" xref="S4.SS1.p2.2.m2.3.3.3.cmml">,</mo><msub id="S4.SS1.p2.2.m2.3.3.2.2" xref="S4.SS1.p2.2.m2.3.3.2.2.cmml"><mi id="S4.SS1.p2.2.m2.3.3.2.2.2" xref="S4.SS1.p2.2.m2.3.3.2.2.2.cmml">y</mi><mi id="S4.SS1.p2.2.m2.3.3.2.2.3" xref="S4.SS1.p2.2.m2.3.3.2.2.3.cmml">n</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.3b"><list id="S4.SS1.p2.2.m2.3.3.3.cmml" xref="S4.SS1.p2.2.m2.3.3.2"><apply id="S4.SS1.p2.2.m2.2.2.1.1.cmml" xref="S4.SS1.p2.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.2.2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.2.2.1.1">subscript</csymbol><ci id="S4.SS1.p2.2.m2.2.2.1.1.2.cmml" xref="S4.SS1.p2.2.m2.2.2.1.1.2">𝑦</ci><cn type="integer" id="S4.SS1.p2.2.m2.2.2.1.1.3.cmml" xref="S4.SS1.p2.2.m2.2.2.1.1.3">1</cn></apply><ci id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">⋯</ci><apply id="S4.SS1.p2.2.m2.3.3.2.2.cmml" xref="S4.SS1.p2.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.3.3.2.2.1.cmml" xref="S4.SS1.p2.2.m2.3.3.2.2">subscript</csymbol><ci id="S4.SS1.p2.2.m2.3.3.2.2.2.cmml" xref="S4.SS1.p2.2.m2.3.3.2.2.2">𝑦</ci><ci id="S4.SS1.p2.2.m2.3.3.2.2.3.cmml" xref="S4.SS1.p2.2.m2.3.3.2.2.3">𝑛</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.3c">{y_{1},\cdots,y_{n}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.2.m2.3d">italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ⋯ , italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math>. Each solution <math id="S4.SS1.p2.3.m3.2" class="ltx_Math" alttext="y_{i}=[z_{i},a_{i}]" display="inline"><semantics id="S4.SS1.p2.3.m3.2a"><mrow id="S4.SS1.p2.3.m3.2.2" xref="S4.SS1.p2.3.m3.2.2.cmml"><msub id="S4.SS1.p2.3.m3.2.2.4" xref="S4.SS1.p2.3.m3.2.2.4.cmml"><mi id="S4.SS1.p2.3.m3.2.2.4.2" xref="S4.SS1.p2.3.m3.2.2.4.2.cmml">y</mi><mi id="S4.SS1.p2.3.m3.2.2.4.3" xref="S4.SS1.p2.3.m3.2.2.4.3.cmml">i</mi></msub><mo id="S4.SS1.p2.3.m3.2.2.3" xref="S4.SS1.p2.3.m3.2.2.3.cmml">=</mo><mrow id="S4.SS1.p2.3.m3.2.2.2.2" xref="S4.SS1.p2.3.m3.2.2.2.3.cmml"><mo stretchy="false" id="S4.SS1.p2.3.m3.2.2.2.2.3" xref="S4.SS1.p2.3.m3.2.2.2.3.cmml">[</mo><msub id="S4.SS1.p2.3.m3.1.1.1.1.1" xref="S4.SS1.p2.3.m3.1.1.1.1.1.cmml"><mi id="S4.SS1.p2.3.m3.1.1.1.1.1.2" xref="S4.SS1.p2.3.m3.1.1.1.1.1.2.cmml">z</mi><mi id="S4.SS1.p2.3.m3.1.1.1.1.1.3" xref="S4.SS1.p2.3.m3.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.SS1.p2.3.m3.2.2.2.2.4" xref="S4.SS1.p2.3.m3.2.2.2.3.cmml">,</mo><msub id="S4.SS1.p2.3.m3.2.2.2.2.2" xref="S4.SS1.p2.3.m3.2.2.2.2.2.cmml"><mi id="S4.SS1.p2.3.m3.2.2.2.2.2.2" xref="S4.SS1.p2.3.m3.2.2.2.2.2.2.cmml">a</mi><mi id="S4.SS1.p2.3.m3.2.2.2.2.2.3" xref="S4.SS1.p2.3.m3.2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S4.SS1.p2.3.m3.2.2.2.2.5" xref="S4.SS1.p2.3.m3.2.2.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.2b"><apply id="S4.SS1.p2.3.m3.2.2.cmml" xref="S4.SS1.p2.3.m3.2.2"><eq id="S4.SS1.p2.3.m3.2.2.3.cmml" xref="S4.SS1.p2.3.m3.2.2.3"></eq><apply id="S4.SS1.p2.3.m3.2.2.4.cmml" xref="S4.SS1.p2.3.m3.2.2.4"><csymbol cd="ambiguous" id="S4.SS1.p2.3.m3.2.2.4.1.cmml" xref="S4.SS1.p2.3.m3.2.2.4">subscript</csymbol><ci id="S4.SS1.p2.3.m3.2.2.4.2.cmml" xref="S4.SS1.p2.3.m3.2.2.4.2">𝑦</ci><ci id="S4.SS1.p2.3.m3.2.2.4.3.cmml" xref="S4.SS1.p2.3.m3.2.2.4.3">𝑖</ci></apply><interval closure="closed" id="S4.SS1.p2.3.m3.2.2.2.3.cmml" xref="S4.SS1.p2.3.m3.2.2.2.2"><apply id="S4.SS1.p2.3.m3.1.1.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.3.m3.1.1.1.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1.1.1.1">subscript</csymbol><ci id="S4.SS1.p2.3.m3.1.1.1.1.1.2.cmml" xref="S4.SS1.p2.3.m3.1.1.1.1.1.2">𝑧</ci><ci id="S4.SS1.p2.3.m3.1.1.1.1.1.3.cmml" xref="S4.SS1.p2.3.m3.1.1.1.1.1.3">𝑖</ci></apply><apply id="S4.SS1.p2.3.m3.2.2.2.2.2.cmml" xref="S4.SS1.p2.3.m3.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p2.3.m3.2.2.2.2.2.1.cmml" xref="S4.SS1.p2.3.m3.2.2.2.2.2">subscript</csymbol><ci id="S4.SS1.p2.3.m3.2.2.2.2.2.2.cmml" xref="S4.SS1.p2.3.m3.2.2.2.2.2.2">𝑎</ci><ci id="S4.SS1.p2.3.m3.2.2.2.2.2.3.cmml" xref="S4.SS1.p2.3.m3.2.2.2.2.2.3">𝑖</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.2c">y_{i}=[z_{i},a_{i}]</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.3.m3.2d">italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = [ italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ]</annotation></semantics></math> comprises a reasoning path (explanation) <math id="S4.SS1.p2.4.m4.1" class="ltx_Math" alttext="z_{i}" display="inline"><semantics id="S4.SS1.p2.4.m4.1a"><msub id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml"><mi id="S4.SS1.p2.4.m4.1.1.2" xref="S4.SS1.p2.4.m4.1.1.2.cmml">z</mi><mi id="S4.SS1.p2.4.m4.1.1.3" xref="S4.SS1.p2.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><apply id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.4.m4.1.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="S4.SS1.p2.4.m4.1.1.2.cmml" xref="S4.SS1.p2.4.m4.1.1.2">𝑧</ci><ci id="S4.SS1.p2.4.m4.1.1.3.cmml" xref="S4.SS1.p2.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">z_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.4.m4.1d">italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> leading to the predicted answer <math id="S4.SS1.p2.5.m5.1" class="ltx_Math" alttext="a_{i}" display="inline"><semantics id="S4.SS1.p2.5.m5.1a"><msub id="S4.SS1.p2.5.m5.1.1" xref="S4.SS1.p2.5.m5.1.1.cmml"><mi id="S4.SS1.p2.5.m5.1.1.2" xref="S4.SS1.p2.5.m5.1.1.2.cmml">a</mi><mi id="S4.SS1.p2.5.m5.1.1.3" xref="S4.SS1.p2.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.5.m5.1b"><apply id="S4.SS1.p2.5.m5.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.5.m5.1.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1">subscript</csymbol><ci id="S4.SS1.p2.5.m5.1.1.2.cmml" xref="S4.SS1.p2.5.m5.1.1.2">𝑎</ci><ci id="S4.SS1.p2.5.m5.1.1.3.cmml" xref="S4.SS1.p2.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.5.m5.1c">a_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.5.m5.1d">italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. Subsequently, the critic model <math id="S4.SS1.p2.6.m6.1" class="ltx_Math" alttext="\mathcal{C}" display="inline"><semantics id="S4.SS1.p2.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p2.6.m6.1.1" xref="S4.SS1.p2.6.m6.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.6.m6.1b"><ci id="S4.SS1.p2.6.m6.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.6.m6.1c">\mathcal{C}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.6.m6.1d">caligraphic_C</annotation></semantics></math> assigns a plausibility score <math id="S4.SS1.p2.7.m7.1" class="ltx_Math" alttext="s_{i}" display="inline"><semantics id="S4.SS1.p2.7.m7.1a"><msub id="S4.SS1.p2.7.m7.1.1" xref="S4.SS1.p2.7.m7.1.1.cmml"><mi id="S4.SS1.p2.7.m7.1.1.2" xref="S4.SS1.p2.7.m7.1.1.2.cmml">s</mi><mi id="S4.SS1.p2.7.m7.1.1.3" xref="S4.SS1.p2.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.7.m7.1b"><apply id="S4.SS1.p2.7.m7.1.1.cmml" xref="S4.SS1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.7.m7.1.1.1.cmml" xref="S4.SS1.p2.7.m7.1.1">subscript</csymbol><ci id="S4.SS1.p2.7.m7.1.1.2.cmml" xref="S4.SS1.p2.7.m7.1.1.2">𝑠</ci><ci id="S4.SS1.p2.7.m7.1.1.3.cmml" xref="S4.SS1.p2.7.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.7.m7.1c">s_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.7.m7.1d">italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> to each candidate reasoning path <math id="S4.SS1.p2.8.m8.1" class="ltx_Math" alttext="z_{i}" display="inline"><semantics id="S4.SS1.p2.8.m8.1a"><msub id="S4.SS1.p2.8.m8.1.1" xref="S4.SS1.p2.8.m8.1.1.cmml"><mi id="S4.SS1.p2.8.m8.1.1.2" xref="S4.SS1.p2.8.m8.1.1.2.cmml">z</mi><mi id="S4.SS1.p2.8.m8.1.1.3" xref="S4.SS1.p2.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.8.m8.1b"><apply id="S4.SS1.p2.8.m8.1.1.cmml" xref="S4.SS1.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.8.m8.1.1.1.cmml" xref="S4.SS1.p2.8.m8.1.1">subscript</csymbol><ci id="S4.SS1.p2.8.m8.1.1.2.cmml" xref="S4.SS1.p2.8.m8.1.1.2">𝑧</ci><ci id="S4.SS1.p2.8.m8.1.1.3.cmml" xref="S4.SS1.p2.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.8.m8.1c">z_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.8.m8.1d">italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. The final selection of the best solution from the scored set <math id="S4.SS1.p2.9.m9.3" class="ltx_Math" alttext="{(z_{i},a_{i},s_{i})}_{i=1}^{n}" display="inline"><semantics id="S4.SS1.p2.9.m9.3a"><msubsup id="S4.SS1.p2.9.m9.3.3" xref="S4.SS1.p2.9.m9.3.3.cmml"><mrow id="S4.SS1.p2.9.m9.3.3.3.3.3" xref="S4.SS1.p2.9.m9.3.3.3.3.4.cmml"><mo stretchy="false" id="S4.SS1.p2.9.m9.3.3.3.3.3.4" xref="S4.SS1.p2.9.m9.3.3.3.3.4.cmml">(</mo><msub id="S4.SS1.p2.9.m9.1.1.1.1.1.1" xref="S4.SS1.p2.9.m9.1.1.1.1.1.1.cmml"><mi id="S4.SS1.p2.9.m9.1.1.1.1.1.1.2" xref="S4.SS1.p2.9.m9.1.1.1.1.1.1.2.cmml">z</mi><mi id="S4.SS1.p2.9.m9.1.1.1.1.1.1.3" xref="S4.SS1.p2.9.m9.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.SS1.p2.9.m9.3.3.3.3.3.5" xref="S4.SS1.p2.9.m9.3.3.3.3.4.cmml">,</mo><msub id="S4.SS1.p2.9.m9.2.2.2.2.2.2" xref="S4.SS1.p2.9.m9.2.2.2.2.2.2.cmml"><mi id="S4.SS1.p2.9.m9.2.2.2.2.2.2.2" xref="S4.SS1.p2.9.m9.2.2.2.2.2.2.2.cmml">a</mi><mi id="S4.SS1.p2.9.m9.2.2.2.2.2.2.3" xref="S4.SS1.p2.9.m9.2.2.2.2.2.2.3.cmml">i</mi></msub><mo id="S4.SS1.p2.9.m9.3.3.3.3.3.6" xref="S4.SS1.p2.9.m9.3.3.3.3.4.cmml">,</mo><msub id="S4.SS1.p2.9.m9.3.3.3.3.3.3" xref="S4.SS1.p2.9.m9.3.3.3.3.3.3.cmml"><mi id="S4.SS1.p2.9.m9.3.3.3.3.3.3.2" xref="S4.SS1.p2.9.m9.3.3.3.3.3.3.2.cmml">s</mi><mi id="S4.SS1.p2.9.m9.3.3.3.3.3.3.3" xref="S4.SS1.p2.9.m9.3.3.3.3.3.3.3.cmml">i</mi></msub><mo stretchy="false" id="S4.SS1.p2.9.m9.3.3.3.3.3.7" xref="S4.SS1.p2.9.m9.3.3.3.3.4.cmml">)</mo></mrow><mrow id="S4.SS1.p2.9.m9.3.3.3.5" xref="S4.SS1.p2.9.m9.3.3.3.5.cmml"><mi id="S4.SS1.p2.9.m9.3.3.3.5.2" xref="S4.SS1.p2.9.m9.3.3.3.5.2.cmml">i</mi><mo id="S4.SS1.p2.9.m9.3.3.3.5.1" xref="S4.SS1.p2.9.m9.3.3.3.5.1.cmml">=</mo><mn id="S4.SS1.p2.9.m9.3.3.3.5.3" xref="S4.SS1.p2.9.m9.3.3.3.5.3.cmml">1</mn></mrow><mi id="S4.SS1.p2.9.m9.3.3.5" xref="S4.SS1.p2.9.m9.3.3.5.cmml">n</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.9.m9.3b"><apply id="S4.SS1.p2.9.m9.3.3.cmml" xref="S4.SS1.p2.9.m9.3.3"><csymbol cd="ambiguous" id="S4.SS1.p2.9.m9.3.3.4.cmml" xref="S4.SS1.p2.9.m9.3.3">superscript</csymbol><apply id="S4.SS1.p2.9.m9.3.3.3.cmml" xref="S4.SS1.p2.9.m9.3.3"><csymbol cd="ambiguous" id="S4.SS1.p2.9.m9.3.3.3.4.cmml" xref="S4.SS1.p2.9.m9.3.3">subscript</csymbol><vector id="S4.SS1.p2.9.m9.3.3.3.3.4.cmml" xref="S4.SS1.p2.9.m9.3.3.3.3.3"><apply id="S4.SS1.p2.9.m9.1.1.1.1.1.1.cmml" xref="S4.SS1.p2.9.m9.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.9.m9.1.1.1.1.1.1.1.cmml" xref="S4.SS1.p2.9.m9.1.1.1.1.1.1">subscript</csymbol><ci id="S4.SS1.p2.9.m9.1.1.1.1.1.1.2.cmml" xref="S4.SS1.p2.9.m9.1.1.1.1.1.1.2">𝑧</ci><ci id="S4.SS1.p2.9.m9.1.1.1.1.1.1.3.cmml" xref="S4.SS1.p2.9.m9.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S4.SS1.p2.9.m9.2.2.2.2.2.2.cmml" xref="S4.SS1.p2.9.m9.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p2.9.m9.2.2.2.2.2.2.1.cmml" xref="S4.SS1.p2.9.m9.2.2.2.2.2.2">subscript</csymbol><ci id="S4.SS1.p2.9.m9.2.2.2.2.2.2.2.cmml" xref="S4.SS1.p2.9.m9.2.2.2.2.2.2.2">𝑎</ci><ci id="S4.SS1.p2.9.m9.2.2.2.2.2.2.3.cmml" xref="S4.SS1.p2.9.m9.2.2.2.2.2.2.3">𝑖</ci></apply><apply id="S4.SS1.p2.9.m9.3.3.3.3.3.3.cmml" xref="S4.SS1.p2.9.m9.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S4.SS1.p2.9.m9.3.3.3.3.3.3.1.cmml" xref="S4.SS1.p2.9.m9.3.3.3.3.3.3">subscript</csymbol><ci id="S4.SS1.p2.9.m9.3.3.3.3.3.3.2.cmml" xref="S4.SS1.p2.9.m9.3.3.3.3.3.3.2">𝑠</ci><ci id="S4.SS1.p2.9.m9.3.3.3.3.3.3.3.cmml" xref="S4.SS1.p2.9.m9.3.3.3.3.3.3.3">𝑖</ci></apply></vector><apply id="S4.SS1.p2.9.m9.3.3.3.5.cmml" xref="S4.SS1.p2.9.m9.3.3.3.5"><eq id="S4.SS1.p2.9.m9.3.3.3.5.1.cmml" xref="S4.SS1.p2.9.m9.3.3.3.5.1"></eq><ci id="S4.SS1.p2.9.m9.3.3.3.5.2.cmml" xref="S4.SS1.p2.9.m9.3.3.3.5.2">𝑖</ci><cn type="integer" id="S4.SS1.p2.9.m9.3.3.3.5.3.cmml" xref="S4.SS1.p2.9.m9.3.3.3.5.3">1</cn></apply></apply><ci id="S4.SS1.p2.9.m9.3.3.5.cmml" xref="S4.SS1.p2.9.m9.3.3.5">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.9.m9.3c">{(z_{i},a_{i},s_{i})}_{i=1}^{n}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.9.m9.3d">( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> is achieved via either ranking or voting.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.2" class="ltx_p">Different critic models have been proposed in various works. For instance, DIVERSE <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib69" title="" class="ltx_ref">2023d</a>)</cite> trains a binary verifier based on DeBERTa <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib42" title="" class="ltx_ref">2021</a>)</cite>, using reasoning paths that correspond to the correct final answer as positive examples and the others as negative examples. The best answer is then determined by a majority vote of positively-verified candidates. <cite class="ltx_cite ltx_citemacro_citet">Weng et al. (<a href="#bib.bib122" title="" class="ltx_ref">2023</a>)</cite> introduced a training-free critic model based on the idea of self-verification, in which the plausibility score is calculated by assessing the consistency between the results of forward reasoning and backward reasoning. In a different vein, the RR <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite> presented a critic model to assess the <span id="S4.SS1.p3.2.1" class="ltx_text ltx_font_italic">faithfulness</span> of each reasoning path by retrieving supporting information from a knowledge base. LEVER <cite class="ltx_cite ltx_citemacro_cite">Ni et al. (<a href="#bib.bib83" title="" class="ltx_ref">2023</a>)</cite> employed this strategy in language-to-code generation, with each solution <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="y_{i}" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><msub id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mi id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml">y</mi><mi id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2">𝑦</ci><ci id="S4.SS1.p3.1.m1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">y_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.1.m1.1d">italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> serving as a candidate SQL program for the question <math id="S4.SS1.p3.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.SS1.p3.2.m2.1a"><mi id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><ci id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.2.m2.1d">italic_x</annotation></semantics></math>. A verifier was trained to predict the likelihood of a program’s correctness based on the program itself and its execution results. A similar idea is adopted in CodeT <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib11" title="" class="ltx_ref">2023a</a>)</cite> where multiple code solutions and the test cases are generated by the LLM and the best code solution is selected by a dual execution agreement.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="872" height="235" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Three typical strategies of <span id="S4.F4.2.1" class="ltx_text ltx_font_italic">post-hoc correction</span>: self-correction (a), post-hoc correction with external feedback (b), and multi-agent debate (c).</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Feedback-Guided Decoding</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The generate-then-rank method, in which the critic model offers <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">output-level feedback</span> on the entire reasoning path, has certain limitations: 1) The output-level feedback is not fine-grained enough to pinpoint the exact error locations, 2) The extensive length of the output can complicate its quality assessment, and 3) This method does not facilitate fine-grained control over the generation process. For example, the LLM cannot correct its errors during the generation process but must wait until the entire output has been generated.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.5" class="ltx_p">To address these issues, several works have adopted the <span id="S4.SS2.p2.5.1" class="ltx_text ltx_font_italic">feedback-guided decoding</span> strategy shown in Figure <a href="#in_generation_methods" title="Figure 3 ‣ Figure 3 ‣ Reward Modeling and RLHF. ‣ 3.1 Learning from Human Feedback ‣ 3 Training-Time Correction ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></span>(b)</a>, which relies on <span id="S4.SS2.p2.5.2" class="ltx_text ltx_font_italic">step-level feedback</span> to offer fine-grained guidance over the generation process. Here, the generation of the output <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">y</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.1d">italic_y</annotation></semantics></math> is broken down into multiple reasoning steps (or thoughts), <span id="S4.SS2.p2.5.3" class="ltx_text ltx_font_italic">i.e.</span>, <math id="S4.SS2.p2.2.m2.4" class="ltx_Math" alttext="y_{i}=[o_{1},o_{2},\cdots,o_{n}]" display="inline"><semantics id="S4.SS2.p2.2.m2.4a"><mrow id="S4.SS2.p2.2.m2.4.4" xref="S4.SS2.p2.2.m2.4.4.cmml"><msub id="S4.SS2.p2.2.m2.4.4.5" xref="S4.SS2.p2.2.m2.4.4.5.cmml"><mi id="S4.SS2.p2.2.m2.4.4.5.2" xref="S4.SS2.p2.2.m2.4.4.5.2.cmml">y</mi><mi id="S4.SS2.p2.2.m2.4.4.5.3" xref="S4.SS2.p2.2.m2.4.4.5.3.cmml">i</mi></msub><mo id="S4.SS2.p2.2.m2.4.4.4" xref="S4.SS2.p2.2.m2.4.4.4.cmml">=</mo><mrow id="S4.SS2.p2.2.m2.4.4.3.3" xref="S4.SS2.p2.2.m2.4.4.3.4.cmml"><mo stretchy="false" id="S4.SS2.p2.2.m2.4.4.3.3.4" xref="S4.SS2.p2.2.m2.4.4.3.4.cmml">[</mo><msub id="S4.SS2.p2.2.m2.2.2.1.1.1" xref="S4.SS2.p2.2.m2.2.2.1.1.1.cmml"><mi id="S4.SS2.p2.2.m2.2.2.1.1.1.2" xref="S4.SS2.p2.2.m2.2.2.1.1.1.2.cmml">o</mi><mn id="S4.SS2.p2.2.m2.2.2.1.1.1.3" xref="S4.SS2.p2.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS2.p2.2.m2.4.4.3.3.5" xref="S4.SS2.p2.2.m2.4.4.3.4.cmml">,</mo><msub id="S4.SS2.p2.2.m2.3.3.2.2.2" xref="S4.SS2.p2.2.m2.3.3.2.2.2.cmml"><mi id="S4.SS2.p2.2.m2.3.3.2.2.2.2" xref="S4.SS2.p2.2.m2.3.3.2.2.2.2.cmml">o</mi><mn id="S4.SS2.p2.2.m2.3.3.2.2.2.3" xref="S4.SS2.p2.2.m2.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS2.p2.2.m2.4.4.3.3.6" xref="S4.SS2.p2.2.m2.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">⋯</mi><mo id="S4.SS2.p2.2.m2.4.4.3.3.7" xref="S4.SS2.p2.2.m2.4.4.3.4.cmml">,</mo><msub id="S4.SS2.p2.2.m2.4.4.3.3.3" xref="S4.SS2.p2.2.m2.4.4.3.3.3.cmml"><mi id="S4.SS2.p2.2.m2.4.4.3.3.3.2" xref="S4.SS2.p2.2.m2.4.4.3.3.3.2.cmml">o</mi><mi id="S4.SS2.p2.2.m2.4.4.3.3.3.3" xref="S4.SS2.p2.2.m2.4.4.3.3.3.3.cmml">n</mi></msub><mo stretchy="false" id="S4.SS2.p2.2.m2.4.4.3.3.8" xref="S4.SS2.p2.2.m2.4.4.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.4b"><apply id="S4.SS2.p2.2.m2.4.4.cmml" xref="S4.SS2.p2.2.m2.4.4"><eq id="S4.SS2.p2.2.m2.4.4.4.cmml" xref="S4.SS2.p2.2.m2.4.4.4"></eq><apply id="S4.SS2.p2.2.m2.4.4.5.cmml" xref="S4.SS2.p2.2.m2.4.4.5"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m2.4.4.5.1.cmml" xref="S4.SS2.p2.2.m2.4.4.5">subscript</csymbol><ci id="S4.SS2.p2.2.m2.4.4.5.2.cmml" xref="S4.SS2.p2.2.m2.4.4.5.2">𝑦</ci><ci id="S4.SS2.p2.2.m2.4.4.5.3.cmml" xref="S4.SS2.p2.2.m2.4.4.5.3">𝑖</ci></apply><list id="S4.SS2.p2.2.m2.4.4.3.4.cmml" xref="S4.SS2.p2.2.m2.4.4.3.3"><apply id="S4.SS2.p2.2.m2.2.2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m2.2.2.1.1.1.1.cmml" xref="S4.SS2.p2.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S4.SS2.p2.2.m2.2.2.1.1.1.2.cmml" xref="S4.SS2.p2.2.m2.2.2.1.1.1.2">𝑜</ci><cn type="integer" id="S4.SS2.p2.2.m2.2.2.1.1.1.3.cmml" xref="S4.SS2.p2.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="S4.SS2.p2.2.m2.3.3.2.2.2.cmml" xref="S4.SS2.p2.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m2.3.3.2.2.2.1.cmml" xref="S4.SS2.p2.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S4.SS2.p2.2.m2.3.3.2.2.2.2.cmml" xref="S4.SS2.p2.2.m2.3.3.2.2.2.2">𝑜</ci><cn type="integer" id="S4.SS2.p2.2.m2.3.3.2.2.2.3.cmml" xref="S4.SS2.p2.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">⋯</ci><apply id="S4.SS2.p2.2.m2.4.4.3.3.3.cmml" xref="S4.SS2.p2.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m2.4.4.3.3.3.1.cmml" xref="S4.SS2.p2.2.m2.4.4.3.3.3">subscript</csymbol><ci id="S4.SS2.p2.2.m2.4.4.3.3.3.2.cmml" xref="S4.SS2.p2.2.m2.4.4.3.3.3.2">𝑜</ci><ci id="S4.SS2.p2.2.m2.4.4.3.3.3.3.cmml" xref="S4.SS2.p2.2.m2.4.4.3.3.3.3">𝑛</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.4c">y_{i}=[o_{1},o_{2},\cdots,o_{n}]</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.2.m2.4d">italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = [ italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_o start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ⋯ , italic_o start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ]</annotation></semantics></math>. At each individual reasoning step <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mi id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><ci id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">t</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.3.m3.1d">italic_t</annotation></semantics></math>, the critic model provides feedback <math id="S4.SS2.p2.4.m4.3" class="ltx_Math" alttext="\mathcal{C}(x,o_{1:t-1},o_{t})" display="inline"><semantics id="S4.SS2.p2.4.m4.3a"><mrow id="S4.SS2.p2.4.m4.3.3" xref="S4.SS2.p2.4.m4.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p2.4.m4.3.3.4" xref="S4.SS2.p2.4.m4.3.3.4.cmml">𝒞</mi><mo id="S4.SS2.p2.4.m4.3.3.3" xref="S4.SS2.p2.4.m4.3.3.3.cmml">⁢</mo><mrow id="S4.SS2.p2.4.m4.3.3.2.2" xref="S4.SS2.p2.4.m4.3.3.2.3.cmml"><mo stretchy="false" id="S4.SS2.p2.4.m4.3.3.2.2.3" xref="S4.SS2.p2.4.m4.3.3.2.3.cmml">(</mo><mi id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml">x</mi><mo id="S4.SS2.p2.4.m4.3.3.2.2.4" xref="S4.SS2.p2.4.m4.3.3.2.3.cmml">,</mo><msub id="S4.SS2.p2.4.m4.2.2.1.1.1" xref="S4.SS2.p2.4.m4.2.2.1.1.1.cmml"><mi id="S4.SS2.p2.4.m4.2.2.1.1.1.2" xref="S4.SS2.p2.4.m4.2.2.1.1.1.2.cmml">o</mi><mrow id="S4.SS2.p2.4.m4.2.2.1.1.1.3" xref="S4.SS2.p2.4.m4.2.2.1.1.1.3.cmml"><mn id="S4.SS2.p2.4.m4.2.2.1.1.1.3.2" xref="S4.SS2.p2.4.m4.2.2.1.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S4.SS2.p2.4.m4.2.2.1.1.1.3.1" xref="S4.SS2.p2.4.m4.2.2.1.1.1.3.1.cmml">:</mo><mrow id="S4.SS2.p2.4.m4.2.2.1.1.1.3.3" xref="S4.SS2.p2.4.m4.2.2.1.1.1.3.3.cmml"><mi id="S4.SS2.p2.4.m4.2.2.1.1.1.3.3.2" xref="S4.SS2.p2.4.m4.2.2.1.1.1.3.3.2.cmml">t</mi><mo id="S4.SS2.p2.4.m4.2.2.1.1.1.3.3.1" xref="S4.SS2.p2.4.m4.2.2.1.1.1.3.3.1.cmml">−</mo><mn id="S4.SS2.p2.4.m4.2.2.1.1.1.3.3.3" xref="S4.SS2.p2.4.m4.2.2.1.1.1.3.3.3.cmml">1</mn></mrow></mrow></msub><mo id="S4.SS2.p2.4.m4.3.3.2.2.5" xref="S4.SS2.p2.4.m4.3.3.2.3.cmml">,</mo><msub id="S4.SS2.p2.4.m4.3.3.2.2.2" xref="S4.SS2.p2.4.m4.3.3.2.2.2.cmml"><mi id="S4.SS2.p2.4.m4.3.3.2.2.2.2" xref="S4.SS2.p2.4.m4.3.3.2.2.2.2.cmml">o</mi><mi id="S4.SS2.p2.4.m4.3.3.2.2.2.3" xref="S4.SS2.p2.4.m4.3.3.2.2.2.3.cmml">t</mi></msub><mo stretchy="false" id="S4.SS2.p2.4.m4.3.3.2.2.6" xref="S4.SS2.p2.4.m4.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.3b"><apply id="S4.SS2.p2.4.m4.3.3.cmml" xref="S4.SS2.p2.4.m4.3.3"><times id="S4.SS2.p2.4.m4.3.3.3.cmml" xref="S4.SS2.p2.4.m4.3.3.3"></times><ci id="S4.SS2.p2.4.m4.3.3.4.cmml" xref="S4.SS2.p2.4.m4.3.3.4">𝒞</ci><vector id="S4.SS2.p2.4.m4.3.3.2.3.cmml" xref="S4.SS2.p2.4.m4.3.3.2.2"><ci id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1">𝑥</ci><apply id="S4.SS2.p2.4.m4.2.2.1.1.1.cmml" xref="S4.SS2.p2.4.m4.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.4.m4.2.2.1.1.1.1.cmml" xref="S4.SS2.p2.4.m4.2.2.1.1.1">subscript</csymbol><ci id="S4.SS2.p2.4.m4.2.2.1.1.1.2.cmml" xref="S4.SS2.p2.4.m4.2.2.1.1.1.2">𝑜</ci><apply id="S4.SS2.p2.4.m4.2.2.1.1.1.3.cmml" xref="S4.SS2.p2.4.m4.2.2.1.1.1.3"><ci id="S4.SS2.p2.4.m4.2.2.1.1.1.3.1.cmml" xref="S4.SS2.p2.4.m4.2.2.1.1.1.3.1">:</ci><cn type="integer" id="S4.SS2.p2.4.m4.2.2.1.1.1.3.2.cmml" xref="S4.SS2.p2.4.m4.2.2.1.1.1.3.2">1</cn><apply id="S4.SS2.p2.4.m4.2.2.1.1.1.3.3.cmml" xref="S4.SS2.p2.4.m4.2.2.1.1.1.3.3"><minus id="S4.SS2.p2.4.m4.2.2.1.1.1.3.3.1.cmml" xref="S4.SS2.p2.4.m4.2.2.1.1.1.3.3.1"></minus><ci id="S4.SS2.p2.4.m4.2.2.1.1.1.3.3.2.cmml" xref="S4.SS2.p2.4.m4.2.2.1.1.1.3.3.2">𝑡</ci><cn type="integer" id="S4.SS2.p2.4.m4.2.2.1.1.1.3.3.3.cmml" xref="S4.SS2.p2.4.m4.2.2.1.1.1.3.3.3">1</cn></apply></apply></apply><apply id="S4.SS2.p2.4.m4.3.3.2.2.2.cmml" xref="S4.SS2.p2.4.m4.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.p2.4.m4.3.3.2.2.2.1.cmml" xref="S4.SS2.p2.4.m4.3.3.2.2.2">subscript</csymbol><ci id="S4.SS2.p2.4.m4.3.3.2.2.2.2.cmml" xref="S4.SS2.p2.4.m4.3.3.2.2.2.2">𝑜</ci><ci id="S4.SS2.p2.4.m4.3.3.2.2.2.3.cmml" xref="S4.SS2.p2.4.m4.3.3.2.2.2.3">𝑡</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.3c">\mathcal{C}(x,o_{1:t-1},o_{t})</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.4.m4.3d">caligraphic_C ( italic_x , italic_o start_POSTSUBSCRIPT 1 : italic_t - 1 end_POSTSUBSCRIPT , italic_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math> that indicates the quality of <math id="S4.SS2.p2.5.m5.1" class="ltx_Math" alttext="o_{t}" display="inline"><semantics id="S4.SS2.p2.5.m5.1a"><msub id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml"><mi id="S4.SS2.p2.5.m5.1.1.2" xref="S4.SS2.p2.5.m5.1.1.2.cmml">o</mi><mi id="S4.SS2.p2.5.m5.1.1.3" xref="S4.SS2.p2.5.m5.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.1b"><apply id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.5.m5.1.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1">subscript</csymbol><ci id="S4.SS2.p2.5.m5.1.1.2.cmml" xref="S4.SS2.p2.5.m5.1.1.2">𝑜</ci><ci id="S4.SS2.p2.5.m5.1.1.3.cmml" xref="S4.SS2.p2.5.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">o_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.5.m5.1d">italic_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> as a candidate step. With the ability to generate and evaluate individual steps, a search algorithm, such as beam search or depth-first search, can be employed for a systematic exploration of the output space, which effectively steers the decoding process toward the generation of an optimal solution. This also allows the LLM to recover from its early mistakes during generation and helps alleviate the <span id="S4.SS2.p2.5.4" class="ltx_text ltx_font_italic">reasoning inconsistency</span> problem <cite class="ltx_cite ltx_citemacro_cite">Zelikman et al. (<a href="#bib.bib140" title="" class="ltx_ref">2022</a>); Creswell and Shanahan (<a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite>, <span id="S4.SS2.p2.5.5" class="ltx_text ltx_font_italic">i.e.</span>, incorrect reasoning leads to correct final answer.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">The feedback-guided decoding strategy has been applied in many recent works, such as <span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_italic">Tree-of-Thought</span> <cite class="ltx_cite ltx_citemacro_cite">Yao et al. (<a href="#bib.bib136" title="" class="ltx_ref">2023a</a>)</cite>, <span id="S4.SS2.p3.1.2" class="ltx_text ltx_font_italic">GRACE</span> <cite class="ltx_cite ltx_citemacro_cite">Khalifa et al. (<a href="#bib.bib53" title="" class="ltx_ref">2023</a>)</cite>, and <span id="S4.SS2.p3.1.3" class="ltx_text ltx_font_italic">RAP</span> <cite class="ltx_cite ltx_citemacro_cite">Hao et al. (<a href="#bib.bib40" title="" class="ltx_ref">2023</a>)</cite>.
These works mostly differ in how to obtain the critic model that provides automated step-level feedback, the most challenging but crucial element of this strategy. We classify their employed methods into four categories: human feedback, a trained verifier, external metrics, and self-evaluation.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.p4.1" class="ltx_p"><math id="S4.SS2.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.p4.1.m1.1a"><mo id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><ci id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p4.1.m1.1d">∙</annotation></semantics></math> <span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_bold">Reward Model from Human Feedback.</span> One approach involves training a step-level reward model by gathering human feedback, much like the methods discussed in Section <a href="#S3.SS1" title="3.1 Learning from Human Feedback ‣ 3 Training-Time Correction ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>. <cite class="ltx_cite ltx_citemacro_citet">Uesato et al. (<a href="#bib.bib111" title="" class="ltx_ref">2022</a>)</cite> ask human annotators to evaluate the correctness of each reasoning step for the problems in GSM8K and subsequently train a binary reward model. <cite class="ltx_cite ltx_citemacro_citet">Lightman et al. (<a href="#bib.bib70" title="" class="ltx_ref">2023</a>)</cite> expand this approach by annotating a larger dataset consisting of 800K instances of human step-level feedback. Both studies discovered that step-level feedback assists in training a more reliable reward model, which enhances the faithfulness of reasoning.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para ltx_noindent">
<p id="S4.SS2.p5.1" class="ltx_p"><math id="S4.SS2.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.p5.1.m1.1a"><mo id="S4.SS2.p5.1.m1.1.1" xref="S4.SS2.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.1.m1.1b"><ci id="S4.SS2.p5.1.m1.1.1.cmml" xref="S4.SS2.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p5.1.m1.1d">∙</annotation></semantics></math> <span id="S4.SS2.p5.1.1" class="ltx_text ltx_font_bold">Training Verifier with Synthetic Data.</span> Considering the high cost of collecting human annotations and their limited scalability, some works <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib132" title="" class="ltx_ref">2022a</a>); Tafjord et al. (<a href="#bib.bib109" title="" class="ltx_ref">2022</a>); Li et al. (<a href="#bib.bib69" title="" class="ltx_ref">2023d</a>); Khalifa et al. (<a href="#bib.bib53" title="" class="ltx_ref">2023</a>)</cite> have trained a step-wise verifier using automatically constructed training data. Positive examples are derived from ground-truth reasoning paths, while negative examples are synthesized by proposing an alignment algorithm <cite class="ltx_cite ltx_citemacro_cite">Khalifa et al. (<a href="#bib.bib53" title="" class="ltx_ref">2023</a>)</cite> or by making text perturbations on positive samples <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib132" title="" class="ltx_ref">2022a</a>)</cite>.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para ltx_noindent">
<p id="S4.SS2.p6.1" class="ltx_p"><math id="S4.SS2.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.p6.1.m1.1a"><mo id="S4.SS2.p6.1.m1.1.1" xref="S4.SS2.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.1.m1.1b"><ci id="S4.SS2.p6.1.m1.1.1.cmml" xref="S4.SS2.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p6.1.m1.1d">∙</annotation></semantics></math> <span id="S4.SS2.p6.1.1" class="ltx_text ltx_font_bold">Feedback from External Metric.</span> Several works also leverage external metrics to re-rank or guide text generation. <cite class="ltx_cite ltx_citemacro_citet">Freitag et al. (<a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite> uses minimum bayes risk decoding on unbiased samples to optimize neural metrics as an alternative to beam search. Plug and play <cite class="ltx_cite ltx_citemacro_cite">Dathathri et al. (<a href="#bib.bib22" title="" class="ltx_ref">2020</a>)</cite> combines a pretrained model with attribute classifiers that guide text generation without any further training of the model. It leverages the gradient of the classifier to update LM and increase the likelihood of the desirable attribution at the text generation of LM. FUDGE <cite class="ltx_cite ltx_citemacro_cite">Yang and Klein (<a href="#bib.bib133" title="" class="ltx_ref">2021</a>)</cite> reweights the model predictions at each token and estimates the attribution classification at each partial sequence. Following up to the gradient-based approach, DiffusionLM <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib68" title="" class="ltx_ref">2022</a>)</cite> obtains a sequence of intermediate latent variables by denoising a sequence of Gaussian vectors. It performs the iterative gradient updates over latent representations to satisfy controlled requirements from an attribute classifier.</p>
</div>
<div id="S4.SS2.p7" class="ltx_para ltx_noindent">
<p id="S4.SS2.p7.1" class="ltx_p"><math id="S4.SS2.p7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.p7.1.m1.1a"><mo id="S4.SS2.p7.1.m1.1.1" xref="S4.SS2.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p7.1.m1.1b"><ci id="S4.SS2.p7.1.m1.1.1.cmml" xref="S4.SS2.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p7.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p7.1.m1.1d">∙</annotation></semantics></math> <span id="S4.SS2.p7.1.1" class="ltx_text ltx_font_bold">Self-Evaluation.</span> Some studies have utilized a more flexible strategy, employing the LLM itself as the critic model by designing appropriate prompts. For instance, in Tree-of-Thought <cite class="ltx_cite ltx_citemacro_cite">Yao et al. (<a href="#bib.bib136" title="" class="ltx_ref">2023a</a>)</cite>, the LLM is prompted to assess the value of the current state by producing a scalar value (<span id="S4.SS2.p7.1.2" class="ltx_text ltx_font_italic">e.g.</span>, 1-10) or short phrases (<span id="S4.SS2.p7.1.3" class="ltx_text ltx_font_italic">e.g.</span>, sure/likely/impossible). <cite class="ltx_cite ltx_citemacro_citet">Xie et al. (<a href="#bib.bib126" title="" class="ltx_ref">2023</a>)</cite> employed a similar approach by prompting the LLM with “Is the above step of reasoning: (A) Correct (B) Incorrect”. Self-evaluation provides an efficient evaluation method without requiring task-specific verifier fine-tuning.</p>
</div>
<div id="S4.SS2.p8" class="ltx_para">
<p id="S4.SS2.p8.1" class="ltx_p">Existing works also adopted different strategies to control the decoding process with the help of the step-level critic model. Tree-of-Thought employed breadth-first search and depth-first search, while GRACE <cite class="ltx_cite ltx_citemacro_cite">Khalifa et al. (<a href="#bib.bib53" title="" class="ltx_ref">2023</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Xie et al. (<a href="#bib.bib126" title="" class="ltx_ref">2023</a>)</cite> adopted the beam search strategy. At each step, the top-<math id="S4.SS2.p8.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS2.p8.1.m1.1a"><mi id="S4.SS2.p8.1.m1.1.1" xref="S4.SS2.p8.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.1.m1.1b"><ci id="S4.SS2.p8.1.m1.1.1.cmml" xref="S4.SS2.p8.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p8.1.m1.1d">italic_k</annotation></semantics></math> scoring candidates are selected for subsequent generations. This process is repeated until the final answer is generated. Instead, CoRe <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib145" title="" class="ltx_ref">2023</a>)</cite> and RAP <cite class="ltx_cite ltx_citemacro_cite">Hao et al. (<a href="#bib.bib40" title="" class="ltx_ref">2023</a>)</cite> adopted the Monte Carlo Tree Search (MCTS) to strike a proper balance between exploration and exploitation to find the best reasoning path more efficiently.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Post-hoc Correction</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The success of generation-time correction heavily depends on the critic model’s capability in providing accurate quantifiable feedback for intermediate outputs. However, this might be quite challenging for many NLP tasks, such as summarization, due to the holistic nature of the evaluation, <span id="S5.p1.1.1" class="ltx_text ltx_font_italic">i.e.</span>, the summary can only be accurately assessed after the entire summary is generated. This motivates the employment of <span id="S5.p1.1.2" class="ltx_text ltx_font_italic">post-hoc correction</span> methods, where both the critic and refine models intervene only <span id="S5.p1.1.3" class="ltx_text ltx_font_italic">after</span> the entire output is produced. Post-hoc correction also provides a more effective interface with various forms of insightful natural language feedback. This feedback can be as detailed as a diagnostic report pinpointing exact error locations, or as general as suggestions for overall writing improvement. As illustrated in Figure <a href="#S4.F4" title="Figure 4 ‣ 4.1 Generate-then-Rank ‣ 4 Generation-Time Correction ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we summarize three primary strategies for post-hoc correction: <span id="S5.p1.1.4" class="ltx_text ltx_font_italic">Self-Correction</span>, <span id="S5.p1.1.5" class="ltx_text ltx_font_italic">Correction with External Feedback</span>, and <span id="S5.p1.1.6" class="ltx_text ltx_font_italic">Multi-Agent Debate</span>.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Self-Correction</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The simplest approach to implement post-hoc correction is the “Self-Correction” technique, where an LLM is employed to generate feedback and refine its own output. As depicted in Figure <a href="#posthoc_methods" title="Figure 4 ‣ Figure 4 ‣ 4.1 Generate-then-Rank ‣ 4 Generation-Time Correction ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></span>(a)</a>, an LLM is initially used to produce a initial output, and subsequently, the same model acts as a critic to generate feedback and refine this initial output based on the received feedback. This process is typically iterative and continues until an output of acceptable quality is obtained or a pre-specified number of iterations are reached.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_italic">Self-Refine</span> <cite class="ltx_cite ltx_citemacro_citep">(Madaan et al., <a href="#bib.bib79" title="" class="ltx_ref">2023</a>)</cite> proposed a simple-yet-effective self-correction framework by simply using a single powerful pre-trained LLM to generate output, provide feedback, and refine the output based on that feedback.
All these steps are conducted using the same LLM, guided by different prompts.
Similarly, <span id="S5.SS1.p2.1.2" class="ltx_text ltx_font_italic">Clinical Self-Verification</span> <cite class="ltx_cite ltx_citemacro_citep">(Gero et al., <a href="#bib.bib34" title="" class="ltx_ref">2023</a>)</cite> employs the self-correction framework to extract patient data from clinical notes. They specifically generate feedback to find missing elements in the initially extracted data and to validate the generated data. The output is then refined by eliminating unsupported elements. In contrast, <span id="S5.SS1.p2.1.3" class="ltx_text ltx_font_italic">Reflexion</span> <cite class="ltx_cite ltx_citemacro_citep">(Shinn et al., <a href="#bib.bib104" title="" class="ltx_ref">2023</a>)</cite> highlighted that prior self-correction research focused on single-turn generation tasks and failed to retain a record of past errors. To address this, <span id="S5.SS1.p2.1.4" class="ltx_text ltx_font_italic">Reflexion</span> proposed to use the same self-correction framework with an addition of a “long-term memory” capable of storing prior feedback and outputs, thereby avoiding the repetition of previous mistakes. Also, <span id="S5.SS1.p2.1.5" class="ltx_text ltx_font_italic">Reflexion</span> improves <span id="S5.SS1.p2.1.6" class="ltx_text ltx_font_italic">Self-Refine</span> by incorporating scalar-valued feedback and other forms of feedback.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">While self-correction has shown effective for a wide variety of text-generation tasks, this strategy requires the use of powerful, large-scale LLMs capable of refining text based on provided feedback. As noted by <cite class="ltx_cite ltx_citemacro_citet">Madaan et al. (<a href="#bib.bib79" title="" class="ltx_ref">2023</a>)</cite>, smaller, open-source models often struggle to refine their output effectively, even when the correct feedback is provided. A possible solution involves explicitly training models for this self-correction process. <span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_italic">SelFee</span> <cite class="ltx_cite ltx_citemacro_citep">(Ye et al., <a href="#bib.bib138" title="" class="ltx_ref">2023</a>)</cite> proposes training a model to emulate the self-correction process by generating output, feedback, and a refined solution in an auto-regressive manner. They use more powerful LLMs to provide feedback and refinement data, with data collection facilitated through ChatGPT.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Models/Tools as Feedback</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">As self-correction relies on the language model for feedback, the quality of the feedback is inherently constrained by the inherent limitations of LLMs, such as the inability to access up-to-date information, take actions, or perform precise mathematical reasoning. To address this, recent works have investigated the use of external tools for providing feedback. Illustrated in Figure <a href="#posthoc_methods" title="Figure 4 ‣ Figure 4 ‣ 4.1 Generate-then-Rank ‣ 4 Generation-Time Correction ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></span>(b)</a>, a broad array of external tools, including trained models, code interpreters, and search engines, can be integrated to provide specialized feedback.</p>
</div>
<section id="S5.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Code Interpreter.</h4>

<div id="S5.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px1.p1.1" class="ltx_p">In code generation, the program executor is frequently used as a source of feedback for refining the initial code written by the model. For example, <span id="S5.SS2.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">Self-Edit</span> <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib141" title="" class="ltx_ref">2023a</a>)</cite> and <span id="S5.SS2.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_italic">Self-Evolve</span> execute the initial program on example test cases and provide the execution results back as feedback. Afterward, an LLM is prompted to refine the initial code based on the feedback. <span id="S5.SS2.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_italic">Self-Debug</span> <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib13" title="" class="ltx_ref">2023c</a>)</cite> investigated using program explanation, unit tests, and program interpreter as feedback types. <span id="S5.SS2.SSS0.Px1.p1.1.4" class="ltx_text ltx_font_italic">ALGO</span> <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib142" title="" class="ltx_ref">2023b</a>)</cite> explored a more fine-grained feedback for code generation. For each problem, it first generates a reference oracle program that solves the problem with an exhaustive search. The feedback is collected by comparing the outputs from the LLM-generated program with the oracle outputs for a given set of test inputs. The self-correction strategy has also been adopted for formal verification of software. <cite class="ltx_cite ltx_citemacro_citet">Charalambous et al. (<a href="#bib.bib10" title="" class="ltx_ref">2023</a>)</cite> employed Bounded Model Checking to locate the software vulnerability and then used the LLM for correction.</p>
</div>
</section>
<section id="S5.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Logic Reasoner.</h4>

<div id="S5.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px2.p1.1" class="ltx_p">Tool-assisted feedback has also been used to enhance the faithfulness of LLMs’ reasoning. For example, Logic-LM <cite class="ltx_cite ltx_citemacro_cite">Pan et al. (<a href="#bib.bib88" title="" class="ltx_ref">2023</a>)</cite> solves a logical reasoning problem by first translating it into logical form with LLMs and then performing inference on it with external symbolic solvers. Due to the complexity of correctly parsing the problem at the first attempt, a self-refinement module is introduced to modify inaccurate logical forms using the error messages returned by the symbolic reasoner as feedback. Similarly, Baldur <cite class="ltx_cite ltx_citemacro_cite">First et al. (<a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite> uses existing search-based proof assistants as a source of feedback to improve language models’ ability to generate theorem proofs.</p>
</div>
</section>
<section id="S5.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">External Knowledge.</h4>

<div id="S5.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px3.p1.1" class="ltx_p">External knowledge is also frequently incorporated as a source of feedback to detect and revise factual errors in LLM’s output and to support LLM-generated facts with evidence or citations. For example, <span id="S5.SS2.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">RARR</span> <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a href="#bib.bib32" title="" class="ltx_ref">2023b</a>)</cite> and <span id="S5.SS2.SSS0.Px3.p1.1.2" class="ltx_text ltx_font_italic">REFEED</span> <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib139" title="" class="ltx_ref">2023</a>)</cite> directly prompt LLMs to raise questions about different aspects of the generated output. An external retriever then searches for evidence to investigate each query. Finally, a refine model is employed to amend the output based on any detected discrepancies between the output and the retrieved evidence. <span id="S5.SS2.SSS0.Px3.p1.1.3" class="ltx_text ltx_font_italic">LLM-Augmenter</span> <cite class="ltx_cite ltx_citemacro_cite">Peng et al. (<a href="#bib.bib91" title="" class="ltx_ref">2023</a>)</cite> proposes a similar method but differentiates itself by automatically generating natural language feedback based on the evidence retrieved. This feedback identifies error locations and provides revision suggestions. These models are evaluated on knowledge-intensive QA tasks. To broaden the adaptability, <span id="S5.SS2.SSS0.Px3.p1.1.4" class="ltx_text ltx_font_italic">FACTOOL</span> <cite class="ltx_cite ltx_citemacro_cite">Chern et al. (<a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite> extends knowledge-assisted factual error correction to a wider range of tasks, including code generation, mathematical reasoning, and scientific literature review.</p>
</div>
</section>
<section id="S5.SS2.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Trained Model.</h4>

<div id="S5.SS2.SSS0.Px4.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px4.p1.1" class="ltx_p">Existing works also fine-tune specialized models for feedback generation. These critic models can then be paired with similar or more potent language models in an iterative-refinement cycle. For example, <span id="S5.SS2.SSS0.Px4.p1.1.1" class="ltx_text ltx_font_italic">CodeRL</span> <cite class="ltx_cite ltx_citemacro_cite">Le et al. (<a href="#bib.bib59" title="" class="ltx_ref">2022</a>)</cite> treats program synthesis as a reinforcement learning task and trains a critic model whose output optimizes the main model. In contrast, <span id="S5.SS2.SSS0.Px4.p1.1.2" class="ltx_text ltx_font_italic">REFINER</span> <cite class="ltx_cite ltx_citemacro_cite">Paul et al. (<a href="#bib.bib90" title="" class="ltx_ref">2023</a>)</cite> trains a task model to produce an intermediate representation for problem-solving, while a critique model provides feedback on each intermediate training step. The critique model can subsequently be employed to generate feedback for larger task models, such as ChatGPT. Similarly, <span id="S5.SS2.SSS0.Px4.p1.1.3" class="ltx_text ltx_font_italic">RL4F</span> <cite class="ltx_cite ltx_citemacro_citep">(Akyürek et al., <a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite> utilizes reinforcement learning for training a critic while keeping the downstream task model fixed. The critic model is initially fine-tuned to produce feedback given an initial output and then further fine-tuned using a policy optimization method. The reward is defined by evaluating the accuracy of the refined output or comparing it with the ground truth, assuming that the downstream model can effectively refine the output if accurate feedback is provided.</p>
</div>
</section>
<section id="S5.SS2.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Integrating Multiple Tools.</h4>

<div id="S5.SS2.SSS0.Px5.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px5.p1.1" class="ltx_p">Broadening the idea of tool-assisted feedback, <span id="S5.SS2.SSS0.Px5.p1.1.1" class="ltx_text ltx_font_italic">CRITIC</span> <cite class="ltx_cite ltx_citemacro_citep">(Gou et al., <a href="#bib.bib38" title="" class="ltx_ref">2023</a>)</cite> integrates a variety of tools in a unified framework, including program interpreters for coding feedback, external knowledge and search engines for factual information, calculators for verifying mathematical equations, and LLM-based natural language feedback. Each tool is proficient at providing feedback for different aspects, contributing to a more comprehensive feedback system.</p>
</div>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Multi-Agent Debate</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Besides integrating external tools, recent studies have also explored the strategy of <span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_italic">debating between multiple LLMs</span>, drawing inspiration from collaborative intelligence, where diverse perspectives often converge to a more refined solution. This approach aims to improve the output quality by employing multiple instances of LLMs, each proposing and debating their individual responses over multiple rounds to arrive at a common final answer.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Du et al. (<a href="#bib.bib24" title="" class="ltx_ref">2023</a>)</cite> first applied and evaluated this strategy in arithmetic reasoning tasks. Each agent (a duplicate of LLM) initially generates its individual solution along with justifications. The debate phase involves collating responses from all agents and presenting this as context to each agent. Based on this context, each agent is then instructed to craft a revised response. The models are found to converge on a shared solution following multiple debate iterations. Experiments show that multi-agent debate leads to improved performance over the self-correction strategy. Furthering this concept, <span id="S5.SS3.p2.1.1" class="ltx_text ltx_font_italic">PRD</span> <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib66" title="" class="ltx_ref">2023c</a>)</cite> proposed the peer rank algorithm for better obtaining a consensus answer after debating. It considers pairwise preferences between all possible answer pairs from individual LLMs and uses these preferences to generate a final ranking of models.
</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">In addition to reasoning tasks, <span id="S5.SS3.p3.1.1" class="ltx_text ltx_font_italic">LM vs LM</span> <cite class="ltx_cite ltx_citemacro_cite">Cohen et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite> further demonstrated the effectiveness of multi-agent debate for detecting factual errors. The approach involves a generator LLM creating a claim, while an examiner LLM probes for factual inaccuracies through a multi-turn interaction.To broaden this concept, <cite class="ltx_cite ltx_citemacro_citet">Fu et al. (<a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite> demonstrated that interactions between different LLMs could mimic human behavior in real-world tasks. The study showcased this through a bargaining scenario where different LLM agents assumed the roles of buyer and seller. This further highlights the versatile applications of multi-agent debates.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Applications</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Following our above outline of automated correction techniques, we now will briefly discuss the various application domains for which automated correction is useful, and point out commonalities in self-correction strategies, and discuss how improvements to performance in self- or feedback-driven correction will give rise to downstream performance improvements.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Factual Correction</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">Many of the aforementioned automated correction strategies implicitly engage factual correction to improve various errors in output <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a href="#bib.bib32" title="" class="ltx_ref">2023b</a>); Peng et al. (<a href="#bib.bib91" title="" class="ltx_ref">2023</a>)</cite>.
It is therefore natural that these capabilities can be directly engaged for factuality detection and factual correction as an end task, such as in <span id="S6.SS1.p1.1.1" class="ltx_text ltx_font_italic">LM vs LM</span> <cite class="ltx_cite ltx_citemacro_cite">Cohen et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite> or <span id="S6.SS1.p1.1.2" class="ltx_text ltx_font_italic">Multiagent Debate</span> <cite class="ltx_cite ltx_citemacro_cite">Du et al. (<a href="#bib.bib24" title="" class="ltx_ref">2023</a>)</cite>.
When external tool use is acceptable, retrieved facts can be leveraged to further improve the factuality of generated text in a self-correcting manner without pure reliance on memorized knowledge <cite class="ltx_cite ltx_citemacro_cite">Gou et al. (<a href="#bib.bib38" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">In short, self-correcting is a foundational technique in many LLM-based fact correction or fact-checking systems, and <span id="S6.SS1.p2.1.1" class="ltx_text ltx_font_italic">models that are better at self-directed factual correction will probably be better in a host of other self-correction settings</span>.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Reasoning Tasks</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">In most <span id="S6.SS2.p1.1.1" class="ltx_text ltx_font_italic">reasoning tasks</span>, no good references from which outputs can be sanity-checked are readily available <cite class="ltx_cite ltx_citemacro_cite">Choi (<a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite>. This is unfortunate, as the “reasoning” capabilities provided by LLMs—in particular, their ability to <span id="S6.SS2.p1.1.2" class="ltx_text ltx_font_italic">operate on</span> natural language from <span id="S6.SS2.p1.1.3" class="ltx_text ltx_font_italic">instructions specified in</span> natural language—is a core driver for their popularity. <span id="S6.SS2.p1.1.4" class="ltx_text ltx_font_italic">Reasoning tasks</span> constitute a broad array of problems where this capacity is most necessary. For example, more complex multi-hop question answering tasks <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib135" title="" class="ltx_ref">2018</a>); Chen et al. (<a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite> can be construed as requiring both factual correction and reasoning correction capabilities <cite class="ltx_cite ltx_citemacro_cite">Ho et al. (<a href="#bib.bib43" title="" class="ltx_ref">2023</a>)</cite>. Thus the question becomes, <span id="S6.SS2.p1.1.5" class="ltx_text ltx_font_italic">how can we prompt the LLM to identify and correct intermediate reasoning errors?</span> Similarly to other application areas, both pure-LLM-driven and external tool-based error detection techniques have been proposed.
</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">LLM-based implementations of reasoning error detection include debate-based techniques, which can be thought of as implicitly rolling consistent reasoning enforcement in the “critic” module <cite class="ltx_cite ltx_citemacro_cite">Cohen et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>); Li et al. (<a href="#bib.bib66" title="" class="ltx_ref">2023c</a>)</cite>, and self-refinement techniques <cite class="ltx_cite ltx_citemacro_cite">Manakul et al. (<a href="#bib.bib80" title="" class="ltx_ref">2023</a>)</cite>.
In short, a given passage exhibiting reasoning (eg, step-by-step <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a href="#bib.bib120" title="" class="ltx_ref">2022b</a>)</cite>) is fed into an LLM, which is prompted to check for and/or correct reasoning errors directly. The error detection often collaborates with a decoding algorithm such as the beam search to lead the reasoning towards the correct direction <cite class="ltx_cite ltx_citemacro_cite">Hao et al. (<a href="#bib.bib40" title="" class="ltx_ref">2023</a>); Yao et al. (<a href="#bib.bib136" title="" class="ltx_ref">2023a</a>)</cite>.</p>
</div>
<div id="S6.SS2.p3" class="ltx_para">
<p id="S6.SS2.p3.1" class="ltx_p">External feedback using techniques such as natural language inference (NLI) can be both directly leveraged to spot errors as a heuristic for correction, and as a means to score the quality <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib132" title="" class="ltx_ref">2022a</a>); Golovneva et al. (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite>.
However, there are some open questions regarding the quality of the supervised learning-based tools like NLI <cite class="ltx_cite ltx_citemacro_cite">Srikanth and Rudinger (<a href="#bib.bib107" title="" class="ltx_ref">2022</a>); Saxon et al. (<a href="#bib.bib97" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S6.SS2.p4" class="ltx_para">
<p id="S6.SS2.p4.1" class="ltx_p">Among different types of reasoning, the self-correction strategy has been well studied and implemented for <span id="S6.SS2.p4.1.1" class="ltx_text ltx_font_italic">arithmetic reasoning</span>, as outlined in Table <a href="#S2.T1" title="Table 1 ‣ 2.4 What is the format of the feedback? ‣ 2 A Taxonomy for Correcting LLMs with Automated Feedback ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. One of the reasons for this skew is the relative ease of verifying intermediate reasoning steps within arithmetic problems. Some recent studies <cite class="ltx_cite ltx_citemacro_cite">Pan et al. (<a href="#bib.bib88" title="" class="ltx_ref">2023</a>); First et al. (<a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite> have started to extend the application of this strategy to deductive reasoning. However, the implementation of self-correction in a wider array of reasoning tasks, including inductive and abductive reasoning, is still relatively under-explored.
</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Code Synthesis</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">Code generation is a burgeoning application domain for LLMs for which correction is particularly important. Even human programmers tend to write code through an iterative process of addition and correction. For humans, strategies such as reading linter warnings, compiler/runtime errors, and incorrect outputs to diagnose necessary changes to the source are all employed in the software development process. Each of these strategies has natural analogues in the code generation pipeline.</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.1" class="ltx_p">The aforementioned warnings, errors, or outputs are usually fed directly back into the LLM to guide the code correction process <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib141" title="" class="ltx_ref">2023a</a>); Chen et al. (<a href="#bib.bib11" title="" class="ltx_ref">2023a</a>)</cite>. After all, compiler failures are a particularly strong signal that a piece of code will not work, having great utility in guiding LLM self-correction. More excitingly, other work proposed to utilize more fine-grained feedback such as program explanations generated by LLMs <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib13" title="" class="ltx_ref">2023c</a>)</cite> and the comparison with a reference oracle program <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib142" title="" class="ltx_ref">2023b</a>)</cite>.
</p>
</div>
<div id="S6.SS3.p3" class="ltx_para">
<p id="S6.SS3.p3.1" class="ltx_p">The above works only <span id="S6.SS3.p3.1.1" class="ltx_text ltx_font_italic">empirically</span> show that LLMs exhibited a remarkable capability for self-repairing the codes. In recent work, <cite class="ltx_cite ltx_citemacro_citet">Olausson et al. (<a href="#bib.bib84" title="" class="ltx_ref">2023</a>)</cite> further conducted an in-depth analysis of how and when self-repair works effectively. They found that self-repair is bottlenecked by the feedback stage: substantial performance improvements were only noticed when feedback was provided by expert human programmers or GPT-4. This revelation raises intriguing questions, such as whether self-repair is an emergent ability exclusive to certain LLMs and how could we endow smaller models with similar capabilities.</p>
</div>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Other Applications</h3>

<section id="S6.SS4.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Open-ended Generation.</h4>

<div id="S6.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S6.SS4.SSS0.Px1.p1.1" class="ltx_p">In addition to handling pure factuality issues, self-correction can be applied to subjective qualities of the generated text. These interventions include post-hoc <span id="S6.SS4.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">toxicity reduction</span> <cite class="ltx_cite ltx_citemacro_cite">Gou et al. (<a href="#bib.bib38" title="" class="ltx_ref">2023</a>); Welleck et al. (<a href="#bib.bib121" title="" class="ltx_ref">2023</a>)</cite>, enhancing the narrative quality in <span id="S6.SS4.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_italic">story generation</span> <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib134" title="" class="ltx_ref">2022b</a>)</cite>, and refining <span id="S6.SS4.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_italic">response generation</span> in dialogues <cite class="ltx_cite ltx_citemacro_cite">Ye et al. (<a href="#bib.bib138" title="" class="ltx_ref">2023</a>); Yu et al. (<a href="#bib.bib139" title="" class="ltx_ref">2023</a>)</cite>. Given the subjectivity involved in assessing the outputs, these studies often rely on detailed, natural language feedback and employ an iterative refinement strategy for post-hoc refinement.</p>
</div>
</section>
<section id="S6.SS4.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Machine Translation.</h4>

<div id="S6.SS4.SSS0.Px2.p1" class="ltx_para">
<p id="S6.SS4.SSS0.Px2.p1.1" class="ltx_p">The concept of post-hoc self-correction has deep roots in the field of machine translation (MT), where it is often called <span id="S6.SS4.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">Automatic Post-Editing</span> (APE) <cite class="ltx_cite ltx_citemacro_cite">do Carmo et al. (<a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite>. A long line of prior works train models to fix translation errors by either learning from human correction data <cite class="ltx_cite ltx_citemacro_cite">Alabau et al. (<a href="#bib.bib2" title="" class="ltx_ref">2014</a>)</cite> or from synthetic training data <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a href="#bib.bib61" title="" class="ltx_ref">2021</a>)</cite>. To minimize the cost of data collection, recent works <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib12" title="" class="ltx_ref">2023b</a>); Raunak et al. (<a href="#bib.bib94" title="" class="ltx_ref">2023</a>)</cite> have leveraged the in-context learning ability of LLMs for post-editing translations. Besides post-hoc methods, training-time correction <cite class="ltx_cite ltx_citemacro_cite">Unanue et al. (<a href="#bib.bib112" title="" class="ltx_ref">2021</a>)</cite> and decoding-time correction <cite class="ltx_cite ltx_citemacro_cite">Freitag et al. (<a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite> are also adopted by prior works.</p>
</div>
</section>
<section id="S6.SS4.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Summarization.</h4>

<div id="S6.SS4.SSS0.Px3.p1" class="ltx_para">
<p id="S6.SS4.SSS0.Px3.p1.1" class="ltx_p">Automated model correction is commonly used in summarization to ensure the <span id="S6.SS4.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">factuality</span> of the generated summary. There are two mainstream methods: 1) training-time correction that imposes factuality constraints during training <cite class="ltx_cite ltx_citemacro_cite">Liu and Liu (<a href="#bib.bib75" title="" class="ltx_ref">2021</a>); Wan and Bansal (<a href="#bib.bib113" title="" class="ltx_ref">2022</a>); Scheurer et al. (<a href="#bib.bib98" title="" class="ltx_ref">2023</a>)</cite>, and 2) post-hoc correction that post-edits generated summaries to correct factual errors <cite class="ltx_cite ltx_citemacro_cite">Cao et al. (<a href="#bib.bib6" title="" class="ltx_ref">2020</a>); Saunders et al. (<a href="#bib.bib96" title="" class="ltx_ref">2022</a>)</cite>. Recent works have investigated using RL to refine the model guided by automated feedback from either reward models <cite class="ltx_cite ltx_citemacro_cite">Akyürek et al. (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite> or language models <cite class="ltx_cite ltx_citemacro_cite">Pang et al. (<a href="#bib.bib89" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Research Gaps and Future Directions</h2>

<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Theoretical Justifications</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">Although LLMs have exhibited a remarkable capability for self-analysis and self-improvement, there remains a lack of theoretical justifications to uncover the mystery of such ability. Therefore, we argue that the study of underlying theoretical principles can offer a more transparent understanding of self-correction. Subsequently, we propose several potential directions for such explorations.</p>
</div>
<div id="S7.SS1.p2" class="ltx_para">
<p id="S7.SS1.p2.1" class="ltx_p">The ability of language models to self-correct is closely associated with their capacity to exhibit metacognitive awareness, <span id="S7.SS1.p2.1.1" class="ltx_text ltx_font_italic">i.e.</span>, their understanding of their own knowledge and uncertainties <cite class="ltx_cite ltx_citemacro_citep">(Kadavath et al., <a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>. Similarly, the notion of <span id="S7.SS1.p2.1.2" class="ltx_text ltx_font_italic">calibration</span> in language models, referring to their ability to produce well-calibrated predictions with probabilities aligning closely with observed frequencies of outcomes, is of paramount importance <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib73" title="" class="ltx_ref">2023</a>)</cite>. Recent research by <cite class="ltx_cite ltx_citemacro_citet">Kadavath et al. (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite> reveals that pre-trained language models, when presented with properly formatted multiple-choice and true/false questions, demonstrate good calibration. Particularly, language models exhibit well-calibrated responses to self-evaluation questions in few-shot settings. On the other hand, fine-tuned language models, such as those incorporating RLHF, require temperature adjustments to achieve calibration since the model distribution is tailored to optimize specific behaviors.</p>
</div>
<div id="S7.SS1.p3" class="ltx_para">
<p id="S7.SS1.p3.1" class="ltx_p">While language models demonstrate some capacity for self-feedback, achieving superior performance often necessitates incorporating external feedback signals. The integration of feedback signals is closely linked to the alignment of language models, a domain that still lacks comprehensive understanding. For example, in RLHF, the choice of the metric to minimize between the reward model output and the final model output significantly impacts downstream task performance <cite class="ltx_cite ltx_citemacro_citep">(Go et al., <a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite>, yet this aspect remains underexplored in many applications. Furthermore, the optimal method for automatically generating prompts to instruct language models effectively, for tasks such as evaluating and refining their outputs, remains an open question. Although <cite class="ltx_cite ltx_citemacro_citet">Sordoni et al. (<a href="#bib.bib106" title="" class="ltx_ref">2023</a>)</cite> have addressed this issue by treating natural language prompts as parameters of the language model and performing discrete optimization, their approach still requires hand-crafted meta prompts to implement the algorithm.</p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Measuring the Ability of Self-Correction</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.1" class="ltx_p">Despite the promising progress in enabling LLMs to self-correct and improve their outputs, current research only provides <span id="S7.SS2.p1.1.1" class="ltx_text ltx_font_italic">empirical</span> evidence of its effectiveness across diverse applications. However, a gap remains in establishing robust quantitative metrics to understand and evaluate the self-correction capability of LLMs. While various strategies have been proposed to rectify LLM outputs, a comprehensive comparative evaluation of these approaches is still missing. This includes metrics to assess the effectiveness, applicability, complexity, and potential upper-bound limits of each strategy within a unified context. Future research could aim to create comprehensive evaluation frameworks which take into account variables such as the complexity of the task, the degree of initial error, the improvement in quality after self-correction, etc.</p>
</div>
<div id="S7.SS2.p2" class="ltx_para">
<p id="S7.SS2.p2.1" class="ltx_p">In addition to this, building benchmarks to diagnose self-correction capabilities represents another promising research direction. Such diagnostic datasets would enable a more standardized, objective evaluation of different LLMs and self-correction strategies, driving the development of more accurate and efficient models.</p>
</div>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span>Continual Self-Improvement</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p id="S7.SS3.p1.1" class="ltx_p">Another promising yet under-explored area of LLM self-correction is the idea of continual, life-long self-improvement. As LLMs are utilized in more diverse, dynamic, and real-time contexts, the ability to adapt and improve continually over time becomes essential. This is closely related to the concept of continual (life-long) learning  <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib115" title="" class="ltx_ref">2023b</a>)</cite>, where the model continually learns new skills and adapts to novel environments and contexts. Translating this to self-correction implies that LLMs continuously evaluate their outputs, learn from errors, update their knowledge, and adapt their decision-making strategies accordingly.</p>
</div>
<div id="S7.SS3.p2" class="ltx_para">
<p id="S7.SS3.p2.1" class="ltx_p">Studies on self-training such as  <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a href="#bib.bib45" title="" class="ltx_ref">2022</a>); Zelikman et al. (<a href="#bib.bib140" title="" class="ltx_ref">2022</a>)</cite> have evidenced that LLMs can self-improve by continuously training on their own outputs that are positively evaluated by humans or models. However, these studies typically concentrate on a single, one-time correction process and evaluate improvements in a particular aspect. The robustness and stability of self-training under continual settings remain uncertain. For example, a major challenge of continual learning is catastrophic forgetting <cite class="ltx_cite ltx_citemacro_cite">Kirkpatrick et al. (<a href="#bib.bib55" title="" class="ltx_ref">2016</a>)</cite>, where the acquisition of new skills often leads to a considerable decrease in previous capabilities. It is unclear whether similar issues may emerge in a continual self-improve LLM, such as whether correcting one behavior may unintentionally alter a previously corrected behavior.</p>
</div>
<div id="S7.SS3.p3" class="ltx_para">
<p id="S7.SS3.p3.1" class="ltx_p">Finally, exploring how to integrate various self-correction techniques to efficiently build a continual self-improvement LLM is also worth investigating. For example, post-hoc correction represents a more immediate and less costly strategy, while training-time correction addresses model behavior more fundamentally, with a higher computational cost. To combine these strategies, post-hoc correction could be used to collect training data (<span id="S7.SS3.p3.1.1" class="ltx_text ltx_font_italic">e.g.</span>, most frequently made mistakes and their corrections), which are used to guide the periodically training-time correction to address these recurring issues permanently.
</p>
</div>
</section>
<section id="S7.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4 </span>Self-Correction with Model Editing</h3>

<div id="S7.SS4.p1" class="ltx_para">
<p id="S7.SS4.p1.1" class="ltx_p">Recent years have witnessed a surge in techniques for <span id="S7.SS4.p1.1.1" class="ltx_text ltx_font_italic">model editing</span> <cite class="ltx_cite ltx_citemacro_cite">Sinitsin et al. (<a href="#bib.bib105" title="" class="ltx_ref">2020</a>); Cao et al. (<a href="#bib.bib7" title="" class="ltx_ref">2021</a>); Yao et al. (<a href="#bib.bib137" title="" class="ltx_ref">2023b</a>)</cite>, aiming to adjust the model’s behavior for examples within the editing scope while leaving its performance for out-of-scope examples unaltered. Model editing has been employed in updating outdated knowledge embedded in LLMs <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a href="#bib.bib60" title="" class="ltx_ref">2022</a>); Onoe et al. (<a href="#bib.bib85" title="" class="ltx_ref">2023</a>)</cite> and in addressing issues related to false associations memorized during LLM training <cite class="ltx_cite ltx_citemacro_cite">Murty et al. (<a href="#bib.bib82" title="" class="ltx_ref">2022</a>); Tanno et al. (<a href="#bib.bib110" title="" class="ltx_ref">2022</a>)</cite>. Current model editing methods have shown some efficacy in adjusting factual knowledge within LLMs, yet they still suffer problems such as a lack of robust generalization capabilities <cite class="ltx_cite ltx_citemacro_cite">Yao et al. (<a href="#bib.bib137" title="" class="ltx_ref">2023b</a>)</cite> and the introduction of substantial unintended side effects <cite class="ltx_cite ltx_citemacro_cite">Hoelscher-Obermaier et al. (<a href="#bib.bib44" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S7.SS4.p2" class="ltx_para">
<p id="S7.SS4.p2.1" class="ltx_p">Nevertheless, the advancements in model editing present promising opportunities for the self-correction of LLMs. Primarily, model editing enables accurate, fine-grained corrections at the level of individual neurons or layers, circumventing the need for extensive retraining associated with training-time correction. Moreover, through the analysis of the impact of model edits, we can deepen our understanding of the self-correction mechanism. Further, methods developed to curtail undesired side effects in model editing <cite class="ltx_cite ltx_citemacro_cite">Hoelscher-Obermaier et al. (<a href="#bib.bib44" title="" class="ltx_ref">2023</a>)</cite> could foster more robust self-correction strategies by mitigating the issue of inadvertently introducing new errors while resolving existing ones. Therefore, we forecast future research to incorporate model editing into LLM self-correction processes, an under-explored area.</p>
</div>
</section>
<section id="S7.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.5 </span>Multi-Modal Self-Correction</h3>

<div id="S7.SS5.p1" class="ltx_para">
<p id="S7.SS5.p1.1" class="ltx_p">As discussed in Section <a href="#S6" title="6 Applications ‣ Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, self-correction strategies for LLMs have been successfully employed in an extensive array of NLP tasks. However, most existing works are limited to the textual modality, where both the model outputs and the feedback are in textual form. The recent surge in multi-modal data usage, including image, audio, and video modalities, presents enticing opportunities for expansion. These include the exploration of self-correction capabilities within multi-modal LLMs, the incorporation of visual feedback, and improving vision-language tasks through self-correction.</p>
</div>
<div id="S7.SS5.p2" class="ltx_para">
<p id="S7.SS5.p2.1" class="ltx_p">A handful of pioneering studies have investigated this domain. For example, <span id="S7.SS5.p2.1.1" class="ltx_text ltx_font_italic">MaskGIT</span> <cite class="ltx_cite ltx_citemacro_cite">Chang et al. (<a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite> employed a self-refinement approach to image generation, where the model progressively refines the generated image conditioned on the previous generation. <cite class="ltx_cite ltx_citemacro_citet">Ke et al. (<a href="#bib.bib50" title="" class="ltx_ref">2019</a>)</cite> utilized a self-correction strategy for vision-and-language navigation. However, despite these initial explorations, self-correction strategies are yet to be broadly adopted in multi-modal settings. A comprehensive understanding of how self-correction methods generalize across various modalities is crucial for improving their robustness and versatility.</p>
</div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">In this paper, we present a comprehensive survey of self-correcting large language models with automated feedback. We broadly categorize and analyze various self-correction strategies, including training-time, generation-time, and post-hoc corrections. We also discuss the major application areas of self-correction, including correcting factual errors, enhancing reasoning abilities, and improving code generation, among others. Finally, we outline a number of potential future directions and associated challenges in this field. Our goal with this paper is to provide a comprehensive and useful resource for readers interested in the development of this rapidly evolving domain. To aid in this effort, we create a continually-updated reading list in a GitHub repository: <a href="https://github.com/teacherpeterpan/self-correction-llm-papers" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/teacherpeterpan/self-correction-llm-papers</a>.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work was supported by the National Science Foundation Award #2048122. The views expressed are those of the authors and do not reflect the official policy or position of the US government. Thanks to Xinyuan Lu for assisting with the Github reading list repo.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Akyürek et al. (2023)</span>
<span class="ltx_bibblock">
Afra Feyza Akyürek, Ekin Akyürek, Ashwin Kalyan, Peter Clark,
Derry Tanti Wijaya, and Niket Tandon. 2023.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/2023.acl-long.427" title="" class="ltx_ref ltx_href">RL4F:
generating natural language feedback with reinforcement learning for
repairing model outputs</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (ACL)</em>, pages 7716–7733.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alabau et al. (2014)</span>
<span class="ltx_bibblock">
Vicent Alabau, Christian Buck, Michael Carl, Francisco Casacuberta, Mercedes
García-Martínez, Ulrich Germann, Jesús
González-Rubio, Robin L. Hill, Philipp Koehn, Luis A. Leiva,
Bartolomé Mesa-Lao, Daniel Ortiz-Martínez, Herve
Saint-Amand, Germán Sanchis-Trilles, and Chara Tsoukala. 2014.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.3115/v1/e14-2007" title="" class="ltx_ref ltx_href">CASMACAT: A
computer-assisted translation workbench</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 14th Conference of the European Chapter
of the Association for Computational Linguistics (EACL)</em>, pages 25–28. The
Association for Computer Linguistics.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2022a)</span>
<span class="ltx_bibblock">
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph,
Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El Showk, Nelson Elhage,
Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna
Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom B.
Brown, Jack Clark, Sam McCandlish, Chris Olah, Benjamin Mann, and Jared
Kaplan. 2022a.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2204.05862" title="" class="ltx_ref ltx_href">Training a helpful
and harmless assistant with reinforcement learning from human feedback</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2204.05862.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2022b)</span>
<span class="ltx_bibblock">
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,
Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,
Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain,
Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared
Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosiute,
Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer,
Noemí Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam
Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera
Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume,
Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas
Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. 2022b.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2212.08073" title="" class="ltx_ref ltx_href">Constitutional
AI: harmlessness from AI feedback</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2212.08073.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Begus et al. (2023)</span>
<span class="ltx_bibblock">
Gasper Begus, Maksymilian Dabkowski, and Ryan Rhodes. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.00948" title="" class="ltx_ref ltx_href">Large linguistic
models: Analyzing theoretical linguistic abilities of llms</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.00948.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. (2020)</span>
<span class="ltx_bibblock">
Meng Cao, Yue Dong, Jiapeng Wu, and Jackie Chi Kit Cheung. 2020.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/2020.emnlp-main.506" title="" class="ltx_ref ltx_href">Factual error
correction for abstractive summarization models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 6251–6258.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. (2021)</span>
<span class="ltx_bibblock">
Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.18653/v1/2021.emnlp-main.522" title="" class="ltx_ref ltx_href">Editing
factual knowledge in language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 6491–6506.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al. (2022)</span>
<span class="ltx_bibblock">
Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. 2022.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.1109/CVPR52688.2022.01103" title="" class="ltx_ref ltx_href">Maskgit: Masked
generative image transformer</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, pages 11305–11315. IEEE.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al. (2023)</span>
<span class="ltx_bibblock">
Jonathan D. Chang, Kianté Brantley, Rajkumar Ramamurthy, Dipendra Misra,
and Wen Sun. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2306.11816" title="" class="ltx_ref ltx_href">Learning to
generate better than your LLM</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2306.11816.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Charalambous et al. (2023)</span>
<span class="ltx_bibblock">
Yiannis Charalambous, Norbert Tihanyi, Ridhi Jain, Youcheng Sun, Mohamed Amine
Ferrag, and Lucas C. Cordeiro. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.14752" title="" class="ltx_ref ltx_href">A new era in
software security: Towards self-healing software via large language models
and formal verification</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.14752.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023a)</span>
<span class="ltx_bibblock">
Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou,
and Weizhu Chen. 2023a.

</span>
<span class="ltx_bibblock"><a href="https://openreview.net/pdf?id=ktrw68Cmu9c" title="" class="ltx_ref ltx_href">Codet: Code
generation with generated tests</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 11th International Conference on Learning
Representations (ICLR)</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023b)</span>
<span class="ltx_bibblock">
Pinzhen Chen, Zhicheng Guo, Barry Haddow, and Kenneth Heafield.
2023b.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2306.03856" title="" class="ltx_ref ltx_href">Iterative
translation refinement with large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2306.03856.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023c)</span>
<span class="ltx_bibblock">
Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.
2023c.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2304.05128" title="" class="ltx_ref ltx_href">Teaching large
language models to self-debug</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2304.05128.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2021)</span>
<span class="ltx_bibblock">
Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan
Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan R. Routledge, and
William Yang Wang. 2021.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.18653/v1/2021.emnlp-main.300" title="" class="ltx_ref ltx_href">Finqa: A
dataset of numerical reasoning over financial data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 3697–3711.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chern et al. (2023)</span>
<span class="ltx_bibblock">
I-Chun Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou,
Junxian He, Graham Neubig, and Pengfei Liu. 2023.

</span>
<span class="ltx_bibblock"><a href="https://arxiv.org/abs/2307.13528" title="" class="ltx_ref ltx_href">Factool: Factuality
detection in generative ai – a tool augmented framework for multi-task and
multi-domain scenarios</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2307.13528.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi (2023)</span>
<span class="ltx_bibblock">
Yejin Choi. 2023.

</span>
<span class="ltx_bibblock"><a href="https://dl.acm.org/doi/10.5555/3545946.3598609" title="" class="ltx_ref ltx_href">Common sense:
The dark matter of language and intelligence</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 International Conference on
Autonomous Agents and Multiagent Systems (AAMAS)</em>, page 2. ACM.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et al. (2021)</span>
<span class="ltx_bibblock">
Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan,
and Noah A. Smith. 2021.

</span>
<span class="ltx_bibblock"><a href="https://arxiv.org/abs/2107.00061" title="" class="ltx_ref ltx_href">All that’s ’human’ is not
gold: Evaluating human evaluation of generated text</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Processings of the 59th Annual Meeting of the Association
for Computational Linguistics (ACL)</em>, pages 7282–7296.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe et al. (2021)</span>
<span class="ltx_bibblock">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
Christopher Hesse, and John Schulman. 2021.

</span>
<span class="ltx_bibblock"><a href="https://arxiv.org/abs/2110.14168" title="" class="ltx_ref ltx_href">Training verifiers to solve
math word problems</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2110.14168.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cohen et al. (2023)</span>
<span class="ltx_bibblock">
Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.13281" title="" class="ltx_ref ltx_href">LM vs LM:
detecting factual errors via cross examination</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.13281.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Creswell and Shanahan (2022)</span>
<span class="ltx_bibblock">
Antonia Creswell and Murray Shanahan. 2022.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2208.14271" title="" class="ltx_ref ltx_href">Faithful reasoning
using large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2208.14271.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dasgupta et al. (2022)</span>
<span class="ltx_bibblock">
Ishita Dasgupta, Andrew K. Lampinen, Stephanie C. Y. Chan, Antonia Creswell,
Dharshan Kumaran, James L. McClelland, and Felix Hill. 2022.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2207.07051" title="" class="ltx_ref ltx_href">Language models
show human-like content effects on reasoning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2207.07051.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dathathri et al. (2020)</span>
<span class="ltx_bibblock">
Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero
Molino, Jason Yosinski, and Rosanne Liu. 2020.

</span>
<span class="ltx_bibblock"><a href="https://openreview.net/forum?id=H1edEyBKDS" title="" class="ltx_ref ltx_href">Plug and play
language models: A simple approach to controlled text generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 8th International Conference on Learning
Representations (ICLR)</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">do Carmo et al. (2021)</span>
<span class="ltx_bibblock">
Félix do Carmo, Dimitar Shterionov, Joss Moorkens, Joachim Wagner, Murhaf
Hossari, Eric Paquin, Dag Schmidtke, Declan Groves, and Andy Way. 2021.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.1007/s10590-020-09252-y" title="" class="ltx_ref ltx_href">A review of the
state-of-the-art in automatic post-editing</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Machine Translation</em>, 35(2):101–143.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al. (2023)</span>
<span class="ltx_bibblock">
Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch.
2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.14325" title="" class="ltx_ref ltx_href">Improving
factuality and reasoning in language models through multiagent debate</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.14325.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubois et al. (2023)</span>
<span class="ltx_bibblock">
Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba,
Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.14387" title="" class="ltx_ref ltx_href">Alpacafarm: A
simulation framework for methods that learn from human feedback</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.14387.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fernandes et al. (2023)</span>
<span class="ltx_bibblock">
Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas,
Pedro Henrique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan
Zhou, Tongshuang Wu, Graham Neubig, and André F. T. Martins. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.00955" title="" class="ltx_ref ltx_href">Bridging the gap:
A survey on integrating (human) feedback for natural language generation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.00955.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">First et al. (2023)</span>
<span class="ltx_bibblock">
Emily First, Markus N. Rabe, Talia Ringer, and Yuriy Brun. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2303.04910" title="" class="ltx_ref ltx_href">Baldur:
Whole-proof generation and repair with large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2303.04910.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freitag et al. (2022)</span>
<span class="ltx_bibblock">
Markus Freitag, David Grangier, Qijun Tan, and Bowen Liang. 2022.

</span>
<span class="ltx_bibblock"><a href="https://transacl.org/ojs/index.php/tacl/article/view/3735" title="" class="ltx_ref ltx_href">High quality rather than high model probability: Minimum bayes risk decoding
with neural metrics</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics
(TACL)</em>, pages 811–825.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al. (2023)</span>
<span class="ltx_bibblock">
Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.10142" title="" class="ltx_ref ltx_href">Improving language
model negotiation with self-play and in-context learning from AI feedback</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.10142.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ganguli et al. (2023)</span>
<span class="ltx_bibblock">
Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I. Liao, Kamile
Lukosiute, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny
Hernandez, Dawn Drain, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jackson
Kernion, Jamie Kerr, Jared Mueller, Joshua Landau, Kamal Ndousse, Karina
Nguyen, Liane Lovitt, Michael Sellitto, Nelson Elhage, Noemí Mercado,
Nova DasSarma, Oliver Rausch, Robert Lasenby, Robin Larson, Sam Ringer,
Sandipan Kundu, Saurav Kadavath, Scott Johnston, Shauna Kravec, Sheer El
Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Henighan, Tristan Hume,
Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph,
Sam McCandlish, Tom Brown, Christopher Olah, Jack Clark, Samuel R. Bowman,
and Jared Kaplan. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2302.07459" title="" class="ltx_ref ltx_href">The capacity for
moral self-correction in large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2302.07459.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2023a)</span>
<span class="ltx_bibblock">
Ge Gao, Hung-Ting Chen, Yoav Artzi, and Eunsol Choi. 2023a.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.12473" title="" class="ltx_ref ltx_href">Continually
improving extractive QA via human feedback</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.12473.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2023b)</span>
<span class="ltx_bibblock">
Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty,
Yicheng Fan, Vincent Y. Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin
Guu. 2023b.

</span>
<span class="ltx_bibblock"><a href="https://arxiv.org/abs/2210.08726" title="" class="ltx_ref ltx_href">Rarr: Researching and
revising what language models say, using language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61th Annual Meeting of the Association
for Computational Linguistics (ACL)</em>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gehman et al. (2020)</span>
<span class="ltx_bibblock">
Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith.
2020.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/2020.findings-emnlp.301" title="" class="ltx_ref ltx_href">RealToxicityPrompts: Evaluating neural toxic degeneration in language
models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
EMNLP 2020</em>, pages 3356–3369.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gero et al. (2023)</span>
<span class="ltx_bibblock">
Zelalem Gero, Chandan Singh, Hao Cheng, Tristan Naumann, Michel Galley,
Jianfeng Gao, and Hoifung Poon. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2306.00024" title="" class="ltx_ref ltx_href">Self-verification
improves few-shot clinical information extraction</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2306.00024.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Glaese et al. (2022)</span>
<span class="ltx_bibblock">
Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo
Ewalds, Maribeth Rauh, Laura Weidinger, Martin J. Chadwick, Phoebe Thacker,
Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona
Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie
Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Sona Mokrá,
Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel,
William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne
Hendricks, and Geoffrey Irving. 2022.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2209.14375" title="" class="ltx_ref ltx_href">Improving
alignment of dialogue agents via targeted human judgements</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2209.14375.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Go et al. (2023)</span>
<span class="ltx_bibblock">
Dongyoung Go, Tomasz Korbak, Germán Kruszewski, Jos Rozen, Nahyeon Ryu,
and Marc Dymetman. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2302.08215" title="" class="ltx_ref ltx_href">Aligning language
models with preferences through f-divergence minimization</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2302.08215.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Golovneva et al. (2023)</span>
<span class="ltx_bibblock">
Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer,
Maryam Fazel-Zarandi, and Asli Celikyilmaz. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2212.07919" title="" class="ltx_ref ltx_href">ROSCOE: A
suite of metrics for scoring step-by-step reasoning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 11th International Conference on Learning
Representations (ICLR)</em>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gou et al. (2023)</span>
<span class="ltx_bibblock">
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and
Weizhu Chen. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.11738" title="" class="ltx_ref ltx_href">CRITIC: large
language models can self-correct with tool-interactive critiquing</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.11738.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2023)</span>
<span class="ltx_bibblock">
Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding,
Jianwei Yue, and Yupeng Wu. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2301.07597" title="" class="ltx_ref ltx_href">How close is
chatgpt to human experts? comparison corpus, evaluation, and detection</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2301.07597.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hao et al. (2023)</span>
<span class="ltx_bibblock">
Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and
Zhiting Hu. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.14992" title="" class="ltx_ref ltx_href">Reasoning with
language model is planning with world model</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.14992.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2023)</span>
<span class="ltx_bibblock">
Hangfeng He, Hongming Zhang, and Dan Roth. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2301.00303" title="" class="ltx_ref ltx_href">Rethinking with
retrieval: Faithful large language model inference</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2301.00303.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2021)</span>
<span class="ltx_bibblock">
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021.

</span>
<span class="ltx_bibblock"><a href="https://openreview.net/forum?id=XPZIaotutsD" title="" class="ltx_ref ltx_href">Deberta:
decoding-enhanced bert with disentangled attention</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of The 9th International Conference on Learning
Representations (ICLR)</em>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al. (2023)</span>
<span class="ltx_bibblock">
Matthew Ho, Aditya Sharma, Justin Chang, Michael Saxon, Sharon Levy, Yujie Lu,
and William Yang Wang. 2023.

</span>
<span class="ltx_bibblock"><a href="https://openreview.net/forum?id=vaxnu-Utr4l" title="" class="ltx_ref ltx_href">Wikiwhy:
Answering and explaining cause-and-effect questions</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 11th International Conference on Learning
Representations (ICLR)</em>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoelscher-Obermaier et al. (2023)</span>
<span class="ltx_bibblock">
Jason Hoelscher-Obermaier, Julia Persson, Esben Kran, Ioannis Konstas, and
Fazl Barez. 2023.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/2023.findings-acl.733" title="" class="ltx_ref ltx_href">Detecting
edit failures in large language models: An improved specificity benchmark</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
ACL 2023</em>, pages 11548–11559.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2022)</span>
<span class="ltx_bibblock">
Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu,
and Jiawei Han. 2022.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2210.11610" title="" class="ltx_ref ltx_href">Large language
models can self-improve</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2210.11610.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jang et al. (2017)</span>
<span class="ltx_bibblock">
Eric Jang, Shixiang Gu, and Ben Poole. 2017.

</span>
<span class="ltx_bibblock"><a href="https://openreview.net/forum?id=rkE3y85ee" title="" class="ltx_ref ltx_href">Categorical
reparameterization with gumbel-softmax</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of 5th International Conference on Learning
Representations (ICLR)</em>.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023)</span>
<span class="ltx_bibblock">
Shuyang Jiang, Yuhao Wang, and Yu Wang. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2306.02907" title="" class="ltx_ref ltx_href">Selfevolve: A
code evolution framework via large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2306.02907.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jung et al. (2022)</span>
<span class="ltx_bibblock">
Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula,
Ronan Le Bras, and Yejin Choi. 2022.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/2022.emnlp-main.82" title="" class="ltx_ref ltx_href">Maieutic
prompting: Logically consistent reasoning with recursive explanations</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 1266–1279.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kadavath et al. (2022)</span>
<span class="ltx_bibblock">
Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan
Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli
Tran-Johnson, Scott Johnston, Sheer El Showk, Andy Jones, Nelson Elhage,
Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep
Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec,
Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom
Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and
Jared Kaplan. 2022.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2207.05221" title="" class="ltx_ref ltx_href">Language models
(mostly) know what they know</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2207.05221.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ke et al. (2019)</span>
<span class="ltx_bibblock">
Liyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtzman, Zhe Gan, Jingjing Liu,
Jianfeng Gao, Yejin Choi, and Siddhartha S. Srinivasa. 2019.

</span>
<span class="ltx_bibblock"><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Ke_Tactical_Rewind_Self-Correction_via_Backtracking_in_Vision-And-Language_Navigation_CVPR_2019_paper.html" title="" class="ltx_ref ltx_href">Tactical rewind: Self-correction via backtracking in vision-and-language
navigation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>, pages 6741–6749.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kenton et al. (2021)</span>
<span class="ltx_bibblock">
Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik,
and Geoffrey Irving. 2021.

</span>
<span class="ltx_bibblock"><a href="https://arxiv.org/abs/2103.14659" title="" class="ltx_ref ltx_href">Alignment of language
agents</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2103.14659.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Keskar et al. (2019)</span>
<span class="ltx_bibblock">
Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and
Richard Socher. 2019.

</span>
<span class="ltx_bibblock"><a href="http://arxiv.org/abs/1909.05858" title="" class="ltx_ref ltx_href">CTRL: A conditional
transformer language model for controllable generation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1909.05858.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khalifa et al. (2023)</span>
<span class="ltx_bibblock">
Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, and Lu Wang.
2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.14934" title="" class="ltx_ref ltx_href">Discriminator-guided multi-step reasoning with language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.14934.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2023)</span>
<span class="ltx_bibblock">
Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2303.17491" title="" class="ltx_ref ltx_href">Language models
can solve computer tasks</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2303.17491.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirkpatrick et al. (2016)</span>
<span class="ltx_bibblock">
James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume
Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and
Raia Hadsell. 2016.

</span>
<span class="ltx_bibblock"><a href="http://arxiv.org/abs/1612.00796" title="" class="ltx_ref ltx_href">Overcoming catastrophic
forgetting in neural networks</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1612.00796.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kojima et al. (2022)</span>
<span class="ltx_bibblock">
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
Iwasawa. 2022.

</span>
<span class="ltx_bibblock"><a href="http://papers.nips.cc/paper_files/paper/2022/hash/8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html" title="" class="ltx_ref ltx_href">Large language models are zero-shot reasoners</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Annual Conference on Neural
Information Processing Systems (NeurIPS)</em>.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Korbak et al. (2023)</span>
<span class="ltx_bibblock">
Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Bhalerao, Christopher L.
Buckley, Jason Phang, Samuel R. Bowman, and Ethan Perez. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2302.08582" title="" class="ltx_ref ltx_href">Pretraining
language models with human preferences</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2302.08582.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kreutzer et al. (2018)</span>
<span class="ltx_bibblock">
Julia Kreutzer, Shahram Khadivi, Evgeny Matusov, and Stefan Riezler. 2018.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/N18-3012" title="" class="ltx_ref ltx_href">Can neural machine
translation be improved with user feedback?</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies (NAACL-HIT)</em>.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le et al. (2022)</span>
<span class="ltx_bibblock">
Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and
Steven Chu-Hong Hoi. 2022.

</span>
<span class="ltx_bibblock"><a href="http://papers.nips.cc/paper_files/paper/2022/hash/8636419dea1aa9fbd25fc4248e702da4-Abstract-Conference.html" title="" class="ltx_ref ltx_href">Coderl: Mastering code generation through pretrained models and deep
reinforcement learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Annual Conference on Neural Information
Processing Systems (NeurIPS)</em>.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2022)</span>
<span class="ltx_bibblock">
Kyungjae Lee, Wookje Han, Seung-won Hwang, Hwaran Lee, Joonsuk Park, and
Sang-Woo Lee. 2022.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.18653/v1/2022.findings-acl.37" title="" class="ltx_ref ltx_href">Plug-and-play adaptation for continuously-updated QA</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
ACL 2022</em>, pages 438–447.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2021)</span>
<span class="ltx_bibblock">
WonKee Lee, Baikjin Jung, Jaehun Shin, and Jong-Hyeok Lee. 2021.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/2021.eacl-main.322" title="" class="ltx_ref ltx_href">Adaptation of
back-translation to automatic post-editing for synthetic data generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 16th Conference of the European Chapter
of the Association for Computational Linguistics (EACL)</em>, pages 3685–3691.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levy et al. (2022)</span>
<span class="ltx_bibblock">
Sharon Levy, Emily Allaway, Melanie Subbiah, Lydia Chilton, Desmond Patton,
Kathleen McKeown, and William Yang Wang. 2022.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/2022.emnlp-main.154" title="" class="ltx_ref ltx_href">SafeText: A
benchmark for exploring physical safety in language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 2407–2421.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levy et al. (2021)</span>
<span class="ltx_bibblock">
Sharon Levy, Michael Saxon, and William Yang Wang. 2021.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.18653/v1/2021.findings-acl.416" title="" class="ltx_ref ltx_href">Investigating memorization of conspiracy theories in text generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
ACL-IJCNLP 2021</em>, pages 4718–4729, Online. Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023a)</span>
<span class="ltx_bibblock">
Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen.
2023a.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.11747" title="" class="ltx_ref ltx_href">Halueval: A
large-scale hallucination evaluation benchmark for large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.11747.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023b)</span>
<span class="ltx_bibblock">
Miaoran Li, Baolin Peng, and Zhu Zhang. 2023b.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.14623" title="" class="ltx_ref ltx_href">Self-checker:
Plug-and-play modules for fact-checking with large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.14623.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023c)</span>
<span class="ltx_bibblock">
Ruosen Li, Teerth Patel, and Xinya Du. 2023c.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2307.02762" title="" class="ltx_ref ltx_href">PRD: peer rank
and discussion improve large language model based evaluations</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2307.02762.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2019)</span>
<span class="ltx_bibblock">
Siyao Li, Deren Lei, Pengda Qin, and William Yang Wang. 2019.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.18653/v1/D19-1623" title="" class="ltx_ref ltx_href">Deep reinforcement
learning with distributional semantic rewards for abstractive summarization</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pages 6037–6043.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022)</span>
<span class="ltx_bibblock">
Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori B.
Hashimoto. 2022.

</span>
<span class="ltx_bibblock"><a href="http://papers.nips.cc/paper_files/paper/2022/hash/1be5bc25d50895ee656b8c2d9eb89d6a-Abstract-Conference.html" title="" class="ltx_ref ltx_href">Diffusion-lm improves controllable text generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Annual Conference on Neural Information
Processing Systems (NeurIPS)</em>.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023d)</span>
<span class="ltx_bibblock">
Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and
Weizhu Chen. 2023d.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/2023.acl-long.291" title="" class="ltx_ref ltx_href">Making language
models better reasoners with step-aware verifier</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (ACL)</em>, pages 5315–5333.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lightman et al. (2023)</span>
<span class="ltx_bibblock">
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harrison Edwards, Bowen Baker,
Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.20050" title="" class="ltx_ref ltx_href">Let’s verify step
by step</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.20050.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2022)</span>
<span class="ltx_bibblock">
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/2022.acl-long.229" title="" class="ltx_ref ltx_href">TruthfulQA:
Measuring how models mimic human falsehoods</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (ACL)</em>, pages 3214–3252.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin and Chen (2023)</span>
<span class="ltx_bibblock">
Yen-Ting Lin and Yun-Nung Chen. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.13711" title="" class="ltx_ref ltx_href">Llm-eval: Unified
multi-dimensional automatic evaluation for open-domain conversations with
large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.13711.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2023)</span>
<span class="ltx_bibblock">
Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.19187" title="" class="ltx_ref ltx_href">Generating with
confidence: Uncertainty quantification for black-box large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.19187.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2302.02676" title="" class="ltx_ref ltx_href">Chain of hindsight
aligns language models with feedback</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2302.02676.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and Liu (2021)</span>
<span class="ltx_bibblock">
Yixin Liu and Pengfei Liu. 2021.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.18653/v1/2021.acl-short.135" title="" class="ltx_ref ltx_href">Simcls: A
simple framework for contrastive learning of abstractive summarization</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (ACL/IJCNLP)</em>, pages 1065–1072.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2022)</span>
<span class="ltx_bibblock">
Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West,
Prithviraj Ammanabrolu, and Yejin Choi. 2022.

</span>
<span class="ltx_bibblock"><a href="http://papers.nips.cc/paper_files/paper/2022/hash/b125999bde7e80910cbdbd323087df8f-Abstract-Conference.html" title="" class="ltx_ref ltx_href">QUARK: controllable text generation with reinforced unlearning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Annual Conference on Neural Information
Processing Systems (NeurIPS)</em>.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al. (2023a)</span>
<span class="ltx_bibblock">
Chenyang Lyu, Jitao Xu, and Longyue Wang. 2023a.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.01181" title="" class="ltx_ref ltx_href">New trends in
machine translation using large language models: Case examples with chatgpt</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.01181.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al. (2023b)</span>
<span class="ltx_bibblock">
Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna
Apidianaki, and Chris Callison-Burch. 2023b.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2301.13379" title="" class="ltx_ref ltx_href">Faithful
chain-of-thought reasoning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2301.13379.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madaan et al. (2023)</span>
<span class="ltx_bibblock">
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah
Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean
Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and
Peter Clark. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2303.17651" title="" class="ltx_ref ltx_href">Self-refine:
Iterative refinement with self-feedback</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2303.17651.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manakul et al. (2023)</span>
<span class="ltx_bibblock">
Potsawee Manakul, Adian Liusie, and Mark J. F. Gales. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2303.08896" title="" class="ltx_ref ltx_href">Selfcheckgpt:
Zero-resource black-box hallucination detection for generative large language
models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2303.08896.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Min et al. (2023)</span>
<span class="ltx_bibblock">
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh,
Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.14251" title="" class="ltx_ref ltx_href">Factscore:
Fine-grained atomic evaluation of factual precision in long form text
generation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.14251.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Murty et al. (2022)</span>
<span class="ltx_bibblock">
Shikhar Murty, Christopher D. Manning, Scott M. Lundberg, and Marco Túlio
Ribeiro. 2022.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/2022.emnlp-main.797" title="" class="ltx_ref ltx_href">Fixing model
bugs with natural language patches</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 11600–11613.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ni et al. (2023)</span>
<span class="ltx_bibblock">
Ansong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov, Wen-tau Yih, Sida I.
Wang, and Xi Victoria Lin. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2302.08468" title="" class="ltx_ref ltx_href">LEVER: learning
to verify language-to-code generation with execution</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th International Conference on Machine
Learning (ICML)</em>.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Olausson et al. (2023)</span>
<span class="ltx_bibblock">
Theo X. Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and
Armando Solar-Lezama. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2306.09896" title="" class="ltx_ref ltx_href">Demystifying GPT
self-repair for code generation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2306.09896.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Onoe et al. (2023)</span>
<span class="ltx_bibblock">
Yasumasa Onoe, Michael J. Q. Zhang, Shankar Padmanabhan, Greg Durrett, and
Eunsol Choi. 2023.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/2023.acl-long.300" title="" class="ltx_ref ltx_href">Can lms learn new
entities from descriptions? challenges in propagating injected knowledge</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (ACL)</em>, pages 5469–5485.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2303.08774" title="" class="ltx_ref ltx_href">GPT-4 technical
report</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2303.08774.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela
Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022.

</span>
<span class="ltx_bibblock"><a href="http://papers.nips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html" title="" class="ltx_ref ltx_href">Training language models to follow instructions with human feedback</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Annual Conference on Neural Information
Processing Systems (NeurIPS)</em>.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al. (2023)</span>
<span class="ltx_bibblock">
Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.12295" title="" class="ltx_ref ltx_href">Logic-lm:
Empowering large language models with symbolic solvers for faithful logical
reasoning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.12295.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pang et al. (2023)</span>
<span class="ltx_bibblock">
Jing-Cheng Pang, Pengyuan Wang, Kaiyuan Li, Xiong-Hui Chen, Jiacheng Xu,
Zongzhang Zhang, and Yang Yu. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.14483" title="" class="ltx_ref ltx_href">Language model
self-improvement by reinforcement learning contemplation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.14483.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paul et al. (2023)</span>
<span class="ltx_bibblock">
Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine
Bosselut, Robert West, and Boi Faltings. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2304.01904" title="" class="ltx_ref ltx_href">REFINER:
reasoning feedback on intermediate representations</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2304.01904.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. (2023)</span>
<span class="ltx_bibblock">
Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan
Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2302.12813" title="" class="ltx_ref ltx_href">Check your facts
and try again: Improving large language models with external knowledge and
automated feedback</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2302.12813.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pu and Demberg (2023)</span>
<span class="ltx_bibblock">
Dongqi Pu and Vera Demberg. 2023.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/2023.acl-srw.1" title="" class="ltx_ref ltx_href">Chatgpt vs
human-authored text: Insights into controllable text summarization and
sentence style transfer</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics: Student Research Workshop (ACL)</em>, pages 1–18.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al. (2023)</span>
<span class="ltx_bibblock">
Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and
Diyi Yang. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2302.06476" title="" class="ltx_ref ltx_href">Is chatgpt a
general-purpose natural language processing task solver?</a>

</span>
<span class="ltx_bibblock"><em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2302.06476.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raunak et al. (2023)</span>
<span class="ltx_bibblock">
Vikas Raunak, Amr Sharaf, Hany Hassan Awadallah, and Arul Menezes. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.14878" title="" class="ltx_ref ltx_href">Leveraging GPT-4
for automatic translation post-editing</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib94.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.14878.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ribeiro et al. (2023)</span>
<span class="ltx_bibblock">
Danilo Neves Ribeiro, Shen Wang, Xiaofei Ma, Henry Zhu, Rui Dong, Deguang Kong,
Juliette Burger, Anjelica Ramos, William Yang Wang, Zhiheng Huang, George
Karypis, Bing Xiang, and Dan Roth. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2302.06729" title="" class="ltx_ref ltx_href">STREET: A
multi-task structured reasoning and explanation benchmark</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 11th International Conference on Learning
Representations (ICLR)</em>.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saunders et al. (2022)</span>
<span class="ltx_bibblock">
William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan
Ward, and Jan Leike. 2022.

</span>
<span class="ltx_bibblock"><a href="https://arxiv.org/abs/2206.05802" title="" class="ltx_ref ltx_href">Self-critiquing models for
assisting human evaluators</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2206.05802.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saxon et al. (2023)</span>
<span class="ltx_bibblock">
Michael Saxon, Xinyi Wang, Wenda Xu, and William Yang Wang. 2023.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/2023.eacl-main.223" title="" class="ltx_ref ltx_href">PECO:
Examining single sentence label leakage in natural language inference
datasets through progressive evaluation of cluster outliers</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib97.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 17th Conference of the European Chapter
of the Association for Computational Linguistics (EACL)</em>, pages 3053–3066.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scheurer et al. (2023)</span>
<span class="ltx_bibblock">
Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan,
Angelica Chen, Kyunghyun Cho, and Ethan Perez. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2303.16755" title="" class="ltx_ref ltx_href">Training language
models with language feedback at scale</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib98.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2303.16755.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et al. (2023)</span>
<span class="ltx_bibblock">
Timo Schick, Jane A. Yu, Zhengbao Jiang, Fabio Petroni, Patrick S. H. Lewis,
Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and
Sebastian Riedel. 2023.

</span>
<span class="ltx_bibblock"><a href="https://arxiv.org/abs/2208.11663" title="" class="ltx_ref ltx_href">PEER: A collaborative
language model</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib99.1.1" class="ltx_emph ltx_font_italic">Proceedings of The 11th International Conference on Learning
Representations (ICLR)</em>.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schulman et al. (2017)</span>
<span class="ltx_bibblock">
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
2017.

</span>
<span class="ltx_bibblock"><a href="http://arxiv.org/abs/1707.06347" title="" class="ltx_ref ltx_href">Proximal policy optimization
algorithms</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib100.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1707.06347.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sellam et al. (2020)</span>
<span class="ltx_bibblock">
Thibault Sellam, Dipanjan Das, and Ankur P. Parikh. 2020.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.18653/v1/2020.acl-main.704" title="" class="ltx_ref ltx_href">BLEURT:
learning robust metrics for text generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib101.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics (ACL)</em>, pages 7881–7892.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shaikh et al. (2023)</span>
<span class="ltx_bibblock">
Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, and Diyi Yang.
2023.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/2023.acl-long.244" title="" class="ltx_ref ltx_href">On second
thought, let’s not think step by step! bias and toxicity in zero-shot
reasoning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (ACL)</em>, pages 4454–4470.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2016)</span>
<span class="ltx_bibblock">
Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu.
2016.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/P16-1159" title="" class="ltx_ref ltx_href">Minimum risk training for
neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib103.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (ACL)</em>, pages 1683–1692.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shinn et al. (2023)</span>
<span class="ltx_bibblock">
Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan,
and Shunyu Yao. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2303.11366" title="" class="ltx_ref ltx_href">Reflexion:
Language agents with verbal reinforcement learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib104.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2303.11366.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sinitsin et al. (2020)</span>
<span class="ltx_bibblock">
Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitry V. Pyrkin, Sergei Popov, and Artem
Babenko. 2020.

</span>
<span class="ltx_bibblock"><a href="https://openreview.net/forum?id=HJedXaEtvS" title="" class="ltx_ref ltx_href">Editable neural
networks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib105.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 8th International Conference on Learning
Representations (ICLR)</em>.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sordoni et al. (2023)</span>
<span class="ltx_bibblock">
Alessandro Sordoni, Xingdi Yuan, Marc-Alexandre Côté, Matheus
Pereira, Adam Trischler, Ziang Xiao, Arian Hosseini, Friederike Niedtner, and
Nicolas Le Roux. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2306.12509" title="" class="ltx_ref ltx_href">Deep language
networks: Joint prompt training of stacked llms using variational inference</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib106.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2306.12509.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srikanth and Rudinger (2022)</span>
<span class="ltx_bibblock">
Neha Srikanth and Rachel Rudinger. 2022.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/2022.naacl-main.350" title="" class="ltx_ref ltx_href">Partial-input
baselines show that nli models can ignore context, but they don’t.</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib107.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies (NAACL-HLT)</em>, pages 4753–4763.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suzgun et al. (2023)</span>
<span class="ltx_bibblock">
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann,
Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou,
and Jason Wei. 2023.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/2023.findings-acl.824" title="" class="ltx_ref ltx_href">Challenging
big-bench tasks and whether chain-of-thought can solve them</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib108.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
ACL 2023</em>, pages 13003–13051.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tafjord et al. (2022)</span>
<span class="ltx_bibblock">
Oyvind Tafjord, Bhavana Dalvi Mishra, and Peter Clark. 2022.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/2022.emnlp-main.134" title="" class="ltx_ref ltx_href">Entailer:
Answering questions with faithful and truthful chains of reasoning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib109.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 2078–2093.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tanno et al. (2022)</span>
<span class="ltx_bibblock">
Ryutaro Tanno, Melanie F. Pradier, Aditya V. Nori, and Yingzhen Li. 2022.

</span>
<span class="ltx_bibblock"><a href="http://papers.nips.cc/paper_files/paper/2022/hash/552260cfb5e292e511eaa780806ac984-Abstract-Conference.html" title="" class="ltx_ref ltx_href">Repairing neural networks by leaving the right past behind</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib110.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Annual Conference on Neural
Information Processing Systems (NeurIPS)</em>.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Uesato et al. (2022)</span>
<span class="ltx_bibblock">
Jonathan Uesato, Nate Kushman, Ramana Kumar, H. Francis Song, Noah Y. Siegel,
Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2211.14275" title="" class="ltx_ref ltx_href">Solving math word
problems with process- and outcome-based feedback</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib111.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2211.14275.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Unanue et al. (2021)</span>
<span class="ltx_bibblock">
Inigo Jauregi Unanue, Jacob Parnell, and Massimo Piccardi. 2021.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.18653/v1/2021.acl-short.115" title="" class="ltx_ref ltx_href">Berttune:
Fine-tuning neural machine translation with bertscore</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib112.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (ACL/IJCNLP)</em>, pages 915–924.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wan and Bansal (2022)</span>
<span class="ltx_bibblock">
David Wan and Mohit Bansal. 2022.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.18653/v1/2022.naacl-main.74" title="" class="ltx_ref ltx_href">Factpegasus:
Factuality-aware pre-training and fine-tuning for abstractive summarization</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib113.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies (NAACL-HLT)</em>, pages 1010–1028.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023a)</span>
<span class="ltx_bibblock">
Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang,
Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran
Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn
Song, and Bo Li. 2023a.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2306.11698" title="" class="ltx_ref ltx_href">Decodingtrust: A
comprehensive assessment of trustworthiness in GPT models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib114.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2306.11698.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023b)</span>
<span class="ltx_bibblock">
Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. 2023b.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2302.00487" title="" class="ltx_ref ltx_href">A comprehensive
survey of continual learning: Theory, method and application</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib115.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2302.00487.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023c)</span>
<span class="ltx_bibblock">
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang,
Aakanksha Chowdhery, and Denny Zhou. 2023c.

</span>
<span class="ltx_bibblock"><a href="https://openreview.net/pdf?id=1PL1NIMMrw" title="" class="ltx_ref ltx_href">Self-consistency
improves chain of thought reasoning in language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib116.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 11th International Conference on Learning
Representations (ICLR)</em>.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023d)</span>
<span class="ltx_bibblock">
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel
Khashabi, and Hannaneh Hajishirzi. 2023d.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/2023.acl-long.754" title="" class="ltx_ref ltx_href">Self-instruct:
Aligning language models with self-generated instructions</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib117.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (ACL)</em>, pages 13484–13508.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023e)</span>
<span class="ltx_bibblock">
Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang,
Lifeng Shang, Xin Jiang, and Qun Liu. 2023e.

</span>
<span class="ltx_bibblock"><a href="https://arxiv.org/abs/2307.12966" title="" class="ltx_ref ltx_href">Aligning large language
models with human: A survey</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib118.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2307.12966.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022a)</span>
<span class="ltx_bibblock">
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H.
Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William
Fedus. 2022a.

</span>
<span class="ltx_bibblock">Emergent abilities of large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib119.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2206.07682.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022b)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia,
Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022b.

</span>
<span class="ltx_bibblock"><a href="http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html" title="" class="ltx_ref ltx_href">Chain-of-thought prompting elicits reasoning in large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib120.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Annual Conference on Neural Information
Processing Systems (NeurIPS)</em>.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Welleck et al. (2023)</span>
<span class="ltx_bibblock">
Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel
Khashabi, and Yejin Choi. 2023.

</span>
<span class="ltx_bibblock"><a href="https://openreview.net/pdf?id=hH36JeQZDaO" title="" class="ltx_ref ltx_href">Generating
sequences by learning to self-correct</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib121.1.1" class="ltx_emph ltx_font_italic">Proceedings of The 11th International Conference on Learning
Representations (ICLR)</em>.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weng et al. (2023)</span>
<span class="ltx_bibblock">
Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, and Jun Zhao.
2023.

</span>
<span class="ltx_bibblock"><a href="https://arxiv.org/abs/2212.09561" title="" class="ltx_ref ltx_href">Large language models are
better reasoners with self-verification</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib122.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2212.09561.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2021)</span>
<span class="ltx_bibblock">
Qingyang Wu, Lei Li, and Zhou Yu. 2021.

</span>
<span class="ltx_bibblock"><a href="https://ojs.aaai.org/index.php/AAAI/article/view/17656" title="" class="ltx_ref ltx_href">Textgail: Generative adversarial imitation learning for text generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib123.1.1" class="ltx_emph ltx_font_italic">Proceedings of 35th Conference on Artificial Intelligence
(AAAI)</em>, pages 14067–14075.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2023a)</span>
<span class="ltx_bibblock">
Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj
Ammanabrolu, Noah A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi.
2023a.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2306.01693" title="" class="ltx_ref ltx_href">Fine-grained human
feedback gives better rewards for language model training</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib124.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2306.01693.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2023b)</span>
<span class="ltx_bibblock">
Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin
Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. 2023b.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2307.02477" title="" class="ltx_ref ltx_href">Reasoning or
reciting? exploring the capabilities and limitations of language models
through counterfactual tasks</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib125.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2307.02477.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2023)</span>
<span class="ltx_bibblock">
Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and
Qizhe Xie. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.00633" title="" class="ltx_ref ltx_href">Decomposition
enhances reasoning via self-evaluation guided decoding</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib126.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.00633.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2023a)</span>
<span class="ltx_bibblock">
Wenda Xu, Xian Qian, Mingxuan Wang, Lei Li, and William Yang Wang.
2023a.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/2023.acl-long.283" title="" class="ltx_ref ltx_href">SESCORE2:
learning text generation evaluation via synthesizing realistic mistakes</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib127.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (ACL)</em>, pages 5166–5183.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2022)</span>
<span class="ltx_bibblock">
Wenda Xu, Yi-Lin Tuan, Yujie Lu, Michael Saxon, Lei Li, and William Yang
Wang. 2022.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/2022.findings-emnlp.489" title="" class="ltx_ref ltx_href">Not all
errors are equal: Learning text generation metrics using stratified error
synthesis</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib128.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics
(EMNLP)</em>, pages 6559–6574.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2023b)</span>
<span class="ltx_bibblock">
Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag,
William Yang Wang, and Lei Li. 2023b.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.14282" title="" class="ltx_ref ltx_href">INSTRUCTSCORE:
towards explainable text generation evaluation with automatic feedback</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib129.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.14282.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. (2023a)</span>
<span class="ltx_bibblock">
Hao Yan, Saurabh Srivastava, Yintao Tai, Sida I. Wang, Wen-tau Yih, and Ziyu
Yao. 2023a.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/2023.acl-long.177" title="" class="ltx_ref ltx_href">Learning to
simulate natural language feedback for interactive semantic parsing</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib130.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61th Annual Meeting of the Association
for Computational Linguistics (ACL)</em>, pages 3149–3170.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. (2023b)</span>
<span class="ltx_bibblock">
Yiming Yan, Tao Wang, Chengqi Zhao, Shujian Huang, Jiajun Chen, and Mingxuan
Wang. 2023b.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/2023.acl-long.297" title="" class="ltx_ref ltx_href">BLEURT has
universal translations: An analysis of automatic metrics by minimum risk
training</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib131.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics(ACL)</em>, pages 5428–5443.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2022a)</span>
<span class="ltx_bibblock">
Kaiyu Yang, Jia Deng, and Danqi Chen. 2022a.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/2022.emnlp-main.7" title="" class="ltx_ref ltx_href">Generating
natural language proofs with verifier-guided search</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib132.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 89–105.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang and Klein (2021)</span>
<span class="ltx_bibblock">
Kevin Yang and Dan Klein. 2021.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/2021.naacl-main.276" title="" class="ltx_ref ltx_href">FUDGE:
Controlled text generation with future discriminators</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib133.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies (NAACL-HLT)</em>, pages 3511–3535.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2022b)</span>
<span class="ltx_bibblock">
Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. 2022b.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/2022.emnlp-main.296" title="" class="ltx_ref ltx_href">Re3: Generating
longer stories with recursive reprompting and revision</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib134.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 4393–4479.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018)</span>
<span class="ltx_bibblock">
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan
Salakhutdinov, and Christopher D. Manning. 2018.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.18653/v1/d18-1259" title="" class="ltx_ref ltx_href">Hotpotqa: A dataset
for diverse, explainable multi-hop question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib135.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 2369–2380.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. (2023a)</span>
<span class="ltx_bibblock">
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan
Cao, and Karthik Narasimhan. 2023a.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.10601" title="" class="ltx_ref ltx_href">Tree of thoughts:
Deliberate problem solving with large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib136.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.10601.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. (2023b)</span>
<span class="ltx_bibblock">
Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng,
Huajun Chen, and Ningyu Zhang. 2023b.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.13172" title="" class="ltx_ref ltx_href">Editing large
language models: Problems, methods, and opportunities</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib137.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.13172.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. (2023)</span>
<span class="ltx_bibblock">
Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, and
Minjoon Seo. 2023.

</span>
<span class="ltx_bibblock"><a href="https://kaistai.github.io/SelFee/" title="" class="ltx_ref ltx_href">Selfee: Iterative
self-revising llm empowered by self-feedback generation</a>.

</span>
<span class="ltx_bibblock">Blog post.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023)</span>
<span class="ltx_bibblock">
Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, and Ashish Sabharwal. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.14002" title="" class="ltx_ref ltx_href">Improving language
models via plug-and-play retrieval feedback</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib139.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.14002.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zelikman et al. (2022)</span>
<span class="ltx_bibblock">
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. 2022.

</span>
<span class="ltx_bibblock"><a href="http://papers.nips.cc/paper_files/paper/2022/hash/639a9a172c044fbb64175b5fad42e9a5-Abstract-Conference.html" title="" class="ltx_ref ltx_href">Star: Bootstrapping reasoning with reasoning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib140.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Annual Conference on Neural Information
Processing Systems (NeurIPS)</em>.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023a)</span>
<span class="ltx_bibblock">
Kechi Zhang, Zhuo Li, Jia Li, Ge Li, and Zhi Jin. 2023a.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.04087" title="" class="ltx_ref ltx_href">Self-edit:
Fault-aware code editor for code generation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib141.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.04087.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023b)</span>
<span class="ltx_bibblock">
Kexun Zhang, Danqing Wang, Jingtao Xia, William Yang Wang, and Lei Li.
2023b.

</span>
<span class="ltx_bibblock"><a href="https://arxiv.org/abs/2305.14591" title="" class="ltx_ref ltx_href">Algo: Synthesizing
algorithmic programs with generated oracle verifiers</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib142.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.14591.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023c)</span>
<span class="ltx_bibblock">
Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A. Smith.
2023c.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2305.13534" title="" class="ltx_ref ltx_href">How language model
hallucinations can snowball</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib143.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.13534.

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020)</span>
<span class="ltx_bibblock">
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi.
2020.

</span>
<span class="ltx_bibblock"><a href="https://openreview.net/forum?id=SkeHuCVFDr" title="" class="ltx_ref ltx_href">Bertscore:
Evaluating text generation with BERT</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib144.1.1" class="ltx_emph ltx_font_italic">Proceedings of 8th International Conference on Learning
Representations (ICLR)</em>.

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2023)</span>
<span class="ltx_bibblock">
Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan,
Jiaxing Zhang, and Yujiu Yang. 2023.

</span>
<span class="ltx_bibblock"><a href="https://aclanthology.org/2023.acl-long.245" title="" class="ltx_ref ltx_href">Solving math word
problems via cooperative reasoning induced language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib145.1.1" class="ltx_emph ltx_font_italic">Processings of the 61th Annual Meeting of the Association
for Computational Linguistics (ACL)</em>, pages 4471–4485.

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhuo et al. (2023)</span>
<span class="ltx_bibblock">
Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. 2023.

</span>
<span class="ltx_bibblock"><a href="https://doi.org/10.48550/arXiv.2301.12867" title="" class="ltx_ref ltx_href">Red teaming
chatgpt via jailbreaking: Bias, robustness, reliability and toxicity</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib146.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2301.12867.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Aug 10 19:11:16 2023 by <a href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"></a>
</div></footer>
</div>
</body>
</html>
