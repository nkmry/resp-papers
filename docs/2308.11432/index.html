<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>A Survey on Large Language Model based Autonomous Agents</title>
<!--Generated on Mon Aug 28 04:10:36 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/dginev/ar5iv-css@0.7.6/css/ar5iv.min.css" type="text/css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Survey on Large Language Model based Autonomous Agents</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lei Wang,Â Â Chen Ma<span id="footnotex1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>These authors contribute equally to this paper.</span></span></span>,Â Â Xueyang Feng<span id="footnotex2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>These authors contribute equally to this paper.</span></span></span>,Â Â Zeyu Zhang,Â Â Hao Yang,Â Â Jingsen Zhang,
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_bold">Zhiyuan Chen</span>,Â Â <span id="id2.2.id2" class="ltx_text ltx_font_bold">Jiakai Tang</span>,Â Â <span id="id3.3.id3" class="ltx_text ltx_font_bold">Xu Chen</span>,Â Â <span id="id4.4.id4" class="ltx_text ltx_font_bold">Yankai Lin</span>,Â Â <span id="id5.5.id5" class="ltx_text ltx_font_bold">Wayne Xin Zhao</span>,Â Â <span id="id6.6.id6" class="ltx_text ltx_font_bold">Zhewei Wei</span>,
<br class="ltx_break">Â <span id="id7.7.id7" class="ltx_text ltx_font_bold">Ji-Rong Wen
<br class="ltx_break"></span>Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id8.id1" class="ltx_p">Autonomous agents have long been a prominent research topic in the academic community.
Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from the human learning processes, and thus makes the agents hard to achieve human-like decisions.
Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence.
This has sparked an upsurge in studies investigating autonomous agents based on LLMs. To harness the full potential of LLMs, researchers have devised diverse agent architectures tailored to different applications.
In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of autonomous agents from a holistic perspective.
More specifically, our focus lies in the construction of LLM-based agents, for which we propose a unified framework that encompasses a majority of the previous work. Additionally, we provide a summary of the various applications of LLM-based AI agents in the domains of social science, natural science, and engineering. Lastly, we discuss the commonly employed evaluation strategies for LLM-based AI agents.
Based on the previous studies, we also present several challenges and future directions in this field.
To keep track of this field and continuously update our survey, we maintain a repository for the related references at https://github.com/Paitesanshi/LLM-Agent-Survey.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<blockquote id="S1.p1.1" class="ltx_quote ltx_epigraph " style="width:325.21pt; margin-left:auto;;">
<div id="S1.p1.1.1" class="ltx_block ltx_epigraph_text" style="text-align:left; ;">
<p id="S1.p1.1.1.1" class="ltx_p"><span id="S1.p1.1.1.1.1" class="ltx_text ltx_font_italic">â€œAn autonomous agent is a system situated within and a part of an environment that senses that environment and acts on it, over time, in pursuit of its own agenda and so as to effect what it senses in the future.â€</span></p>
</div>
<div id="S1.p1.1.2" class="ltx_block ltx_epigraph_source" style="border-top:solid 0.4pt; text-align:right; ;">
<p id="S1.p1.1.2.1" class="ltx_p">Franklin and Graesser (1997)</p>
</div>
</blockquote>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Autonomous agents have long been seen as a promising path toward artificial general intelligence (AGI), capable of accomplishing tasks through self-directed planning and instructions.
In earlier paradigms, the policy functions that dictated the agentâ€™s actions were conceived through heuristic methodologies and subsequently refined through environmental engagementsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>, <a href="#bib.bib86" title="" class="ltx_ref">86</a>, <a href="#bib.bib120" title="" class="ltx_ref">120</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite>. A discernible gap has emerged wherein these functions often fall short of replicating human-level proficiency, particularly in unconstrained, open-domain settings. Such discrepancies can be traced back to potential inaccuracies inherent in the heuristic designs and the circumscribed knowledge furnished by the training environments.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">In recent years, large language models (LLMs) have achieved remarkable success, indicating their potential for achieving human-like intelligenceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>, <a href="#bib.bib116" title="" class="ltx_ref">116</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib130" title="" class="ltx_ref">130</a>, <a href="#bib.bib131" title="" class="ltx_ref">131</a>]</cite>.
This capability emerges from the utilization of comprehensive training datasets coupled with a substantial array of model parameters.
Motivated by this capability, a burgeoning trend has emerged in recent years (see FigureÂ <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ A Survey on Large Language Model based Autonomous Agents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> for the growth trend of this field), wherein LLMs are harnessed as core orchestrators in the creation of autonomous agentsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib125" title="" class="ltx_ref">125</a>, <a href="#bib.bib123" title="" class="ltx_ref">123</a>, <a href="#bib.bib115" title="" class="ltx_ref">115</a>, <a href="#bib.bib119" title="" class="ltx_ref">119</a>, <a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite>. This strategic employment aims to emulate human-like decision-making processes, thereby providing a pathway towards more sophisticated and adaptive artificial intelligence systems. Along the direction of LLM-based autonomous agents, people have designed many promising models, focusing on enhancing LLMs with essential capabilities, such as memory and planning, enabling them to stimulate human actions and proficiently undertake a range of tasks. However, these models are proposed independently, and there have been limited efforts in summarizing and comparing them holistically. It is crucial to construct a holistic summarization analysis for existing LLM-based autonomous agents works, which holds great significance in developing a comprehensive understanding of this field and serving as inspiration for future research.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<p id="S1.F1.1" class="ltx_p ltx_align_center"><span id="S1.F1.1.1" class="ltx_text ltx_framed_rectangle" style="border-color: black;">
<img src="x1.png" id="S1.F1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="789" height="400" alt="Refer to caption">
</span>



</p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
Illustration of the growth trend on the field of LLM-based autonomous agents. </figcaption>
</figure>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">In this paper, we conduct a comprehensive survey of the field of LLM-based autonomous agents.
Specifically, we organize our survey based on three aspects including the construction, application, and evaluation of LLM-based autonomous agents.
For the agent construction, we present a unified framework composed of four components, that is, a profile module to represent agent attributes, a memory module to store historical information, a planning module to strategize future actions, and an action module to execute the planned decisions.
By disabling one or more modules, the majority of previous studies can be viewed as specific examples of this framework.
After introducing the typical agent modules, we also provide a summary of the commonly-used fine-tuning strategies to enhance the adaptability of the agent for different application scenarios.
In addition to constructing the agent, we provide an overview of the potential applications of autonomous agents, exploring how these agents can enhance the fields of social science, natural science, and engineering.
Finally, we discuss the methods for evaluating autonomous agents, focusing on both subjective and objective strategies.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">In summary, this survey provides a systematic review and establishes clear taxonomies for existing studies in the field of LLM-based autonomous agents.
It focuses on three aspects including the agent construction, application, and evaluation.
Based on the previous studies, we identify several challenges in this field and discuss potential future directions.
We believe this field is still at its early stages, and therefore, we maintain a repository to continually keep track of the studies in this field at https://github.com/Paitesanshi/LLM-Agent-Survey.
</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>LLM-based Autonomous Agent Construction</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">LLM-based autonomous agents are expected to effectively accomplish different tasks based on the human-like capabilities of LLMs.
In order to achieve this goal, there are two significant aspects, that is, (1) which architecture should be designed to better use LLMs and (2) how to learn the parameters of the architecture.
Within the context of architectural design, we contribute a systematic synthesis of existing research, culminating in a comprehensive unified framework.
As for the second aspect, we summarize three commonly employed strategies including (1) learning from examples, where the model is fine-tuned based on curated datasets, (2) learning from environment feedback, leveraging real-time interactions and observations, and (3) learning from human feedback, capitalizing on human expertise and intervention for refinement.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Agent Architecture Design</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">Recent advancements in Language Models (LLMs) have demonstrated their potential to accomplish a wide range of tasks.
However, only based on LLMs, it is hard to effectively realize an autonomous agent due to their architecture limitations.
To bridge this gap, previous work has developed a number of modules to inspire and enhance the capabilities of LLMs for building autonomous agents.
In this section, we propose a unified framework to summarize the architectures proposed in the previous work<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_tag ltx_tag_note">*</span>Our framework is also inspired by a pioneer work at https://lilianweng.github.io/posts/2023-06-23-agent/</span></span></span>.
In specific, the overall structure of our framework is illustrated FigureÂ <a href="#S2.F2" title="Figure 2 â€£ 2.1.1 Profiling Module â€£ 2.1 Agent Architecture Design â€£ 2 LLM-based Autonomous Agent Construction â€£ A Survey on Large Language Model based Autonomous Agents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, which is composed of a profiling module, a memory module, a planning module, and an action module.
The purpose of the profiling module is to identify the role of the agent.
The memory and planning modules place the agent into a dynamic environment, enabling it to recall past behaviors and plan future actions.
The action module is responsible for translating the agentâ€™s decisions into specific outputs.
Within these modules, the profiling module impacts the memory and planning modules, and collectively, these three modules influence the action module.
In the following, we detail these modules.</p>
</div>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Profiling Module</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">autonomous agents typically perform tasks by assuming specific roles, such as coders, teachers and domain expertsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.
The profiling module aims to indicate the role profiles of the agents, which are usually written into the prompt to influence the LLM behaviors.
In existing work, there are three commonly used strategies for generating agent profiles.</p>
</div>
<div id="S2.SS1.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS1.p2.1" class="ltx_p"><span id="S2.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Handcrafting Method</span>:
in this method, the profiles of agents are manually specified. For instance, if one would like to design agents with different personalities, he can use "you are an outgoing person" or "you are an introverted person" to profile the agent.
The handcrafting method has been leveraged in a lot of previous work to indicate the agent profiles.
In specific, Generative AgentÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib156" title="" class="ltx_ref">156</a>]</cite> describes the agent by the information like name, objectives, and relationships with other agents.
MetaGPTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>, ChatDevÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite>, and Self-collaborationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> predefine various roles and their corresponding responsibilities in software development, manually assigning distinct profiles to each agent to facilitate collaboration.
A recent workÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> demonstrates that manually assigning different personas significantly impacts LLMâ€™s generation, including toxicity. By assigning specific personas, they show increased toxicity versus default personas.
In general, the handcrafting method is very flexible. However, it can be labor-intensive, particularly when dealing with a large number of agents.</p>
</div>
<div id="S2.SS1.SSS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS1.p3.1" class="ltx_p"><span id="S2.SS1.SSS1.p3.1.1" class="ltx_text ltx_font_bold">LLM-generation Method</span>:
in this method, the agent profiles are automatically generated based on LLMs.
Typically, it begins by providing manual prompts that outline specific generation rules and elucidates the composition and attributes of the agent profiles within the target population. In addition, it may specify initial seed agent profiles to serve as few-shot examples. These profiles then serve as the foundation for generating other agent information based on LLMs.
For example, RecAgentÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib134" title="" class="ltx_ref">134</a>]</cite> first creates seed profiles for a small number of agents by manually crafting details like age, gender, personal traits, and movie preferences. Then, it leverages ChatGPT to generate more agent profiles based on the seed information.
The LLM-generation method can save significant time when the number of agents is large, but it may lack precise control over the generated profiles.</p>
</div>
<div id="S2.SS1.SSS1.p4" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS1.p4.1" class="ltx_p"><span id="S2.SS1.SSS1.p4.1.1" class="ltx_text ltx_font_bold">Dataset Alignment Method</span>:
in this method, the agent profiles are indicated based on real-world datasets.
The basic information of real humans is fully or selectively leveraged to describe the agents.
For example, the agents inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> are initialized based on the participant demographic backgrounds in real-world survey datasets.
The dataset alignment method can accurately capture the attributes of the real population, effectively bridging the gap between the virtual and real worlds.</p>
</div>
<div id="S2.SS1.SSS1.p5" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS1.p5.1" class="ltx_p">In addition to the profile generation strategies, another important problem is how to specify the information for profiling the agents.
Examples of information include demographic information, which introduces the characteristics of a population (e.g., age, gender, and income), psychology information, which indicates the personalities of the agents, and social information, which describes the relationships between agents.
The choice of information to profile the agent is largely determined by the specific application scenarios.
For instance, if the study focuses on user social behaviors, social profile information becomes pivotal.
However, establishing the relationship between the profile information and downstream tasks is not always straightforward.
A potential solution would be to input all possible profile information initially and then develop automatic methods (<em id="S2.SS1.SSS1.p5.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, based on LLMs) to select the most suitable one.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<p id="S2.F2.1" class="ltx_p ltx_align_center"><span id="S2.F2.1.1" class="ltx_text ltx_framed_rectangle" style="border-color: black;">
<img src="x2.png" id="S2.F2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="789" height="350" alt="Refer to caption">
</span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A unified framework for the architecture design of LLM-based autonomous AI agent.</figcaption>
</figure>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Memory Module</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">The memory module plays a very important role in the construction of AI agents.
It stores information perceived from the environment and leverages the recorded memories to facilitate future actions.
The memory module can help the agent to accumulate experiences, self-evolve, and behave in a more consistent, reasonable, and effective manner.
This section provides a comprehensive overview of the memory module, focusing on its structures, formats, and operations.</p>
</div>
<div id="S2.SS1.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS2.p2.1" class="ltx_p"><span id="S2.SS1.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Memory Structures</span>: LLM-based autonomous agents usually incorporate principles and mechanisms derived from cognitive science research on human memory processes. Human memory follows a general progression from sensory memory that registers perceptual inputs, to short-term memory that maintains information transiently, to long-term memory that consolidates information over extended periods. When designing memory architectures for AI agents, researchers take inspiration from these aspects of human memory while also recognizing key differences in capabilities.
Short-term memory in AI agents is analogous to learning capacities supported within the context window constraints of the Transformer architecture. Long-term memory resembles the external vector storage that agents can rapidly query and retrieve from as needed. Thus, while humans gradually transfer perceived information from short-term to long-term stores via reinforcement, AI agents can engineer more optimized writing and reading processes between their algorithmically implemented memory systems. By emulating aspects of human memory, designers can create agents that leverage memory processes for improved reasoning and autonomy. In the following, we introduce two types of commonly used memory structures.</p>
</div>
<div id="S2.SS1.SSS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS2.p3.1" class="ltx_p"><math id="S2.SS1.SSS2.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS2.p3.1.m1.1a"><mo id="S2.SS1.SSS2.p3.1.m1.1.1" xref="S2.SS1.SSS2.p3.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p3.1.m1.1b"><ci id="S2.SS1.SSS2.p3.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p3.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p3.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p3.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS2.p3.1.1" class="ltx_text ltx_font_italic">Unified Memory</span>.
In this structure, the memories are organized into a single framework, and there are no distinctions between the short- and long-term memories. The framework has unified interfaces for memory reading, writing, and reflection.
For example,
AtlasÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> stores document memories based on universal dense vectors, which are generated from a dual-encoder model.
Augmented LLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite> employs a unified external storage for its memory, which can be accessed via prompts.
VoyagerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite> also utilizes a unified memory architecture, where skills of different complexities are gathered in a central library.
During code generation, skills can be indexed based on their relevance for matching and retrieval.
ChatLogÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref">132</a>]</cite> maintains a unified memory stream, which allows the model to retain important historical information and adaptively adjust the agents themselves for different environments.</p>
</div>
<div id="S2.SS1.SSS2.p4" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS2.p4.1" class="ltx_p"><math id="S2.SS1.SSS2.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS2.p4.1.m1.1a"><mo id="S2.SS1.SSS2.p4.1.m1.1.1" xref="S2.SS1.SSS2.p4.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p4.1.m1.1b"><ci id="S2.SS1.SSS2.p4.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p4.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p4.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p4.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS2.p4.1.1" class="ltx_text ltx_font_italic">Hybrid Memory</span>.
Hybrid memory clearly differentiates between short-term and long-term functions. The short-term component temporarily buffers recent perceptions, while long-term memory consolidates important information over time.
For instance, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite> employs a dual-layered memory structure to store an agentâ€™s experiences and knowledge, comprising a long-term memory and a short-term memory. Long-term memory is utilized to preserve the agentâ€™s understanding and summarization of the entire world, while short-term memory is employed to retain the agentâ€™s comprehension and annotations of individual events.
AgentSims <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> also implements a hybrid memory architecture. The long-term memory utilizes a vector database to efficiently store and retrieve the episodic memories of each agent. LLMs are employed to realize the short-term memory and perform abstraction, validation, correction, and simulation tasks.
In GITMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite>, the short-term memory stores the current trajectory, and the long-term memory saves reference plans summarized from successful prior trajectories.
Long-term memory provides stable knowledge, while short-term memory allows flexible planning.
ReflexionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite> utilizes a short-term sliding window to capture recent feedback and incorporates persistent long-term storage to retain condensed insights. This combination allows for the utilization of both detailed immediate experiences and high-level abstractions.
SCMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite> selectively activates the most relevant long-term knowledge to combine with short-term memory, enabling reasoning over complex contextual dialogues. SWIFTSAGEÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite> employs a small LM to manage short-term memory for generating intuition and associative thinking, while utilizing a LLM for handling long-term memory to generate well-considered decisions.</p>
</div>
<div id="S2.SS1.SSS2.p5" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS2.p5.1" class="ltx_p"><span id="S2.SS1.SSS2.p5.1.1" class="ltx_text ltx_font_bold">Memory Formats</span>:
Information can be stored in memory using various formats, each offering unique advantages.
For example, natural languages can retain comprehensive semantic information, while embeddings can enhance the efficiency of memory reading. In the following, we present four types of commonly used memory formats.</p>
</div>
<div id="S2.SS1.SSS2.p6" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS2.p6.1" class="ltx_p"><math id="S2.SS1.SSS2.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS2.p6.1.m1.1a"><mo id="S2.SS1.SSS2.p6.1.m1.1.1" xref="S2.SS1.SSS2.p6.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p6.1.m1.1b"><ci id="S2.SS1.SSS2.p6.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p6.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p6.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p6.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS2.p6.1.1" class="ltx_text ltx_font_italic">Natural Languages</span>. Using natural languages for task reasoning/programming enables flexible, semantic-rich storage/access. For instance, ReflexionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite> stores experiential feedback in natural language within a sliding window.
VoyagerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite> employs natural language descriptions to represent skills within the Minecraft game, which are directly stored in memory.</p>
</div>
<div id="S2.SS1.SSS2.p7" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS2.p7.1" class="ltx_p"><math id="S2.SS1.SSS2.p7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS2.p7.1.m1.1a"><mo id="S2.SS1.SSS2.p7.1.m1.1.1" xref="S2.SS1.SSS2.p7.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p7.1.m1.1b"><ci id="S2.SS1.SSS2.p7.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p7.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p7.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p7.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS2.p7.1.1" class="ltx_text ltx_font_italic">Embeddings</span>.
Using embeddings to store information can enhance memory retrieval and reading efficiency.
For example, MemoryBankÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib158" title="" class="ltx_ref">158</a>]</cite> encodes each memory segment into an embedding vector, building an indexed corpus for retrieval.
GITMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite> represents reference plans as embeddings to facilitate matching and reuse. ChatDevÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite> encodes dialogue history into vectors for retrieval.</p>
</div>
<div id="S2.SS1.SSS2.p8" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS2.p8.1" class="ltx_p"><math id="S2.SS1.SSS2.p8.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS2.p8.1.m1.1a"><mo id="S2.SS1.SSS2.p8.1.m1.1.1" xref="S2.SS1.SSS2.p8.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p8.1.m1.1b"><ci id="S2.SS1.SSS2.p8.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p8.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p8.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p8.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS2.p8.1.1" class="ltx_text ltx_font_italic">Databases</span>. External databases provide structured storage, and one can manipulate the memories with efficient and comprehensive operations.
For example, ChatDBÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> utilizes a database as symbolic long-term memory. SQL statements generated by the LLM controller can accurately operate on the database.</p>
</div>
<div id="S2.SS1.SSS2.p9" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS2.p9.1" class="ltx_p"><math id="S2.SS1.SSS2.p9.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS2.p9.1.m1.1a"><mo id="S2.SS1.SSS2.p9.1.m1.1.1" xref="S2.SS1.SSS2.p9.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p9.1.m1.1b"><ci id="S2.SS1.SSS2.p9.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p9.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p9.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p9.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS2.p9.1.1" class="ltx_text ltx_font_italic">Structured Lists</span>.
Another type of memory format is the structured list, based on which the information can be delivered in a more concise and efficient manner.
For example, GITMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite> stores action lists for sub-goals in a hierarchical tree structure.
The hierarchical structure explicitly captures the relationships between goals and corresponding plans.
RET-LLMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite> initially converts natural language sentences into triplet phrases, and subsequently stores them in memory.</p>
</div>
<div id="S2.SS1.SSS2.p10" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS2.p10.1" class="ltx_p">Above, we mainly discuss the internal designs of the memory module. In the following, we turn our focus to memory operations, which are used to interact with external environments.</p>
</div>
<div id="S2.SS1.SSS2.p11" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS2.p11.1" class="ltx_p"><span id="S2.SS1.SSS2.p11.1.1" class="ltx_text ltx_font_bold">Memory Operations</span>:
There are three critical memory operations include reading, writing, and self-reflection.
In the following, we introduce these operations more in detail.
</p>
</div>
<div id="S2.SS1.SSS2.p12" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS2.p12.1" class="ltx_p"><math id="S2.SS1.SSS2.p12.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS2.p12.1.m1.1a"><mo id="S2.SS1.SSS2.p12.1.m1.1.1" xref="S2.SS1.SSS2.p12.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.1.m1.1b"><ci id="S2.SS1.SSS2.p12.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p12.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS2.p12.1.1" class="ltx_text ltx_font_italic">Memory Reading</span>.
The key to memory reading lies in extracting information from the memory.
Usually, there three commonly used criteria for information extraction, that is, the recency, relevance, and importanceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite>.
Memories that are more recent, relevant, and important are more likely to be extracted.
Formally, we conclude the following equation to extract information:</p>
<table id="S2.E1" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S2.E1X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E1X.2.1.1.m1.6" class="ltx_Math" alttext="\displaystyle m^{*}=\arg\min_{m\in M}\alpha s^{rec}(q,m)+\beta s^{rel}(q,m)+\gamma s^{imp}(m)," display="inline"><semantics id="S2.E1X.2.1.1.m1.6a"><mrow id="S2.E1X.2.1.1.m1.6.6.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.cmml"><mrow id="S2.E1X.2.1.1.m1.6.6.1.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.cmml"><msup id="S2.E1X.2.1.1.m1.6.6.1.1.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.cmml"><mi id="S2.E1X.2.1.1.m1.6.6.1.1.2.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2.cmml">m</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.2.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.3.cmml">*</mo></msup><mo id="S2.E1X.2.1.1.m1.6.6.1.1.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.1.cmml">=</mo><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.cmml"><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.3.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.cmml"><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.cmml"><mi id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.1.cmml">arg</mi><mo lspace="0.167em" id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2a" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.cmml">â¡</mo><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.cmml"><munder id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.1.cmml"><mi id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.1.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.1.2.cmml">min</mi><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.1.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.1.3.cmml"><mi id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.1.3.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.1.3.2.cmml">m</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.1.3.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.1.3.1.cmml">âˆˆ</mo><mi id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.1.3.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.1.3.3.cmml">M</mi></mrow></munder><mo lspace="0.167em" id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2a" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.cmml">â¡</mo><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.cmml"><mi id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.2.cmml">Î±</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.1.cmml">â¢</mo><msup id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.cmml"><mi id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.2.cmml">s</mi><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.3.cmml"><mi id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.3.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.3.2.cmml">r</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.3.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.3.1.cmml">â¢</mo><mi id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.3.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.3.3.cmml">e</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.3.1a" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.3.1.cmml">â¢</mo><mi id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.3.4" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.3.4.cmml">c</mi></mrow></msup></mrow></mrow></mrow><mo id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.1.cmml">â¢</mo><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.3.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.3.1.cmml"><mo stretchy="false" id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.3.2.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.3.1.cmml">(</mo><mi id="S2.E1X.2.1.1.m1.1.1" xref="S2.E1X.2.1.1.m1.1.1.cmml">q</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.3.2.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.3.1.cmml">,</mo><mi id="S2.E1X.2.1.1.m1.2.2" xref="S2.E1X.2.1.1.m1.2.2.cmml">m</mi><mo stretchy="false" id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.3.2.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.3.1.cmml">)</mo></mrow></mrow><mo id="S2.E1X.2.1.1.m1.6.6.1.1.3.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.1.cmml">+</mo><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.3.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.cmml"><mi id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.2.cmml">Î²</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.1.cmml">â¢</mo><msup id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.cmml"><mi id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.2.cmml">s</mi><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.3.cmml"><mi id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.3.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.3.2.cmml">r</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.3.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.3.1.cmml">â¢</mo><mi id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.3.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.3.3.cmml">e</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.3.1a" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.3.1.cmml">â¢</mo><mi id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.3.4" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.3.4.cmml">l</mi></mrow></msup><mo id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.1a" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.1.cmml">â¢</mo><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.4.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.4.1.cmml"><mo stretchy="false" id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.4.2.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.4.1.cmml">(</mo><mi id="S2.E1X.2.1.1.m1.3.3" xref="S2.E1X.2.1.1.m1.3.3.cmml">q</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.4.2.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.4.1.cmml">,</mo><mi id="S2.E1X.2.1.1.m1.4.4" xref="S2.E1X.2.1.1.m1.4.4.cmml">m</mi><mo stretchy="false" id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.4.2.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.4.1.cmml">)</mo></mrow></mrow><mo id="S2.E1X.2.1.1.m1.6.6.1.1.3.1a" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.1.cmml">+</mo><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.3.4" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.4.cmml"><mi id="S2.E1X.2.1.1.m1.6.6.1.1.3.4.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.4.2.cmml">Î³</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.3.4.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.4.1.cmml">â¢</mo><msup id="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.cmml"><mi id="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.2.cmml">s</mi><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.3.cmml"><mi id="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.3.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.3.2.cmml">i</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.3.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.3.1.cmml">â¢</mo><mi id="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.3.3" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.3.3.cmml">m</mi><mo id="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.3.1a" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.3.1.cmml">â¢</mo><mi id="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.3.4" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.3.4.cmml">p</mi></mrow></msup><mo id="S2.E1X.2.1.1.m1.6.6.1.1.3.4.1a" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.4.1.cmml">â¢</mo><mrow id="S2.E1X.2.1.1.m1.6.6.1.1.3.4.4.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.4.cmml"><mo stretchy="false" id="S2.E1X.2.1.1.m1.6.6.1.1.3.4.4.2.1" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.4.cmml">(</mo><mi id="S2.E1X.2.1.1.m1.5.5" xref="S2.E1X.2.1.1.m1.5.5.cmml">m</mi><mo stretchy="false" id="S2.E1X.2.1.1.m1.6.6.1.1.3.4.4.2.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.4.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S2.E1X.2.1.1.m1.6.6.1.2" xref="S2.E1X.2.1.1.m1.6.6.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1X.2.1.1.m1.6b"><apply id="S2.E1X.2.1.1.m1.6.6.1.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1"><eq id="S2.E1X.2.1.1.m1.6.6.1.1.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.1"></eq><apply id="S2.E1X.2.1.1.m1.6.6.1.1.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2"><csymbol cd="ambiguous" id="S2.E1X.2.1.1.m1.6.6.1.1.2.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2">superscript</csymbol><ci id="S2.E1X.2.1.1.m1.6.6.1.1.2.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.2">ğ‘š</ci><times id="S2.E1X.2.1.1.m1.6.6.1.1.2.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.2.3"></times></apply><apply id="S2.E1X.2.1.1.m1.6.6.1.1.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3"><plus id="S2.E1X.2.1.1.m1.6.6.1.1.3.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.1"></plus><apply id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2"><times id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.1"></times><apply id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2"><arg id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.1"></arg><apply id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2"><apply id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.1"><csymbol cd="ambiguous" id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.1.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.1">subscript</csymbol><min id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.1.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.1.2"></min><apply id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.1.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.1.3"><in id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.1.3.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.1.3.1"></in><ci id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.1.3.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.1.3.2">ğ‘š</ci><ci id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.1.3.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.1.3.3">ğ‘€</ci></apply></apply><apply id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2"><times id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.1"></times><ci id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.2">ğ›¼</ci><apply id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3"><csymbol cd="ambiguous" id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3">superscript</csymbol><ci id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.2">ğ‘ </ci><apply id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.3"><times id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.3.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.3.1"></times><ci id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.3.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.3.2">ğ‘Ÿ</ci><ci id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.3.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.3.3">ğ‘’</ci><ci id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.3.4.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.2.2.2.3.3.4">ğ‘</ci></apply></apply></apply></apply></apply><interval closure="open" id="S2.E1X.2.1.1.m1.6.6.1.1.3.2.3.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.2.3.2"><ci id="S2.E1X.2.1.1.m1.1.1.cmml" xref="S2.E1X.2.1.1.m1.1.1">ğ‘</ci><ci id="S2.E1X.2.1.1.m1.2.2.cmml" xref="S2.E1X.2.1.1.m1.2.2">ğ‘š</ci></interval></apply><apply id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3"><times id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.1"></times><ci id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.2">ğ›½</ci><apply id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3">superscript</csymbol><ci id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.2">ğ‘ </ci><apply id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.3"><times id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.3.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.3.1"></times><ci id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.3.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.3.2">ğ‘Ÿ</ci><ci id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.3.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.3.3">ğ‘’</ci><ci id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.3.4.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.3.3.4">ğ‘™</ci></apply></apply><interval closure="open" id="S2.E1X.2.1.1.m1.6.6.1.1.3.3.4.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.3.4.2"><ci id="S2.E1X.2.1.1.m1.3.3.cmml" xref="S2.E1X.2.1.1.m1.3.3">ğ‘</ci><ci id="S2.E1X.2.1.1.m1.4.4.cmml" xref="S2.E1X.2.1.1.m1.4.4">ğ‘š</ci></interval></apply><apply id="S2.E1X.2.1.1.m1.6.6.1.1.3.4.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.4"><times id="S2.E1X.2.1.1.m1.6.6.1.1.3.4.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.4.1"></times><ci id="S2.E1X.2.1.1.m1.6.6.1.1.3.4.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.4.2">ğ›¾</ci><apply id="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3"><csymbol cd="ambiguous" id="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3">superscript</csymbol><ci id="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.2">ğ‘ </ci><apply id="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.3"><times id="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.3.1.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.3.1"></times><ci id="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.3.2.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.3.2">ğ‘–</ci><ci id="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.3.3.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.3.3">ğ‘š</ci><ci id="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.3.4.cmml" xref="S2.E1X.2.1.1.m1.6.6.1.1.3.4.3.3.4">ğ‘</ci></apply></apply><ci id="S2.E1X.2.1.1.m1.5.5.cmml" xref="S2.E1X.2.1.1.m1.5.5">ğ‘š</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1X.2.1.1.m1.6c">\displaystyle m^{*}=\arg\min_{m\in M}\alpha s^{rec}(q,m)+\beta s^{rel}(q,m)+\gamma s^{imp}(m),</annotation><annotation encoding="application/x-llamapun" id="S2.E1X.2.1.1.m1.6d">italic_m start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT = roman_arg roman_min start_POSTSUBSCRIPT italic_m âˆˆ italic_M end_POSTSUBSCRIPT italic_Î± italic_s start_POSTSUPERSCRIPT italic_r italic_e italic_c end_POSTSUPERSCRIPT ( italic_q , italic_m ) + italic_Î² italic_s start_POSTSUPERSCRIPT italic_r italic_e italic_l end_POSTSUPERSCRIPT ( italic_q , italic_m ) + italic_Î³ italic_s start_POSTSUPERSCRIPT italic_i italic_m italic_p end_POSTSUPERSCRIPT ( italic_m ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(1)</span></td>
</tr>
</tbody>
</table>
<p id="S2.SS1.SSS2.p12.15" class="ltx_p">where <math id="S2.SS1.SSS2.p12.2.m1.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S2.SS1.SSS2.p12.2.m1.1a"><mi id="S2.SS1.SSS2.p12.2.m1.1.1" xref="S2.SS1.SSS2.p12.2.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.2.m1.1b"><ci id="S2.SS1.SSS2.p12.2.m1.1.1.cmml" xref="S2.SS1.SSS2.p12.2.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.2.m1.1c">q</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.2.m1.1d">italic_q</annotation></semantics></math> is the query, for example, the task that the agent should address or the context in which the agent is situated.
<math id="S2.SS1.SSS2.p12.3.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.SS1.SSS2.p12.3.m2.1a"><mi id="S2.SS1.SSS2.p12.3.m2.1.1" xref="S2.SS1.SSS2.p12.3.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.3.m2.1b"><ci id="S2.SS1.SSS2.p12.3.m2.1.1.cmml" xref="S2.SS1.SSS2.p12.3.m2.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.3.m2.1c">M</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.3.m2.1d">italic_M</annotation></semantics></math> is the set of all memories.
<math id="S2.SS1.SSS2.p12.4.m3.1" class="ltx_Math" alttext="s^{rec}(\cdot)" display="inline"><semantics id="S2.SS1.SSS2.p12.4.m3.1a"><mrow id="S2.SS1.SSS2.p12.4.m3.1.2" xref="S2.SS1.SSS2.p12.4.m3.1.2.cmml"><msup id="S2.SS1.SSS2.p12.4.m3.1.2.2" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.cmml"><mi id="S2.SS1.SSS2.p12.4.m3.1.2.2.2" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.2.cmml">s</mi><mrow id="S2.SS1.SSS2.p12.4.m3.1.2.2.3" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.3.cmml"><mi id="S2.SS1.SSS2.p12.4.m3.1.2.2.3.2" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.3.2.cmml">r</mi><mo id="S2.SS1.SSS2.p12.4.m3.1.2.2.3.1" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.3.1.cmml">â¢</mo><mi id="S2.SS1.SSS2.p12.4.m3.1.2.2.3.3" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.3.3.cmml">e</mi><mo id="S2.SS1.SSS2.p12.4.m3.1.2.2.3.1a" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.3.1.cmml">â¢</mo><mi id="S2.SS1.SSS2.p12.4.m3.1.2.2.3.4" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.3.4.cmml">c</mi></mrow></msup><mo id="S2.SS1.SSS2.p12.4.m3.1.2.1" xref="S2.SS1.SSS2.p12.4.m3.1.2.1.cmml">â¢</mo><mrow id="S2.SS1.SSS2.p12.4.m3.1.2.3.2" xref="S2.SS1.SSS2.p12.4.m3.1.2.cmml"><mo stretchy="false" id="S2.SS1.SSS2.p12.4.m3.1.2.3.2.1" xref="S2.SS1.SSS2.p12.4.m3.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p12.4.m3.1.1" xref="S2.SS1.SSS2.p12.4.m3.1.1.cmml">â‹…</mo><mo stretchy="false" id="S2.SS1.SSS2.p12.4.m3.1.2.3.2.2" xref="S2.SS1.SSS2.p12.4.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.4.m3.1b"><apply id="S2.SS1.SSS2.p12.4.m3.1.2.cmml" xref="S2.SS1.SSS2.p12.4.m3.1.2"><times id="S2.SS1.SSS2.p12.4.m3.1.2.1.cmml" xref="S2.SS1.SSS2.p12.4.m3.1.2.1"></times><apply id="S2.SS1.SSS2.p12.4.m3.1.2.2.cmml" xref="S2.SS1.SSS2.p12.4.m3.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p12.4.m3.1.2.2.1.cmml" xref="S2.SS1.SSS2.p12.4.m3.1.2.2">superscript</csymbol><ci id="S2.SS1.SSS2.p12.4.m3.1.2.2.2.cmml" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.2">ğ‘ </ci><apply id="S2.SS1.SSS2.p12.4.m3.1.2.2.3.cmml" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.3"><times id="S2.SS1.SSS2.p12.4.m3.1.2.2.3.1.cmml" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.3.1"></times><ci id="S2.SS1.SSS2.p12.4.m3.1.2.2.3.2.cmml" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.3.2">ğ‘Ÿ</ci><ci id="S2.SS1.SSS2.p12.4.m3.1.2.2.3.3.cmml" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.3.3">ğ‘’</ci><ci id="S2.SS1.SSS2.p12.4.m3.1.2.2.3.4.cmml" xref="S2.SS1.SSS2.p12.4.m3.1.2.2.3.4">ğ‘</ci></apply></apply><ci id="S2.SS1.SSS2.p12.4.m3.1.1.cmml" xref="S2.SS1.SSS2.p12.4.m3.1.1">â‹…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.4.m3.1c">s^{rec}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.4.m3.1d">italic_s start_POSTSUPERSCRIPT italic_r italic_e italic_c end_POSTSUPERSCRIPT ( â‹… )</annotation></semantics></math>, <math id="S2.SS1.SSS2.p12.5.m4.1" class="ltx_Math" alttext="s^{rel}(\cdot)" display="inline"><semantics id="S2.SS1.SSS2.p12.5.m4.1a"><mrow id="S2.SS1.SSS2.p12.5.m4.1.2" xref="S2.SS1.SSS2.p12.5.m4.1.2.cmml"><msup id="S2.SS1.SSS2.p12.5.m4.1.2.2" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.cmml"><mi id="S2.SS1.SSS2.p12.5.m4.1.2.2.2" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.2.cmml">s</mi><mrow id="S2.SS1.SSS2.p12.5.m4.1.2.2.3" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.3.cmml"><mi id="S2.SS1.SSS2.p12.5.m4.1.2.2.3.2" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.3.2.cmml">r</mi><mo id="S2.SS1.SSS2.p12.5.m4.1.2.2.3.1" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.3.1.cmml">â¢</mo><mi id="S2.SS1.SSS2.p12.5.m4.1.2.2.3.3" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.3.3.cmml">e</mi><mo id="S2.SS1.SSS2.p12.5.m4.1.2.2.3.1a" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.3.1.cmml">â¢</mo><mi id="S2.SS1.SSS2.p12.5.m4.1.2.2.3.4" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.3.4.cmml">l</mi></mrow></msup><mo id="S2.SS1.SSS2.p12.5.m4.1.2.1" xref="S2.SS1.SSS2.p12.5.m4.1.2.1.cmml">â¢</mo><mrow id="S2.SS1.SSS2.p12.5.m4.1.2.3.2" xref="S2.SS1.SSS2.p12.5.m4.1.2.cmml"><mo stretchy="false" id="S2.SS1.SSS2.p12.5.m4.1.2.3.2.1" xref="S2.SS1.SSS2.p12.5.m4.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p12.5.m4.1.1" xref="S2.SS1.SSS2.p12.5.m4.1.1.cmml">â‹…</mo><mo stretchy="false" id="S2.SS1.SSS2.p12.5.m4.1.2.3.2.2" xref="S2.SS1.SSS2.p12.5.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.5.m4.1b"><apply id="S2.SS1.SSS2.p12.5.m4.1.2.cmml" xref="S2.SS1.SSS2.p12.5.m4.1.2"><times id="S2.SS1.SSS2.p12.5.m4.1.2.1.cmml" xref="S2.SS1.SSS2.p12.5.m4.1.2.1"></times><apply id="S2.SS1.SSS2.p12.5.m4.1.2.2.cmml" xref="S2.SS1.SSS2.p12.5.m4.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p12.5.m4.1.2.2.1.cmml" xref="S2.SS1.SSS2.p12.5.m4.1.2.2">superscript</csymbol><ci id="S2.SS1.SSS2.p12.5.m4.1.2.2.2.cmml" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.2">ğ‘ </ci><apply id="S2.SS1.SSS2.p12.5.m4.1.2.2.3.cmml" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.3"><times id="S2.SS1.SSS2.p12.5.m4.1.2.2.3.1.cmml" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.3.1"></times><ci id="S2.SS1.SSS2.p12.5.m4.1.2.2.3.2.cmml" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.3.2">ğ‘Ÿ</ci><ci id="S2.SS1.SSS2.p12.5.m4.1.2.2.3.3.cmml" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.3.3">ğ‘’</ci><ci id="S2.SS1.SSS2.p12.5.m4.1.2.2.3.4.cmml" xref="S2.SS1.SSS2.p12.5.m4.1.2.2.3.4">ğ‘™</ci></apply></apply><ci id="S2.SS1.SSS2.p12.5.m4.1.1.cmml" xref="S2.SS1.SSS2.p12.5.m4.1.1">â‹…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.5.m4.1c">s^{rel}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.5.m4.1d">italic_s start_POSTSUPERSCRIPT italic_r italic_e italic_l end_POSTSUPERSCRIPT ( â‹… )</annotation></semantics></math> and <math id="S2.SS1.SSS2.p12.6.m5.1" class="ltx_Math" alttext="s^{imp}(\cdot)" display="inline"><semantics id="S2.SS1.SSS2.p12.6.m5.1a"><mrow id="S2.SS1.SSS2.p12.6.m5.1.2" xref="S2.SS1.SSS2.p12.6.m5.1.2.cmml"><msup id="S2.SS1.SSS2.p12.6.m5.1.2.2" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.cmml"><mi id="S2.SS1.SSS2.p12.6.m5.1.2.2.2" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.2.cmml">s</mi><mrow id="S2.SS1.SSS2.p12.6.m5.1.2.2.3" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.3.cmml"><mi id="S2.SS1.SSS2.p12.6.m5.1.2.2.3.2" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.3.2.cmml">i</mi><mo id="S2.SS1.SSS2.p12.6.m5.1.2.2.3.1" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.3.1.cmml">â¢</mo><mi id="S2.SS1.SSS2.p12.6.m5.1.2.2.3.3" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.3.3.cmml">m</mi><mo id="S2.SS1.SSS2.p12.6.m5.1.2.2.3.1a" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.3.1.cmml">â¢</mo><mi id="S2.SS1.SSS2.p12.6.m5.1.2.2.3.4" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.3.4.cmml">p</mi></mrow></msup><mo id="S2.SS1.SSS2.p12.6.m5.1.2.1" xref="S2.SS1.SSS2.p12.6.m5.1.2.1.cmml">â¢</mo><mrow id="S2.SS1.SSS2.p12.6.m5.1.2.3.2" xref="S2.SS1.SSS2.p12.6.m5.1.2.cmml"><mo stretchy="false" id="S2.SS1.SSS2.p12.6.m5.1.2.3.2.1" xref="S2.SS1.SSS2.p12.6.m5.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p12.6.m5.1.1" xref="S2.SS1.SSS2.p12.6.m5.1.1.cmml">â‹…</mo><mo stretchy="false" id="S2.SS1.SSS2.p12.6.m5.1.2.3.2.2" xref="S2.SS1.SSS2.p12.6.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.6.m5.1b"><apply id="S2.SS1.SSS2.p12.6.m5.1.2.cmml" xref="S2.SS1.SSS2.p12.6.m5.1.2"><times id="S2.SS1.SSS2.p12.6.m5.1.2.1.cmml" xref="S2.SS1.SSS2.p12.6.m5.1.2.1"></times><apply id="S2.SS1.SSS2.p12.6.m5.1.2.2.cmml" xref="S2.SS1.SSS2.p12.6.m5.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p12.6.m5.1.2.2.1.cmml" xref="S2.SS1.SSS2.p12.6.m5.1.2.2">superscript</csymbol><ci id="S2.SS1.SSS2.p12.6.m5.1.2.2.2.cmml" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.2">ğ‘ </ci><apply id="S2.SS1.SSS2.p12.6.m5.1.2.2.3.cmml" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.3"><times id="S2.SS1.SSS2.p12.6.m5.1.2.2.3.1.cmml" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.3.1"></times><ci id="S2.SS1.SSS2.p12.6.m5.1.2.2.3.2.cmml" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.3.2">ğ‘–</ci><ci id="S2.SS1.SSS2.p12.6.m5.1.2.2.3.3.cmml" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.3.3">ğ‘š</ci><ci id="S2.SS1.SSS2.p12.6.m5.1.2.2.3.4.cmml" xref="S2.SS1.SSS2.p12.6.m5.1.2.2.3.4">ğ‘</ci></apply></apply><ci id="S2.SS1.SSS2.p12.6.m5.1.1.cmml" xref="S2.SS1.SSS2.p12.6.m5.1.1">â‹…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.6.m5.1c">s^{imp}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.6.m5.1d">italic_s start_POSTSUPERSCRIPT italic_i italic_m italic_p end_POSTSUPERSCRIPT ( â‹… )</annotation></semantics></math> are the scoring functions for measuring the recency, relevance, and importance of the memory <math id="S2.SS1.SSS2.p12.7.m6.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S2.SS1.SSS2.p12.7.m6.1a"><mi id="S2.SS1.SSS2.p12.7.m6.1.1" xref="S2.SS1.SSS2.p12.7.m6.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.7.m6.1b"><ci id="S2.SS1.SSS2.p12.7.m6.1.1.cmml" xref="S2.SS1.SSS2.p12.7.m6.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.7.m6.1c">m</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.7.m6.1d">italic_m</annotation></semantics></math>.
It should be noted that <math id="S2.SS1.SSS2.p12.8.m7.1" class="ltx_Math" alttext="s^{imp}" display="inline"><semantics id="S2.SS1.SSS2.p12.8.m7.1a"><msup id="S2.SS1.SSS2.p12.8.m7.1.1" xref="S2.SS1.SSS2.p12.8.m7.1.1.cmml"><mi id="S2.SS1.SSS2.p12.8.m7.1.1.2" xref="S2.SS1.SSS2.p12.8.m7.1.1.2.cmml">s</mi><mrow id="S2.SS1.SSS2.p12.8.m7.1.1.3" xref="S2.SS1.SSS2.p12.8.m7.1.1.3.cmml"><mi id="S2.SS1.SSS2.p12.8.m7.1.1.3.2" xref="S2.SS1.SSS2.p12.8.m7.1.1.3.2.cmml">i</mi><mo id="S2.SS1.SSS2.p12.8.m7.1.1.3.1" xref="S2.SS1.SSS2.p12.8.m7.1.1.3.1.cmml">â¢</mo><mi id="S2.SS1.SSS2.p12.8.m7.1.1.3.3" xref="S2.SS1.SSS2.p12.8.m7.1.1.3.3.cmml">m</mi><mo id="S2.SS1.SSS2.p12.8.m7.1.1.3.1a" xref="S2.SS1.SSS2.p12.8.m7.1.1.3.1.cmml">â¢</mo><mi id="S2.SS1.SSS2.p12.8.m7.1.1.3.4" xref="S2.SS1.SSS2.p12.8.m7.1.1.3.4.cmml">p</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.8.m7.1b"><apply id="S2.SS1.SSS2.p12.8.m7.1.1.cmml" xref="S2.SS1.SSS2.p12.8.m7.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p12.8.m7.1.1.1.cmml" xref="S2.SS1.SSS2.p12.8.m7.1.1">superscript</csymbol><ci id="S2.SS1.SSS2.p12.8.m7.1.1.2.cmml" xref="S2.SS1.SSS2.p12.8.m7.1.1.2">ğ‘ </ci><apply id="S2.SS1.SSS2.p12.8.m7.1.1.3.cmml" xref="S2.SS1.SSS2.p12.8.m7.1.1.3"><times id="S2.SS1.SSS2.p12.8.m7.1.1.3.1.cmml" xref="S2.SS1.SSS2.p12.8.m7.1.1.3.1"></times><ci id="S2.SS1.SSS2.p12.8.m7.1.1.3.2.cmml" xref="S2.SS1.SSS2.p12.8.m7.1.1.3.2">ğ‘–</ci><ci id="S2.SS1.SSS2.p12.8.m7.1.1.3.3.cmml" xref="S2.SS1.SSS2.p12.8.m7.1.1.3.3">ğ‘š</ci><ci id="S2.SS1.SSS2.p12.8.m7.1.1.3.4.cmml" xref="S2.SS1.SSS2.p12.8.m7.1.1.3.4">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.8.m7.1c">s^{imp}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.8.m7.1d">italic_s start_POSTSUPERSCRIPT italic_i italic_m italic_p end_POSTSUPERSCRIPT</annotation></semantics></math> only reflects the characters of the memory itself, thus it is unrelated to the query <math id="S2.SS1.SSS2.p12.9.m8.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S2.SS1.SSS2.p12.9.m8.1a"><mi id="S2.SS1.SSS2.p12.9.m8.1.1" xref="S2.SS1.SSS2.p12.9.m8.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.9.m8.1b"><ci id="S2.SS1.SSS2.p12.9.m8.1.1.cmml" xref="S2.SS1.SSS2.p12.9.m8.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.9.m8.1c">q</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.9.m8.1d">italic_q</annotation></semantics></math>.
<math id="S2.SS1.SSS2.p12.10.m9.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S2.SS1.SSS2.p12.10.m9.1a"><mi id="S2.SS1.SSS2.p12.10.m9.1.1" xref="S2.SS1.SSS2.p12.10.m9.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.10.m9.1b"><ci id="S2.SS1.SSS2.p12.10.m9.1.1.cmml" xref="S2.SS1.SSS2.p12.10.m9.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.10.m9.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.10.m9.1d">italic_Î±</annotation></semantics></math>, <math id="S2.SS1.SSS2.p12.11.m10.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S2.SS1.SSS2.p12.11.m10.1a"><mi id="S2.SS1.SSS2.p12.11.m10.1.1" xref="S2.SS1.SSS2.p12.11.m10.1.1.cmml">Î²</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.11.m10.1b"><ci id="S2.SS1.SSS2.p12.11.m10.1.1.cmml" xref="S2.SS1.SSS2.p12.11.m10.1.1">ğ›½</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.11.m10.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.11.m10.1d">italic_Î²</annotation></semantics></math> and <math id="S2.SS1.SSS2.p12.12.m11.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S2.SS1.SSS2.p12.12.m11.1a"><mi id="S2.SS1.SSS2.p12.12.m11.1.1" xref="S2.SS1.SSS2.p12.12.m11.1.1.cmml">Î³</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.12.m11.1b"><ci id="S2.SS1.SSS2.p12.12.m11.1.1.cmml" xref="S2.SS1.SSS2.p12.12.m11.1.1">ğ›¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.12.m11.1c">\gamma</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.12.m11.1d">italic_Î³</annotation></semantics></math> are balancing parameters.
By assigning them with different values, one can obtain various memory reading strategies.
For example, by setting <math id="S2.SS1.SSS2.p12.13.m12.1" class="ltx_Math" alttext="\alpha=\gamma=0" display="inline"><semantics id="S2.SS1.SSS2.p12.13.m12.1a"><mrow id="S2.SS1.SSS2.p12.13.m12.1.1" xref="S2.SS1.SSS2.p12.13.m12.1.1.cmml"><mi id="S2.SS1.SSS2.p12.13.m12.1.1.2" xref="S2.SS1.SSS2.p12.13.m12.1.1.2.cmml">Î±</mi><mo id="S2.SS1.SSS2.p12.13.m12.1.1.3" xref="S2.SS1.SSS2.p12.13.m12.1.1.3.cmml">=</mo><mi id="S2.SS1.SSS2.p12.13.m12.1.1.4" xref="S2.SS1.SSS2.p12.13.m12.1.1.4.cmml">Î³</mi><mo id="S2.SS1.SSS2.p12.13.m12.1.1.5" xref="S2.SS1.SSS2.p12.13.m12.1.1.5.cmml">=</mo><mn id="S2.SS1.SSS2.p12.13.m12.1.1.6" xref="S2.SS1.SSS2.p12.13.m12.1.1.6.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.13.m12.1b"><apply id="S2.SS1.SSS2.p12.13.m12.1.1.cmml" xref="S2.SS1.SSS2.p12.13.m12.1.1"><and id="S2.SS1.SSS2.p12.13.m12.1.1a.cmml" xref="S2.SS1.SSS2.p12.13.m12.1.1"></and><apply id="S2.SS1.SSS2.p12.13.m12.1.1b.cmml" xref="S2.SS1.SSS2.p12.13.m12.1.1"><eq id="S2.SS1.SSS2.p12.13.m12.1.1.3.cmml" xref="S2.SS1.SSS2.p12.13.m12.1.1.3"></eq><ci id="S2.SS1.SSS2.p12.13.m12.1.1.2.cmml" xref="S2.SS1.SSS2.p12.13.m12.1.1.2">ğ›¼</ci><ci id="S2.SS1.SSS2.p12.13.m12.1.1.4.cmml" xref="S2.SS1.SSS2.p12.13.m12.1.1.4">ğ›¾</ci></apply><apply id="S2.SS1.SSS2.p12.13.m12.1.1c.cmml" xref="S2.SS1.SSS2.p12.13.m12.1.1"><eq id="S2.SS1.SSS2.p12.13.m12.1.1.5.cmml" xref="S2.SS1.SSS2.p12.13.m12.1.1.5"></eq><share href="#S2.SS1.SSS2.p12.13.m12.1.1.4.cmml" id="S2.SS1.SSS2.p12.13.m12.1.1d.cmml" xref="S2.SS1.SSS2.p12.13.m12.1.1"></share><cn type="integer" id="S2.SS1.SSS2.p12.13.m12.1.1.6.cmml" xref="S2.SS1.SSS2.p12.13.m12.1.1.6">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.13.m12.1c">\alpha=\gamma=0</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.13.m12.1d">italic_Î± = italic_Î³ = 0</annotation></semantics></math>, many studiesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>, <a href="#bib.bib161" title="" class="ltx_ref">161</a>, <a href="#bib.bib133" title="" class="ltx_ref">133</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> only consider the relevance score <math id="S2.SS1.SSS2.p12.14.m13.1" class="ltx_Math" alttext="s^{rel}" display="inline"><semantics id="S2.SS1.SSS2.p12.14.m13.1a"><msup id="S2.SS1.SSS2.p12.14.m13.1.1" xref="S2.SS1.SSS2.p12.14.m13.1.1.cmml"><mi id="S2.SS1.SSS2.p12.14.m13.1.1.2" xref="S2.SS1.SSS2.p12.14.m13.1.1.2.cmml">s</mi><mrow id="S2.SS1.SSS2.p12.14.m13.1.1.3" xref="S2.SS1.SSS2.p12.14.m13.1.1.3.cmml"><mi id="S2.SS1.SSS2.p12.14.m13.1.1.3.2" xref="S2.SS1.SSS2.p12.14.m13.1.1.3.2.cmml">r</mi><mo id="S2.SS1.SSS2.p12.14.m13.1.1.3.1" xref="S2.SS1.SSS2.p12.14.m13.1.1.3.1.cmml">â¢</mo><mi id="S2.SS1.SSS2.p12.14.m13.1.1.3.3" xref="S2.SS1.SSS2.p12.14.m13.1.1.3.3.cmml">e</mi><mo id="S2.SS1.SSS2.p12.14.m13.1.1.3.1a" xref="S2.SS1.SSS2.p12.14.m13.1.1.3.1.cmml">â¢</mo><mi id="S2.SS1.SSS2.p12.14.m13.1.1.3.4" xref="S2.SS1.SSS2.p12.14.m13.1.1.3.4.cmml">l</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.14.m13.1b"><apply id="S2.SS1.SSS2.p12.14.m13.1.1.cmml" xref="S2.SS1.SSS2.p12.14.m13.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p12.14.m13.1.1.1.cmml" xref="S2.SS1.SSS2.p12.14.m13.1.1">superscript</csymbol><ci id="S2.SS1.SSS2.p12.14.m13.1.1.2.cmml" xref="S2.SS1.SSS2.p12.14.m13.1.1.2">ğ‘ </ci><apply id="S2.SS1.SSS2.p12.14.m13.1.1.3.cmml" xref="S2.SS1.SSS2.p12.14.m13.1.1.3"><times id="S2.SS1.SSS2.p12.14.m13.1.1.3.1.cmml" xref="S2.SS1.SSS2.p12.14.m13.1.1.3.1"></times><ci id="S2.SS1.SSS2.p12.14.m13.1.1.3.2.cmml" xref="S2.SS1.SSS2.p12.14.m13.1.1.3.2">ğ‘Ÿ</ci><ci id="S2.SS1.SSS2.p12.14.m13.1.1.3.3.cmml" xref="S2.SS1.SSS2.p12.14.m13.1.1.3.3">ğ‘’</ci><ci id="S2.SS1.SSS2.p12.14.m13.1.1.3.4.cmml" xref="S2.SS1.SSS2.p12.14.m13.1.1.3.4">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.14.m13.1c">s^{rel}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.14.m13.1d">italic_s start_POSTSUPERSCRIPT italic_r italic_e italic_l end_POSTSUPERSCRIPT</annotation></semantics></math> for memory reading.
By assigning <math id="S2.SS1.SSS2.p12.15.m14.1" class="ltx_Math" alttext="\alpha=\beta=\gamma=1.0" display="inline"><semantics id="S2.SS1.SSS2.p12.15.m14.1a"><mrow id="S2.SS1.SSS2.p12.15.m14.1.1" xref="S2.SS1.SSS2.p12.15.m14.1.1.cmml"><mi id="S2.SS1.SSS2.p12.15.m14.1.1.2" xref="S2.SS1.SSS2.p12.15.m14.1.1.2.cmml">Î±</mi><mo id="S2.SS1.SSS2.p12.15.m14.1.1.3" xref="S2.SS1.SSS2.p12.15.m14.1.1.3.cmml">=</mo><mi id="S2.SS1.SSS2.p12.15.m14.1.1.4" xref="S2.SS1.SSS2.p12.15.m14.1.1.4.cmml">Î²</mi><mo id="S2.SS1.SSS2.p12.15.m14.1.1.5" xref="S2.SS1.SSS2.p12.15.m14.1.1.5.cmml">=</mo><mi id="S2.SS1.SSS2.p12.15.m14.1.1.6" xref="S2.SS1.SSS2.p12.15.m14.1.1.6.cmml">Î³</mi><mo id="S2.SS1.SSS2.p12.15.m14.1.1.7" xref="S2.SS1.SSS2.p12.15.m14.1.1.7.cmml">=</mo><mn id="S2.SS1.SSS2.p12.15.m14.1.1.8" xref="S2.SS1.SSS2.p12.15.m14.1.1.8.cmml">1.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p12.15.m14.1b"><apply id="S2.SS1.SSS2.p12.15.m14.1.1.cmml" xref="S2.SS1.SSS2.p12.15.m14.1.1"><and id="S2.SS1.SSS2.p12.15.m14.1.1a.cmml" xref="S2.SS1.SSS2.p12.15.m14.1.1"></and><apply id="S2.SS1.SSS2.p12.15.m14.1.1b.cmml" xref="S2.SS1.SSS2.p12.15.m14.1.1"><eq id="S2.SS1.SSS2.p12.15.m14.1.1.3.cmml" xref="S2.SS1.SSS2.p12.15.m14.1.1.3"></eq><ci id="S2.SS1.SSS2.p12.15.m14.1.1.2.cmml" xref="S2.SS1.SSS2.p12.15.m14.1.1.2">ğ›¼</ci><ci id="S2.SS1.SSS2.p12.15.m14.1.1.4.cmml" xref="S2.SS1.SSS2.p12.15.m14.1.1.4">ğ›½</ci></apply><apply id="S2.SS1.SSS2.p12.15.m14.1.1c.cmml" xref="S2.SS1.SSS2.p12.15.m14.1.1"><eq id="S2.SS1.SSS2.p12.15.m14.1.1.5.cmml" xref="S2.SS1.SSS2.p12.15.m14.1.1.5"></eq><share href="#S2.SS1.SSS2.p12.15.m14.1.1.4.cmml" id="S2.SS1.SSS2.p12.15.m14.1.1d.cmml" xref="S2.SS1.SSS2.p12.15.m14.1.1"></share><ci id="S2.SS1.SSS2.p12.15.m14.1.1.6.cmml" xref="S2.SS1.SSS2.p12.15.m14.1.1.6">ğ›¾</ci></apply><apply id="S2.SS1.SSS2.p12.15.m14.1.1e.cmml" xref="S2.SS1.SSS2.p12.15.m14.1.1"><eq id="S2.SS1.SSS2.p12.15.m14.1.1.7.cmml" xref="S2.SS1.SSS2.p12.15.m14.1.1.7"></eq><share href="#S2.SS1.SSS2.p12.15.m14.1.1.6.cmml" id="S2.SS1.SSS2.p12.15.m14.1.1f.cmml" xref="S2.SS1.SSS2.p12.15.m14.1.1"></share><cn type="float" id="S2.SS1.SSS2.p12.15.m14.1.1.8.cmml" xref="S2.SS1.SSS2.p12.15.m14.1.1.8">1.0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p12.15.m14.1c">\alpha=\beta=\gamma=1.0</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p12.15.m14.1d">italic_Î± = italic_Î² = italic_Î³ = 1.0</annotation></semantics></math>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite> equally weights all the above three metrics to extract information from the memory.</p>
</div>
<div id="S2.SS1.SSS2.p13" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS2.p13.1" class="ltx_p"><math id="S2.SS1.SSS2.p13.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS2.p13.1.m1.1a"><mo id="S2.SS1.SSS2.p13.1.m1.1.1" xref="S2.SS1.SSS2.p13.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p13.1.m1.1b"><ci id="S2.SS1.SSS2.p13.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p13.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p13.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p13.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS2.p13.1.1" class="ltx_text ltx_font_italic">Memory Writing</span>.
Agents can acquire knowledge and experiences by storing significant information in their memories.
During the writing process, there are two potential problems that should be carefully addressed.
On one hand, it is crucial to address how to store information that is similar to existing memories (<em id="S2.SS1.SSS2.p13.1.2" class="ltx_emph ltx_font_italic">i.e.</em>, memory duplicated).
On the other hand, it is important to consider how to remove information when the memory reaches its storage limit (<em id="S2.SS1.SSS2.p13.1.3" class="ltx_emph ltx_font_italic">i.e.</em>, memory overflow).
These problems can be resolved based on the following strategies.
(1) <span id="S2.SS1.SSS2.p13.1.4" class="ltx_text ltx_font_italic">Memory Duplicated</span>.
To incorporate similar information, people have developed various methods for integrating new and previous records.
For instance, inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite>, the successful action sequences related to the same sub-goal are stored in a list.
Once the size of the list reaches N(=5), all the sequences in it are condensed into a unified plan solution using LLMs.
The original sequences in the memory are replaced with the newly generated one.
Augmented LLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite> aggregates duplicate information via count accumulation, avoiding redundant storage.
Reflexion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite> consolidates related feedback into high-level insights, replacing raw experiences.
(2) <span id="S2.SS1.SSS2.p13.1.5" class="ltx_text ltx_font_italic">Memory Overflow</span>.
In order to write information into the memory when it is full, people design different methods to delete existing information to continue the memorizing process.
For example, in ChatDBÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>, memories can be explicitly deleted based on the userâ€™s command.
RET-LLMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite> uses a fixed-size cyclic buffer for memory, overwriting the oldest entries based on a first-in-first-out (FIFO) scheme.</p>
</div>
<div id="S2.SS1.SSS2.p14" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS2.p14.1" class="ltx_p"><math id="S2.SS1.SSS2.p14.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS2.p14.1.m1.1a"><mo id="S2.SS1.SSS2.p14.1.m1.1.1" xref="S2.SS1.SSS2.p14.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p14.1.m1.1b"><ci id="S2.SS1.SSS2.p14.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p14.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p14.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p14.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS2.p14.1.1" class="ltx_text ltx_font_italic">Memory Reflection</span>.
This operation seeks to empower agents with the ability to condense and deduce more advanced information, or verify and correct their own actions autonomously.
It assists agents in comprehending their own and othersâ€™ attributes, preferences, objectives, and connections, which in turn directs their behaviors.
Previous studies have studied various forms of memory reflection, that is, (1) <span id="S2.SS1.SSS2.p14.1.2" class="ltx_text ltx_font_italic">Self-summarization</span>.
Reflection can be utilized to condense the agentâ€™s memories into higher-level concepts.
InÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite>, the agent has the capability to summarize its past experiences stored in memory into broader and more abstract insights.
Specifically, the agent first generates three key questions based on its recent memories.
Then, these questions are used to query the memory to obtain relevant information.
Building upon the acquired information, the agent generates five insights, which reflect the agent high-level ideas.
Additionally, reflection can occur hierarchically, meaning that insights can be generated based on existing insights.
(2) <span id="S2.SS1.SSS2.p14.1.3" class="ltx_text ltx_font_italic">Self-verification</span>.
Another form of reflection involves evaluating the effectiveness of the agentâ€™s actions.
InÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite>, the agent aims to accomplish tasks in Minecraft.
During each execution round, the agent utilizes GPT-4 as a critic to assess whether the current action is sufficient to achieve the desired task.
If the task fails, the critic offers feedback by suggesting approaches for completing the task.
Replug <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite> employs a training scheme to further adapt the retrieval model to the target language model. Specifically, it utilizes a language model as a scoring function to assess the contribution of each document towards reducing the perplexity of the language model. The retrieval model parameters are updated by minimizing the KL divergence between the retrieval probabilities and the language model scores.
This approach effectively evaluates the relevance of the retrieved results and adjusts them based on feedback from the language model.
(3) <span id="S2.SS1.SSS2.p14.1.4" class="ltx_text ltx_font_italic">Self-correction</span>.
In this type of reflection, the agent can correct its behaviors by incorporating feedback from the environment.
In MemPrompt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite>, the model can adjust its understanding of the tasks based on user feedback for generating more accurate answers. InÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib137" title="" class="ltx_ref">137</a>]</cite>, the agent is designed to play Minecraft, and it takes actions based on predefined plans.
When the plan fails, the agent rethinks its plan and changes it to continue the exploration process.
(4) <span id="S2.SS1.SSS2.p14.1.5" class="ltx_text ltx_font_italic">Empathy</span>.
Memory reflection can also be leveraged to enhance the agentâ€™s empathy capability.
InÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, the agent is a chatbot, but it generates utterances by considering the human cognition process.
After each round of conversation, the agent evaluates the impact of his words on the listener and updates his beliefs about the listenerâ€™s states.
</p>
</div>
</section>
<section id="S2.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>Planning Module</h4>

<div id="S2.SS1.SSS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS3.p1.1" class="ltx_p">When humans face a complex task, they first break it down into simple subtasks and then solve each subtask one by one.
The planning module empowers LLM-based agents with the ability to think and plan for solving complex tasks, which makes the agent more comprehensive, powerful, and reliable.
In the following, we present two types of planning modules.</p>
</div>
<div id="S2.SS1.SSS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS3.p2.1" class="ltx_p"><span id="S2.SS1.SSS3.p2.1.1" class="ltx_text ltx_font_bold">Planning without Feedback</span>:
In this method, the agent does not receive feedback in the planning process. The plans are generated in a monolithic manner. The following are many representative planning strategies in this direction.</p>
</div>
<div id="S2.SS1.SSS3.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS3.p3.1" class="ltx_p"><math id="S2.SS1.SSS3.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS3.p3.1.m1.1a"><mo id="S2.SS1.SSS3.p3.1.m1.1.1" xref="S2.SS1.SSS3.p3.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p3.1.m1.1b"><ci id="S2.SS1.SSS3.p3.1.m1.1.1.cmml" xref="S2.SS1.SSS3.p3.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p3.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS3.p3.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS3.p3.1.1" class="ltx_text ltx_font_italic">Subgoal Decomposition</span>. Some researchers intend to let LLMs think step by step to solve complex tasks. Chain of Thought (CoT)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib138" title="" class="ltx_ref">138</a>]</cite> has become a standard technique for allowing large models to solve complex tasks. It proposes a simple but effective prompting method, which takes the process of solving complex reasoning problems step by step with a small number of language examples in the prompt.
Zero-shot-CoTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite> allows LLMs to autonomously generate reasoning processes for complex problems by prompting the model to "think step by step", and experimentally proves that LLMs are decent zero-shot reasoners.
InÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>, the LLMs act as Zero-Shot Planners to make goal-driven decisions in an interactive simulation environment.
Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> further uses environmental objects and object relationships as additional inputs for LLMs action plan generation, providing the system with a sense of its surroundings to generate plans. ReWOOÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib147" title="" class="ltx_ref">147</a>]</cite> introduces a paradigm of separating planning from external observations, enabling the LLM to act as a planner that directly generates a series of independent plans without requiring external feedback. In summary, by decomposing complex tasks into executable sub-tasks, the ability of the large language model to make plans and decisions is significantly improved.</p>
</div>
<div id="S2.SS1.SSS3.p4" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS3.p4.1" class="ltx_p"><math id="S2.SS1.SSS3.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS3.p4.1.m1.1a"><mo id="S2.SS1.SSS3.p4.1.m1.1.1" xref="S2.SS1.SSS3.p4.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p4.1.m1.1b"><ci id="S2.SS1.SSS3.p4.1.m1.1.1.cmml" xref="S2.SS1.SSS3.p4.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p4.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS3.p4.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS3.p4.1.1" class="ltx_text ltx_font_italic">Multi-path Thought</span>. Based on CoT, some researchers suggest that the process of human thinking and reasoning is a tree-like structure with multiple paths to the final result. Self-consistent CoT (CoT-SC)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib135" title="" class="ltx_ref">135</a>]</cite> assumes that each complex problem has multiple ways of thinking to deduce the final answer. In specific, CoT is utilized to generate several paths and answers of reasoning, where the answer with the most occurrences will be selected as the final answer output. Tree of Thoughts(ToT)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib150" title="" class="ltx_ref">150</a>]</cite> assumes that humans tend to think in a tree-like way, when making decisions on complex problems for planning purposes, where each tree node is a thinking state. It uses LLM to generate evaluations or votes of thoughts, which can be searched using BFS or DFS. These methods improve the performance of LLMs on complex reasoning tasks. Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib153" title="" class="ltx_ref">153</a>]</cite> discusses the constrained language planning problem. It generates extra scripts and filtering them to improve the quality of script generation. Among the several generated scripts, the script selection is determined by (1) the cosine similarity between the script and the goal, (2) whether the script contains the goal constraint keywords.
DEPSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib137" title="" class="ltx_ref">137</a>]</cite> uses vision-language models as selectors to choose the optimal path in optional subtasks. SayCanÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> combines the probability from the language model (the probability that an action will be useful for the high-level instruction) with the probability from the value function (the probability of successfully executing the said action) and selects the action to take. Then, it appends to the robot response and queries the model again to repeat the process until the output step terminates. In conclusion, multi-path thought further empowers the agent to solve more complex planning tasks, but it also brings additional computational burden.</p>
</div>
<div id="S2.SS1.SSS3.p5" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS3.p5.1" class="ltx_p"><math id="S2.SS1.SSS3.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS3.p5.1.m1.1a"><mo id="S2.SS1.SSS3.p5.1.m1.1.1" xref="S2.SS1.SSS3.p5.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p5.1.m1.1b"><ci id="S2.SS1.SSS3.p5.1.m1.1.1.cmml" xref="S2.SS1.SSS3.p5.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p5.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS3.p5.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS3.p5.1.1" class="ltx_text ltx_font_italic">External Planner</span>. LLMs, even if having significant zero-sample planning power, are not as reliable as traditional planners in many cases, especially when faced with domain-specific long-term planning problems.
LLM+PÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite> transforms natural language descriptions into a formal Planning Domain Definition Language (PDDL). Then, results are computed using an external planner and transformed into natural language by the LLMs finally. Likewise, LLM-DPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> utilizes the LLM to convert observation, current world state, and target objectives into PDDL format. This information is then passed to an external symbolic planner, which efficiently determines the optimal sequence of actions from the current state to the target state. MRKLÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite> is a modular, neural-symbolic AI architecture, where LLMs process the input texts, routes them to each of the experts, and then pass them through the LLMsâ€™ outputs. CO-LLMÂ  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib156" title="" class="ltx_ref">156</a>]</cite> consider that LLMs are good at generating high-level plans, but not good at low-level controlling. They use a heuristically designed low-level planner to robustly implement base actions according to high-level plans. With expert planners in the sub-task domains, it is possible for LLM to navigate the planning of complex tasks in specific domains. LLM-based agentsâ€™ generalized knowledge is difficult to perform best on tasks in all domains, but combining it with the expert knowledge from external planners can be effective to improve performance.</p>
</div>
<div id="S2.SS1.SSS3.p6" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS3.p6.1" class="ltx_p"><span id="S2.SS1.SSS3.p6.1.1" class="ltx_text ltx_font_bold">Planning with Feedback</span>:
When humans deal with tasks, the experience of success or failure directs them to reflect on themselves and improves their planning ability.
The experiences are often obtained and accumulated based on external feedback.
To simulate such human capability, many researchers have designed planning modules, which can receive feedback from the environment, humans, and models, significantly improving the planning ability of the agents.</p>
</div>
<div id="S2.SS1.SSS3.p7" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS3.p7.1" class="ltx_p"><math id="S2.SS1.SSS3.p7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS3.p7.1.m1.1a"><mo id="S2.SS1.SSS3.p7.1.m1.1.1" xref="S2.SS1.SSS3.p7.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p7.1.m1.1b"><ci id="S2.SS1.SSS3.p7.1.m1.1.1.cmml" xref="S2.SS1.SSS3.p7.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p7.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS3.p7.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS3.p7.1.1" class="ltx_text ltx_font_italic">Environmental Feedback</span>.
In many studies, the agents make plans based on environmental feedback.
For example, ReActÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib151" title="" class="ltx_ref">151</a>]</cite> extends the agentâ€™s action space to a collection of actions and language space. Explicit reasoning and actions are performed sequentially, and when the feedback from an action does not have the correct answer, reasoning will be performed again until obtaining the correct answer.
VoyagerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite> self-refines the agent generation scripts by acting on three types of feedback until it passes self-validation and is deposited in the skills library.
GhostÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite>, DEPSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib137" title="" class="ltx_ref">137</a>]</cite> can receive feedback from the environment, including information about the current state of the agent in the environment, and the information about the success or failure of each action performed. By integrating this feedback, the agents can update their understanding of the environment, improve their strategy and adapt their behaviors.
Based on Zero-Shot PlannersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>, Re-promptingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite> uses the pre-condition errors information to detect whether the agent is capable of completing the current planning. It also uses the precondition information to re-prompt the LLM to complete the closed-loop control.
Inner MonologueÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> appends three types of environmental feedback: successful execution of sub-tasks, passive scene description, and active scene description to instruction, thus enabling closed-loop planning for LLM-based agents.
Introspective TipsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> allows LLM to introspect through the history of the environmental feedback.
LLM-PlannerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib127" title="" class="ltx_ref">127</a>]</cite> introduces a grounded re-planning algorithm that dynamically updates plans generated by LLMs when encountering object mismatches and unattainable plans during task completion.
In ProgpromptÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite>, assertions are incorporated into the generated script to provide environment state feedback, allowing for error recovery in case the actionâ€™s preconditions are not satisfied.
In summary, environmental feedback serves as a direct indicator of planning success or failure, thereby enhancing the efficiency of closed-loop planning.</p>
</div>
<div id="S2.SS1.SSS3.p8" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS3.p8.1" class="ltx_p"><math id="S2.SS1.SSS3.p8.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS3.p8.1.m1.1a"><mo id="S2.SS1.SSS3.p8.1.m1.1.1" xref="S2.SS1.SSS3.p8.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p8.1.m1.1b"><ci id="S2.SS1.SSS3.p8.1.m1.1.1.cmml" xref="S2.SS1.SSS3.p8.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p8.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS3.p8.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS3.p8.1.1" class="ltx_text ltx_font_italic">Human Feedback</span>. The agent can make plans with the help of real human feedback. Such signal can help the agent to better align with practical settings, and may also alleviate the hallucination problem.
In VoyagerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite>, it is mentioned that human can act as a critic to ask Voyager to change the previous round of code through multi-model feedback. OpenAGIÂ  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> proposes a reinforcement learning with task feedback (RLTF) mechanism that utilizes manual or benchmark evaluation to improve the capabilities of the LLM-based agent.</p>
</div>
<div id="S2.SS1.SSS3.p9" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS3.p9.1" class="ltx_p"><math id="S2.SS1.SSS3.p9.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS3.p9.1.m1.1a"><mo id="S2.SS1.SSS3.p9.1.m1.1.1" xref="S2.SS1.SSS3.p9.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p9.1.m1.1b"><ci id="S2.SS1.SSS3.p9.1.m1.1.1.cmml" xref="S2.SS1.SSS3.p9.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p9.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS3.p9.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS3.p9.1.1" class="ltx_text ltx_font_italic">Model Feedback</span>. Language models can be used as critics to criticise and improve the generated plans. Self-RefineÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite> introduces the self-refine mechanism to improve the output of LLMs by iterating feedback and improvement. Specifically, the LLM is utilized as a generator, feedback provider, and refiner. First, the generator is used to generate an initial output, then the feedback provider presents specific and actionable feedback for the output, and finally, the refiner is used to improve the output using the feedback. The reasoning power of LLM is improved by an iterative feedback loop between the generator and the critic. ReflexionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite> is a framework for enhancing agent through verbally feedback, which introduces memory mechanism. The actor first generates the action, then the evaluator generates the evaluation, and finally a summary of the past experience is generated by a self-reflective model. The summary will be stored in memory to further improving the generation of the actor through the past experience.
World model typically refers to an agentâ€™s internal representation of the environment, which is utilized for internal simulation and abstraction of the environment. It aids agents in reasoning, planning, and predicting the effects of different actions on the environment. RAPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> involves utilizing LLMs both as the world model and as the agent. During the reasoning process, the agent constructs a reasoning tree, while the world model provides rewards as feedback. The agent performs MCTS (Monte Carlo Tree Search) on the reasoning tree to obtain the optimal plan. Similarly, REXÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite> introduces an accelerated MCTS approach where rewards feedback are furnished by either the environment or the LLM.
TipsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> can learn from demonstrations of other expert models.
In the MAD(Multi-Agent Debate)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite> framework, multiple agents express their arguments in an "eye-for-an-eye" fashion, and a judge manages the debate process to reach a final solution. The MAD framework encourages divergent thinking in LLM, which facilitates tasks requiring deeper thinking.
</p>
</div>
<div id="S2.SS1.SSS3.p10" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS3.p10.1" class="ltx_p">In summary, the planning module is important for the agents to solve complex tasks. While external feedback can always help to make informed plans, it does not always exist.
Planning with and without feedback are both important for building LLM-based agents.</p>
</div>
</section>
<section id="S2.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.4 </span>Action Module</h4>

<div id="S2.SS1.SSS4.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS4.p1.1" class="ltx_p">The action module aims to translate the agentâ€™s decisions into specific outcomes.
It directly interacts with the environment, determining the agentâ€™s effectiveness in completing tasks.
This section offers an overview of the action module, primarily examining the action target, strategy, space, and influence.</p>
</div>
<div id="S2.SS1.SSS4.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS4.p2.1" class="ltx_p"><span id="S2.SS1.SSS4.p2.1.1" class="ltx_text ltx_font_bold">Action Target</span>:
Action target means the goal of the action, which is usually specified by the real-humans or the agent itself.
The three main action targets include task completion, dialogue interaction, and environment exploration and interaction.</p>
</div>
<div id="S2.SS1.SSS4.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS4.p3.1" class="ltx_p"><math id="S2.SS1.SSS4.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS4.p3.1.m1.1a"><mo id="S2.SS1.SSS4.p3.1.m1.1.1" xref="S2.SS1.SSS4.p3.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS4.p3.1.m1.1b"><ci id="S2.SS1.SSS4.p3.1.m1.1.1.cmml" xref="S2.SS1.SSS4.p3.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS4.p3.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS4.p3.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS4.p3.1.1" class="ltx_text ltx_font_italic">Task Completion</span>.
A fundamental goal of the action module is to complete a specific task in a logical manner.
The type of tasks varies in different scenarios, leading to the necessary design of the action module.
For example, VoyagerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite> utilizes LLMs as action module to guide agents in exploring and collecting resources to complete sophisticated tasks in Minecraft.
GITMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite> decomposes an overall task into executable actions, enabling the agents to complete the routine activities step by step.
Generative AgentsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite> similarly conducts executable action sequences by hierarchically decomposing high-level task planning.</p>
</div>
<div id="S2.SS1.SSS4.p4" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS4.p4.1" class="ltx_p"><math id="S2.SS1.SSS4.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS4.p4.1.m1.1a"><mo id="S2.SS1.SSS4.p4.1.m1.1.1" xref="S2.SS1.SSS4.p4.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS4.p4.1.m1.1b"><ci id="S2.SS1.SSS4.p4.1.m1.1.1.cmml" xref="S2.SS1.SSS4.p4.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS4.p4.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS4.p4.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS4.p4.1.1" class="ltx_text ltx_font_italic">Dialogue Interaction</span>.
The ability of LLM-based autonomous agents to conduct a natural language dialogue with humans is essential since human users usually need to obtain the agent status or complete collaborative tasks with agents.
Previous work has improved the dialogue interaction ability of agents in diverse domains.
For example, ChatDevÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite> conducts related dialogues among employees of a software development company.
DERAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite> enhances the dialogue interaction in an iterative manner.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib139" title="" class="ltx_ref">139</a>]</cite> utilizes interaction dialogues between different agents, and therefore encourages them have similar opinions about certain topic.</p>
</div>
<div id="S2.SS1.SSS4.p5" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS4.p5.1" class="ltx_p"><math id="S2.SS1.SSS4.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS4.p5.1.m1.1a"><mo id="S2.SS1.SSS4.p5.1.m1.1.1" xref="S2.SS1.SSS4.p5.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS4.p5.1.m1.1b"><ci id="S2.SS1.SSS4.p5.1.m1.1.1.cmml" xref="S2.SS1.SSS4.p5.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS4.p5.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS4.p5.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS4.p5.1.1" class="ltx_text ltx_font_italic">Environment Exploration and Interaction</span>. Agents are able to acquire new knowledge through interacting with the environment and enhance themselves by summarizing recent experiences.
In this way, the agent can generate novel behaviors which are increasingly attuned to the environment and aligned with common sense.
For example, VoyagerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite> conducts continual learning by allowing the agent to explore in the open-ended environment.
Memory-enhanced reinforcement learning (MERL) framework in SayCanÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> continuously accumulates the textual knowledge and then adjusts the agent action scheme based on external feedback.
Similarly, GITMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite> allows agents to continually collect the textual knowledge and therefore adapt their behaviors based on the environment feedback.</p>
</div>
<div id="S2.SS1.SSS4.p6" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS4.p6.1" class="ltx_p"><span id="S2.SS1.SSS4.p6.1.1" class="ltx_text ltx_font_bold">Action Strategy</span>:
Action strategy means the methods that the agent produces the actions. In existing work, these strategies may be memory recollection, multi-round interaction, feedback adjustment and incorporating external tools.
In the following, we detail these strategies one by one.</p>
</div>
<div id="S2.SS1.SSS4.p7" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS4.p7.1" class="ltx_p"><math id="S2.SS1.SSS4.p7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS4.p7.1.m1.1a"><mo id="S2.SS1.SSS4.p7.1.m1.1.1" xref="S2.SS1.SSS4.p7.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS4.p7.1.m1.1b"><ci id="S2.SS1.SSS4.p7.1.m1.1.1.cmml" xref="S2.SS1.SSS4.p7.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS4.p7.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS4.p7.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS4.p7.1.1" class="ltx_text ltx_font_italic">Memory Recollection</span>.
Memory recollection techniques facilitate agents in making informed decisions based on stored experiences in memory modules <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite>.
Generative AgentsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite> maintain a memory stream of dialogues and experiences.
When taking actions, relevant memory snippets are retrieved as conditional inputs for the LLMs to ensure consistent actions. GITMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite> uses memories to guide actions, like moving towards previously discovered locations.
CAMELÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> constructs a memory stream of historical experiences, enabling the LLMs to generate informed actions based on these memories.</p>
</div>
<div id="S2.SS1.SSS4.p8" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS4.p8.1" class="ltx_p"><math id="S2.SS1.SSS4.p8.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS4.p8.1.m1.1a"><mo id="S2.SS1.SSS4.p8.1.m1.1.1" xref="S2.SS1.SSS4.p8.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS4.p8.1.m1.1b"><ci id="S2.SS1.SSS4.p8.1.m1.1.1.cmml" xref="S2.SS1.SSS4.p8.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS4.p8.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS4.p8.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS4.p8.1.1" class="ltx_text ltx_font_italic">Multi-round Interaction</span>.
This method seeks to leverage dialogue context across multiple rounds for agents to determine appropriate responses as actionsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>, <a href="#bib.bib104" title="" class="ltx_ref">104</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.
ChatDevÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite> encourages the agents to act based on their dialogue histories with others.
DERAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite> proposes a novel dialogue agent, where during the communication process, the researcher agent can provide useful feedback to guide the action of the decider agent.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> constructs a multi-agent debate (MAD) system, where each LLM-based agent engages in iterative rounds of interaction, exchanging challenges and insights, with the ultimate aim of achieving consensus.
ChatCotÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> employs a multi-round dialogue framework to model the process of chain-of-thought reasoning, seamlessly integrating reasoning and tool usage through conversational interactions.</p>
</div>
<div id="S2.SS1.SSS4.p9" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS4.p9.1" class="ltx_p"><math id="S2.SS1.SSS4.p9.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS4.p9.1.m1.1a"><mo id="S2.SS1.SSS4.p9.1.m1.1.1" xref="S2.SS1.SSS4.p9.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS4.p9.1.m1.1b"><ci id="S2.SS1.SSS4.p9.1.m1.1.1.cmml" xref="S2.SS1.SSS4.p9.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS4.p9.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS4.p9.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS4.p9.1.1" class="ltx_text ltx_font_italic">Feedback Adjustment</span>.
The effectiveness of human feedback or engagement with the external environment has been demonstrated in facilitating agents to adapt and enhance their action strategiesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>, <a href="#bib.bib99" title="" class="ltx_ref">99</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
For instance, VoyagerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite> enables agents to improve their policies after experiencing action failures or validate successful strategies using feedback mechanisms.
The Interactive Construction Learning Agent (ICLA)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite> utilizes user feedback on initial actions to iteratively enhance plans, leading to the development of more precise strategies.
SayCanÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> employs a reinforcement learning framework where the agent continuously adjusts actions based solely on environment feedback, enabling automated trial-and-error based enhancement.</p>
</div>
<div id="S2.SS1.SSS4.p10" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS4.p10.1" class="ltx_p"><math id="S2.SS1.SSS4.p10.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS4.p10.1.m1.1a"><mo id="S2.SS1.SSS4.p10.1.m1.1.1" xref="S2.SS1.SSS4.p10.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS4.p10.1.m1.1b"><ci id="S2.SS1.SSS4.p10.1.m1.1.1.cmml" xref="S2.SS1.SSS4.p10.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS4.p10.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS4.p10.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS4.p10.1.1" class="ltx_text ltx_font_italic">Incorporating External Tools</span>.
LLM-based autonomous agents can be enhanced through the incorporation of external tools and the expansion of knowledge sources.
On the one hand, the agents can be equipped with the ability to access and employ a variety of APIs, databases, web applications, and other external resources during the training or inference stage.
For example, ToolformerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite> is trained to determine the appropriate APIs to call upon, the timing for these calls, and the optimal method of integrating the returned results into future token prediction.
ChemCrowÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> designs a chemistry-oriented LLM-based agent, which incorporates seventeen expert-designed tools, to perform tasks including organic synthesis, drug discovery, and materials design.
ViperGPTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite> presents a code-generation framework, which assembles vision-and-language models into subroutines that is able to return results for any given query.
HuggingGPTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite> employs LLMs to connect diverse AI models in machine learning communities (<em id="S2.SS1.SSS4.p10.1.2" class="ltx_emph ltx_font_italic">e.g.</em>, Hugging Face) to resolve AI tasks. In specific, HuggingGPT proposes a meta-learning approach to train LLMs to generate code snippets, and then use these snippets to call upon the desired AI models from the external community hub.
One the other hand, the scope and quality of knowledge that agents directly access can be broadened with the help of external knowledge sources.
In previous work, the external knowledge sources contains databases, knowledge graphs, web pages, and so on.
For example, GorillaÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite> is able to effectively provide appropriate API calls, since it is trained on three extra machine learning hub datasets: Torch Hub, TensorFlow Hub, and HuggingFace.
WebGPTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite> proposes an extension that can incorporate the relevant results retrieved from websites into prompts when using ChatGPT, thus leading to more accurate and timely conversations.
ChatDBÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> is an AI database assistant that utilizes SQL statements generated by the LLM controller to operate the external database accurately.
GITMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite> uses LLMs to generate explainable results of text mining tasks, which employs a novel text mining pipeline that integrates LLMs, knowledge extraction and topic modeling modules.</p>
</div>
<div id="S2.SS1.SSS4.p11" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS4.p11.1" class="ltx_p"><span id="S2.SS1.SSS4.p11.1.1" class="ltx_text ltx_font_bold">Action Space</span>: The action space of LLM-based agents refers to the set of possible actions that can be performed by the agent. This stems from two main sources: external tools that expand the action capabilities, and the agentâ€™s own knowledge and skills such as language generation and memory-based decision making. Specifically, external tools include search engines, knowledge bases, computing tools, other language models, and visual models. By interfacing with these tools, agents can execute diverse realistic actions like information retrieval, data querying, mathematical computations, sophisticated language production, and image analysis. The agentâ€™s self-acquired knowledge based on the language model can empower the agent to plan, generate language, and make decisions, further expanding its action potential.</p>
</div>
<div id="S2.SS1.SSS4.p12" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS4.p12.1" class="ltx_p"><math id="S2.SS1.SSS4.p12.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS4.p12.1.m1.1a"><mo id="S2.SS1.SSS4.p12.1.m1.1.1" xref="S2.SS1.SSS4.p12.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS4.p12.1.m1.1b"><ci id="S2.SS1.SSS4.p12.1.m1.1.1.cmml" xref="S2.SS1.SSS4.p12.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS4.p12.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS4.p12.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS4.p12.1.1" class="ltx_text ltx_font_italic">Tools</span>.
Various external tools or knowledge sources provide much richer action capabilities for agents, including APIs, knowledge bases, visual models, language models, and so on.
(1) <span id="S2.SS1.SSS4.p12.1.2" class="ltx_text ltx_font_italic">APIs</span>.
Leveraging external APIs to complement and expand action space is a popular paradigm in recent years.
For example,
HuggingGPTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite> uses search engines, transforming queries into search request to fetch relevant code.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>, <a href="#bib.bib118" title="" class="ltx_ref">118</a>]</cite> propose to automatically generate queries to extract relevant content from external web pages when responding to user request.
TPTUÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib118" title="" class="ltx_ref">118</a>]</cite> interfaces with both Python interpreters and LaTeX compilers to execute sophisticated computations such as square roots, factorials and matrix operations.
Another type of APIs is the ones that can be directly invoked by LLMs based on natural language or code inputs. For instance,
ToolFormerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite> is an LLM-based tool transformation system that can automatically convert a given tool into another one with different functionalities or formats based on natural language instructions.
API-BankÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> is an LLM-based API recommendation agent that can automatically search and generate appropriate API calls for various programming languages and domains. API-Bank also provides an interactive interface for users to easily modify and execute the generated API calls.
Similarly, ToolBenchÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref">115</a>]</cite> is an LLM-based tool generation system that can automatically design and implement various practical tools based on natural language requirements. The tools generated by ToolBench include calculators, unit converters, calendars, maps, charts, etc.
All these agents utilize external APIs as their external tools, and provide interactive interfaces for users to easily modify and execute the generated or transformed tools.
(2) <span id="S2.SS1.SSS4.p12.1.3" class="ltx_text ltx_font_italic">Knowledge Bases</span>.
Connecting to external knowledge bases can help the agents to obtain specific domain information for generating more realistic actions.
For example, ChatDBÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> employs SQL statements to query databases, facilitating actions by the agents in a logical manner.
ChemCrowÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> presents an LLM-based chemical agent aimed at accomplishing tasks in the fields of organic synthesis, drug discovery, and material design, with the help of seventeen expert-designed tools.
MRKL SystemsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>, OpenAGIÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> incorporates various expert systems such as knowledge bases and planners, invoking them to access domain-specific information in a systematic manner.
(3) <span id="S2.SS1.SSS4.p12.1.4" class="ltx_text ltx_font_italic">Language Models</span>.
Language models can also act as tools to enrich the action spaces.
For example, MemoryBankÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib158" title="" class="ltx_ref">158</a>]</cite> employs two language models, one aims to encode input text while the other is responsible to match the arrived query statements to provide auxiliary textual retrieval.
ViperGPTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite> firstly uses Codex, which is based on language model, to generate Python code from text descriptions, and then executes the code to complete the given tasks.
TPTUÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib118" title="" class="ltx_ref">118</a>]</cite> incorporates various LLMs to accomplish a wide range of language generation tasks such as generating code, producing lyrics, and more.
(4) <span id="S2.SS1.SSS4.p12.1.5" class="ltx_text ltx_font_italic">Visual Models</span>. Integrating visual models with agents can broaden the action space into the multi-modal domain.
ViperGPTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite> leverages models like GLIP to extract image features for visual content-related actions.
HuggingGPTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite> proposes to use visual models for image processing and generation.</p>
</div>
<div id="S2.SS1.SSS4.p13" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS4.p13.1" class="ltx_p"><math id="S2.SS1.SSS4.p13.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS4.p13.1.m1.1a"><mo id="S2.SS1.SSS4.p13.1.m1.1.1" xref="S2.SS1.SSS4.p13.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS4.p13.1.m1.1b"><ci id="S2.SS1.SSS4.p13.1.m1.1.1.cmml" xref="S2.SS1.SSS4.p13.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS4.p13.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS4.p13.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS4.p13.1.1" class="ltx_text ltx_font_italic">Agentâ€™s Self Knowledge</span>. An agentâ€™s self-acquired knowledge also affords diverse behaviors, such as leveraging LLMâ€™s generative powers for planning and language production, making decisions based on memories, etc. An agentâ€™s self-acquired knowledge such as memories, experiences, and language capabilities enable diverse tool-free actions.
For instance, Generative Agents <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite> maintains comprehensive memory logs of all past dialogues. When taking actions, it retrieves relevant memory snippets as conditional inputs to guide the LLM in autoregressively generating logical and consistent language plans.
GITM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite> constructs a memory base of experiences like discovered villages or collected resources. When acting, it queries the memory base for relevant entries, such as recalling a previous village direction to move towards that location again.
SayCan <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> develops a reinforcement learning framework where the agent repeatedly adjusts actions like motions purely based on environment feedback for automated trial-and-error improvements, without any human demonstrations or interventions.
Voyager <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite> leverages the LLMâ€™s broad language generation capabilities to synthesize free-form textual solutions like Python code snippets or conversational responses tailored to current needs.
Similarly, LATMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> empowers LLMs to utilize Python code for crafting its own reusable tools, fostering a flexible approach to problem-solving.
CAMEL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> records all historical experiences in a memory stream. The LLM then draws from relevant memories to autoregressively generate high-level textual plans outlining intended future courses of action.
ChatDev <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite> equips LLM agents with a dialogue history memory to determine appropriate conversational responses and actions based on context.
In summary, an agentâ€™s internal knowledge enables a diverse repertoire of tool-free actions via approaches like memory recollection, feedback adjustment, and open-ended language generation.</p>
</div>
<div id="S2.SS1.SSS4.p14" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS4.p14.1" class="ltx_p"><span id="S2.SS1.SSS4.p14.1.1" class="ltx_text ltx_font_bold">Action Influence</span>:
Action influence refers to the consequences of an action, which encompass changes in the environment, alterations in the internal states of the agent, triggering of new actions, and impacts on human perceptions. In the following, we elaborate on these consequences.</p>
</div>
<div id="S2.SS1.SSS4.p15" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS4.p15.1" class="ltx_p"><math id="S2.SS1.SSS4.p15.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS4.p15.1.m1.1a"><mo id="S2.SS1.SSS4.p15.1.m1.1.1" xref="S2.SS1.SSS4.p15.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS4.p15.1.m1.1b"><ci id="S2.SS1.SSS4.p15.1.m1.1.1.cmml" xref="S2.SS1.SSS4.p15.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS4.p15.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS4.p15.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS4.p15.1.1" class="ltx_text ltx_font_italic">Changing Environments.</span>
Actions can directly alter environment states, such as moving agent positions, collecting items, constructing buildings, etc.
For instance, GITM<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite> and Voyager <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite> change environment states by executing action sequences that complete tasks.
</p>
</div>
<div id="S2.SS1.SSS4.p16" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS4.p16.1" class="ltx_p"><math id="S2.SS1.SSS4.p16.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS4.p16.1.m1.1a"><mo id="S2.SS1.SSS4.p16.1.m1.1.1" xref="S2.SS1.SSS4.p16.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS4.p16.1.m1.1b"><ci id="S2.SS1.SSS4.p16.1.m1.1.1.cmml" xref="S2.SS1.SSS4.p16.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS4.p16.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS4.p16.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS4.p16.1.1" class="ltx_text ltx_font_italic">Altering Internal States.</span>
Actions taken by agent can also change the agent itself, including updating memories, forming new plans, acquiring novel knowledge, and more.
For example, in Generative AgentsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite>, memory streams are updated after performing actions within the system.
SayCanÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> enables agents to take actions to update understandings of the environment and thus adapt the subsequent behaviors.</p>
</div>
<div id="S2.SS1.SSS4.p17" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS4.p17.1" class="ltx_p"><math id="S2.SS1.SSS4.p17.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS4.p17.1.m1.1a"><mo id="S2.SS1.SSS4.p17.1.m1.1.1" xref="S2.SS1.SSS4.p17.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS4.p17.1.m1.1b"><ci id="S2.SS1.SSS4.p17.1.m1.1.1.cmml" xref="S2.SS1.SSS4.p17.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS4.p17.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS4.p17.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS4.p17.1.1" class="ltx_text ltx_font_italic">Triggering New Actions.</span>
For most LLM-based autonomous agents, actions are usually taken in a sequential manner, that is, the former action can trigger the next new action.
For example, VoyagerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite> seeks to construct buildings after collecting the environmental resources in Minecraft scenario.
Generative Agents<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite> firstly decomposes the plans into sub-goals, and then conducts a series of related actions to complete each sub-goal.</p>
</div>
<div id="S2.SS1.SSS4.p18" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS4.p18.1" class="ltx_p"><math id="S2.SS1.SSS4.p18.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.SSS4.p18.1.m1.1a"><mo id="S2.SS1.SSS4.p18.1.m1.1.1" xref="S2.SS1.SSS4.p18.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS4.p18.1.m1.1b"><ci id="S2.SS1.SSS4.p18.1.m1.1.1.cmml" xref="S2.SS1.SSS4.p18.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS4.p18.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS4.p18.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS1.SSS4.p18.1.1" class="ltx_text ltx_font_italic">Impacting Human Perceptions.</span>
The language, imagery and other modalities from actions directly influence user perceptions and experiences.
For example, CAMELÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> generates utterances that are coherent, informative, and engaging for conversational agents.
ViperGPTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite> produces visuals that are realistic, diverse, and relevant for image generation tasks. HuggingGPTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite> can generate visual outputs, such as images, to extend human perceptions into the realm of visual experiences. Moreover, HuggingGPT can also generate multimodal outputs, such as code, music, and video, to enrich human interactions with different media forms.
</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>
Summary of the construction strategies of representative agents (more agents can be seen on https://github.com/Paitesanshi/LLM-Agent-Survey).
For the profile module, we focus on the profile generation strategies, and use â‘ , â‘¡ and â‘¢ to represent the handcrafting method, LLM-generation method, and dataset alignment method, respectively.
For the memory module, we focus on the implementation strategies for memory operation and memory structure.
For memory operation, we use â‘  and â‘¡ to indicate that the model only has read/write operations and has read/write/reflection operations, respectively.
For memory structure, we use â‘  and â‘¡ to represent unified and hybrid memories, respectively.
For the planning module, we use â‘  and â‘¡ to represent planning w/o feedback and w/ feedback, respectively.
For the action module, we use â‘  and â‘¡ to represent that the model does not use tools and use tools, respectively.
Beyond the above agent design strategies, we also present the learning strategies (LS) of these agents. In specific, we use â‘ , â‘¡ and â‘¢ to represent learning from examples, human feedback and environment feedback, respectively.</figcaption>
<div id="S2.T1.1" class="ltx_inline-block ltx_transformed_outer" style="width:439.0pt;height:570.1pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-16.5pt,21.4pt) scale(0.93,0.93) ;">
<table id="S2.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.2.1" class="ltx_tr">
<th id="S2.T1.1.1.2.1.1" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;" rowspan="2"><span id="S2.T1.1.1.2.1.1.1" class="ltx_text ltx_align_center ltx_align_top">Model</span></th>
<th id="S2.T1.1.1.2.1.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;" rowspan="2"><span id="S2.T1.1.1.2.1.2.1" class="ltx_text ltx_align_center ltx_align_top">Profile</span></th>
<th id="S2.T1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.6pt;padding-bottom:0.6pt;" colspan="2">Memory</th>
<th id="S2.T1.1.1.2.1.4" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;" rowspan="2"><span id="S2.T1.1.1.2.1.4.1" class="ltx_text ltx_align_center ltx_align_top">Planning</span></th>
<th id="S2.T1.1.1.2.1.5" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;" rowspan="2"><span id="S2.T1.1.1.2.1.5.1" class="ltx_text ltx_align_center ltx_align_top">Action</span></th>
<th id="S2.T1.1.1.2.1.6" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;" rowspan="2"><span id="S2.T1.1.1.2.1.6.1" class="ltx_text ltx_align_center ltx_align_top">LS</span></th>
<th id="S2.T1.1.1.2.1.7" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;" rowspan="2"><span id="S2.T1.1.1.2.1.7.1" class="ltx_text ltx_align_center ltx_align_top">Time</span></th>
</tr>
<tr id="S2.T1.1.1.3.2" class="ltx_tr">
<th id="S2.T1.1.1.3.2.1" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.3.2.1.1" class="ltx_p ltx_align_top">Operation</p>
</th>
<th id="S2.T1.1.1.3.2.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.3.2.2.1" class="ltx_p ltx_align_top">Structure</p>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.4.1" class="ltx_tr">
<td id="S2.T1.1.1.4.1.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.4.1.1.1" class="ltx_p ltx_align_top">WebGPT<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite></p>
</td>
<td id="S2.T1.1.1.4.1.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.4.1.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.4.1.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.4.1.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.4.1.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.4.1.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.4.1.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.4.1.5.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.4.1.6" class="ltx_td ltx_align_justify ltx_border_t" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.4.1.6.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.4.1.7" class="ltx_td ltx_align_justify ltx_border_t" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.4.1.7.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.4.1.8" class="ltx_td ltx_align_justify ltx_border_t" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.4.1.8.1" class="ltx_p ltx_align_top">12/2021</p>
</td>
</tr>
<tr id="S2.T1.1.1.5.2" class="ltx_tr">
<td id="S2.T1.1.1.5.2.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.5.2.1.1" class="ltx_p ltx_align_top">SayCan<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite></p>
</td>
<td id="S2.T1.1.1.5.2.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.5.2.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.5.2.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.5.2.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.5.2.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.5.2.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.5.2.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.5.2.5.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.5.2.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.5.2.6.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.5.2.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.5.2.7.1" class="ltx_p ltx_align_top">â‘¢</p>
</td>
<td id="S2.T1.1.1.5.2.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.5.2.8.1" class="ltx_p ltx_align_top">04/2022</p>
</td>
</tr>
<tr id="S2.T1.1.1.6.3" class="ltx_tr">
<td id="S2.T1.1.1.6.3.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.6.3.1.1" class="ltx_p ltx_align_top">MRKL<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite></p>
</td>
<td id="S2.T1.1.1.6.3.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.6.3.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.6.3.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.6.3.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.6.3.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.6.3.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.6.3.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.6.3.5.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.6.3.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.6.3.6.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.6.3.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.6.3.7.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.6.3.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.6.3.8.1" class="ltx_p ltx_align_top">05/2022</p>
</td>
</tr>
<tr id="S2.T1.1.1.7.4" class="ltx_tr">
<td id="S2.T1.1.1.7.4.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.7.4.1.1" class="ltx_p ltx_align_top">Inner Monologue<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite></p>
</td>
<td id="S2.T1.1.1.7.4.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.7.4.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.7.4.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.7.4.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.7.4.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.7.4.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.7.4.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.7.4.5.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.7.4.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.7.4.6.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.7.4.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.7.4.7.1" class="ltx_p ltx_align_top">â‘¢</p>
</td>
<td id="S2.T1.1.1.7.4.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.7.4.8.1" class="ltx_p ltx_align_top">07/2022</p>
</td>
</tr>
<tr id="S2.T1.1.1.8.5" class="ltx_tr">
<td id="S2.T1.1.1.8.5.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.8.5.1.1" class="ltx_p ltx_align_top">Social Simulacra<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite></p>
</td>
<td id="S2.T1.1.1.8.5.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.8.5.2.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.8.5.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.8.5.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.8.5.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.8.5.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.8.5.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.8.5.5.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.8.5.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.8.5.6.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.8.5.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.8.5.7.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.8.5.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.8.5.8.1" class="ltx_p ltx_align_top">08/2022</p>
</td>
</tr>
<tr id="S2.T1.1.1.9.6" class="ltx_tr">
<td id="S2.T1.1.1.9.6.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.9.6.1.1" class="ltx_p ltx_align_top">ReAct<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib151" title="" class="ltx_ref">151</a>]</cite></p>
</td>
<td id="S2.T1.1.1.9.6.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.9.6.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.9.6.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.9.6.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.9.6.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.9.6.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.9.6.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.9.6.5.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.9.6.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.9.6.6.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.9.6.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.9.6.7.1" class="ltx_p ltx_align_top">â‘¢</p>
</td>
<td id="S2.T1.1.1.9.6.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.9.6.8.1" class="ltx_p ltx_align_top">10/2022</p>
</td>
</tr>
<tr id="S2.T1.1.1.10.7" class="ltx_tr">
<td id="S2.T1.1.1.10.7.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.10.7.1.1" class="ltx_p ltx_align_top">REPLUG<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite></p>
</td>
<td id="S2.T1.1.1.10.7.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.10.7.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.10.7.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.10.7.3.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.10.7.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.10.7.4.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.10.7.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.10.7.5.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.10.7.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.10.7.6.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.10.7.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.10.7.7.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.10.7.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.10.7.8.1" class="ltx_p ltx_align_top">01/2023</p>
</td>
</tr>
<tr id="S2.T1.1.1.11.8" class="ltx_tr">
<td id="S2.T1.1.1.11.8.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.11.8.1.1" class="ltx_p ltx_align_top">MALLM<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite></p>
</td>
<td id="S2.T1.1.1.11.8.2" class="ltx_td" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;"></td>
<td id="S2.T1.1.1.11.8.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.11.8.3.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.11.8.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.11.8.4.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.11.8.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.11.8.5.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.11.8.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.11.8.6.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.11.8.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.11.8.7.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.11.8.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.11.8.8.1" class="ltx_p ltx_align_top">01/2023</p>
</td>
</tr>
<tr id="S2.T1.1.1.12.9" class="ltx_tr">
<td id="S2.T1.1.1.12.9.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.12.9.1.1" class="ltx_p ltx_align_top">DEPS<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib137" title="" class="ltx_ref">137</a>]</cite></p>
</td>
<td id="S2.T1.1.1.12.9.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.12.9.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.12.9.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.12.9.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.12.9.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.12.9.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.12.9.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.12.9.5.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.12.9.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.12.9.6.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.12.9.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.12.9.7.1" class="ltx_p ltx_align_top">â‘¢</p>
</td>
<td id="S2.T1.1.1.12.9.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.12.9.8.1" class="ltx_p ltx_align_top">02/2023</p>
</td>
</tr>
<tr id="S2.T1.1.1.13.10" class="ltx_tr">
<td id="S2.T1.1.1.13.10.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.13.10.1.1" class="ltx_p ltx_align_top">Toolformer<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite></p>
</td>
<td id="S2.T1.1.1.13.10.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.13.10.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.13.10.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.13.10.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.13.10.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.13.10.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.13.10.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.13.10.5.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.13.10.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.13.10.6.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.13.10.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.13.10.7.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.13.10.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.13.10.8.1" class="ltx_p ltx_align_top">02/2023</p>
</td>
</tr>
<tr id="S2.T1.1.1.14.11" class="ltx_tr">
<td id="S2.T1.1.1.14.11.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.14.11.1.1" class="ltx_p ltx_align_top">Reflexion<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite></p>
</td>
<td id="S2.T1.1.1.14.11.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.14.11.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.14.11.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.14.11.3.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.14.11.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.14.11.4.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.14.11.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.14.11.5.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.14.11.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.14.11.6.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.14.11.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.14.11.7.1" class="ltx_p ltx_align_top">â‘¢</p>
</td>
<td id="S2.T1.1.1.14.11.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.14.11.8.1" class="ltx_p ltx_align_top">03/2023</p>
</td>
</tr>
<tr id="S2.T1.1.1.15.12" class="ltx_tr">
<td id="S2.T1.1.1.15.12.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.15.12.1.1" class="ltx_p ltx_align_top">CAMEL<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite></p>
</td>
<td id="S2.T1.1.1.15.12.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.15.12.2.1" class="ltx_p ltx_align_top">â‘  â‘¡</p>
</td>
<td id="S2.T1.1.1.15.12.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.15.12.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.15.12.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.15.12.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.15.12.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.15.12.5.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.15.12.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.15.12.6.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.15.12.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.15.12.7.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.15.12.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.15.12.8.1" class="ltx_p ltx_align_top">03/2023</p>
</td>
</tr>
<tr id="S2.T1.1.1.16.13" class="ltx_tr">
<td id="S2.T1.1.1.16.13.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.16.13.1.1" class="ltx_p ltx_align_top">ViperGPT<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite></p>
</td>
<td id="S2.T1.1.1.16.13.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.16.13.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.16.13.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.16.13.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.16.13.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.16.13.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.16.13.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.16.13.5.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.16.13.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.16.13.6.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.16.13.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.16.13.7.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.16.13.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.16.13.8.1" class="ltx_p ltx_align_top">03/2023</p>
</td>
</tr>
<tr id="S2.T1.1.1.17.14" class="ltx_tr">
<td id="S2.T1.1.1.17.14.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.17.14.1.1" class="ltx_p ltx_align_top">HuggingGPT<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite></p>
</td>
<td id="S2.T1.1.1.17.14.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.17.14.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.17.14.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.17.14.3.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.17.14.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.17.14.4.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.17.14.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.17.14.5.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.17.14.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.17.14.6.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.17.14.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.17.14.7.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.17.14.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.17.14.8.1" class="ltx_p ltx_align_top">03/2023</p>
</td>
</tr>
<tr id="S2.T1.1.1.18.15" class="ltx_tr">
<td id="S2.T1.1.1.18.15.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.18.15.1.1" class="ltx_p ltx_align_top">Generative Agents<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite></p>
</td>
<td id="S2.T1.1.1.18.15.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.18.15.2.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.18.15.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.18.15.3.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.18.15.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.18.15.4.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.18.15.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.18.15.5.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.18.15.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.18.15.6.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.18.15.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.18.15.7.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.18.15.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.18.15.8.1" class="ltx_p ltx_align_top">04/2023</p>
</td>
</tr>
<tr id="S2.T1.1.1.19.16" class="ltx_tr">
<td id="S2.T1.1.1.19.16.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.19.16.1.1" class="ltx_p ltx_align_top">LLM+P<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite></p>
</td>
<td id="S2.T1.1.1.19.16.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.19.16.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.19.16.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.19.16.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.19.16.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.19.16.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.19.16.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.19.16.5.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.19.16.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.19.16.6.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.19.16.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.19.16.7.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.19.16.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.19.16.8.1" class="ltx_p ltx_align_top">04/2023</p>
</td>
</tr>
<tr id="S2.T1.1.1.20.17" class="ltx_tr">
<td id="S2.T1.1.1.20.17.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.20.17.1.1" class="ltx_p ltx_align_top">ChemCrow<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite></p>
</td>
<td id="S2.T1.1.1.20.17.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.20.17.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.20.17.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.20.17.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.20.17.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.20.17.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.20.17.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.20.17.5.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.20.17.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.20.17.6.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.20.17.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.20.17.7.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.20.17.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.20.17.8.1" class="ltx_p ltx_align_top">04/2023</p>
</td>
</tr>
<tr id="S2.T1.1.1.21.18" class="ltx_tr">
<td id="S2.T1.1.1.21.18.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.21.18.1.1" class="ltx_p ltx_align_top">API-Bank<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite></p>
</td>
<td id="S2.T1.1.1.21.18.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.21.18.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.21.18.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.21.18.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.21.18.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.21.18.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.21.18.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.21.18.5.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.21.18.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.21.18.6.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.21.18.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.21.18.7.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.21.18.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.21.18.8.1" class="ltx_p ltx_align_top">04/2023</p>
</td>
</tr>
<tr id="S2.T1.1.1.22.19" class="ltx_tr">
<td id="S2.T1.1.1.22.19.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.22.19.1.1" class="ltx_p ltx_align_top">OpenAGI<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite></p>
</td>
<td id="S2.T1.1.1.22.19.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.22.19.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.22.19.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.22.19.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.22.19.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.22.19.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.22.19.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.22.19.5.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.22.19.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.22.19.6.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.22.19.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.22.19.7.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.22.19.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.22.19.8.1" class="ltx_p ltx_align_top">04/2023</p>
</td>
</tr>
<tr id="S2.T1.1.1.23.20" class="ltx_tr">
<td id="S2.T1.1.1.23.20.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.23.20.1.1" class="ltx_p ltx_align_top">AutoGPT<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite></p>
</td>
<td id="S2.T1.1.1.23.20.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.23.20.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.23.20.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.23.20.3.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.23.20.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.23.20.4.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.23.20.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.23.20.5.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.23.20.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.23.20.6.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.23.20.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.23.20.7.1" class="ltx_p ltx_align_top">â‘¢</p>
</td>
<td id="S2.T1.1.1.23.20.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.23.20.8.1" class="ltx_p ltx_align_top">04/2023</p>
</td>
</tr>
<tr id="S2.T1.1.1.24.21" class="ltx_tr">
<td id="S2.T1.1.1.24.21.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.24.21.1.1" class="ltx_p ltx_align_top">SCM<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite></p>
</td>
<td id="S2.T1.1.1.24.21.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.24.21.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.24.21.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.24.21.3.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.24.21.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.24.21.4.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.24.21.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.24.21.5.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.24.21.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.24.21.6.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.24.21.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.24.21.7.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.24.21.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.24.21.8.1" class="ltx_p ltx_align_top">04/2023</p>
</td>
</tr>
<tr id="S2.T1.1.1.25.22" class="ltx_tr">
<td id="S2.T1.1.1.25.22.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.25.22.1.1" class="ltx_p ltx_align_top">Socially Alignment<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite></p>
</td>
<td id="S2.T1.1.1.25.22.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.25.22.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.25.22.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.25.22.3.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.25.22.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.25.22.4.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.25.22.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.25.22.5.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.25.22.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.25.22.6.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.25.22.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.25.22.7.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.25.22.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.25.22.8.1" class="ltx_p ltx_align_top">05/2023</p>
</td>
</tr>
<tr id="S2.T1.1.1.26.23" class="ltx_tr">
<td id="S2.T1.1.1.26.23.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.26.23.1.1" class="ltx_p ltx_align_top">GITM<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite></p>
</td>
<td id="S2.T1.1.1.26.23.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.26.23.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.26.23.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.26.23.3.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.26.23.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.26.23.4.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.26.23.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.26.23.5.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.26.23.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.26.23.6.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.26.23.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.26.23.7.1" class="ltx_p ltx_align_top">â‘¢</p>
</td>
<td id="S2.T1.1.1.26.23.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.26.23.8.1" class="ltx_p ltx_align_top">05/2023</p>
</td>
</tr>
<tr id="S2.T1.1.1.27.24" class="ltx_tr">
<td id="S2.T1.1.1.27.24.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.27.24.1.1" class="ltx_p ltx_align_top">Voyager<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite></p>
</td>
<td id="S2.T1.1.1.27.24.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.27.24.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.27.24.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.27.24.3.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.27.24.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.27.24.4.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.27.24.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.27.24.5.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.27.24.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.27.24.6.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.27.24.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.27.24.7.1" class="ltx_p ltx_align_top">â‘¢</p>
</td>
<td id="S2.T1.1.1.27.24.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.27.24.8.1" class="ltx_p ltx_align_top">05/2023</p>
</td>
</tr>
<tr id="S2.T1.1.1.28.25" class="ltx_tr">
<td id="S2.T1.1.1.28.25.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.28.25.1.1" class="ltx_p ltx_align_top">Introspective Tips<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite></p>
</td>
<td id="S2.T1.1.1.28.25.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.28.25.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.28.25.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.28.25.3.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.28.25.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.28.25.4.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.28.25.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.28.25.5.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.28.25.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.28.25.6.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.28.25.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.28.25.7.1" class="ltx_p ltx_align_top">â‘ â‘¢</p>
</td>
<td id="S2.T1.1.1.28.25.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.28.25.8.1" class="ltx_p ltx_align_top">05/2023</p>
</td>
</tr>
<tr id="S2.T1.1.1.29.26" class="ltx_tr">
<td id="S2.T1.1.1.29.26.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.29.26.1.1" class="ltx_p ltx_align_top">RET-LLM<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite></p>
</td>
<td id="S2.T1.1.1.29.26.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.29.26.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.29.26.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.29.26.3.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.29.26.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.29.26.4.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.29.26.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.29.26.5.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.29.26.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.29.26.6.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.29.26.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.29.26.7.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.29.26.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.29.26.8.1" class="ltx_p ltx_align_top">05/2023</p>
</td>
</tr>
<tr id="S2.T1.1.1.30.27" class="ltx_tr">
<td id="S2.T1.1.1.30.27.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.30.27.1.1" class="ltx_p ltx_align_top">ChatDB<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite></p>
</td>
<td id="S2.T1.1.1.30.27.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.30.27.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.30.27.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.30.27.3.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.30.27.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.30.27.4.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.30.27.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.30.27.5.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.30.27.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.30.27.6.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.30.27.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.30.27.7.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.30.27.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.30.27.8.1" class="ltx_p ltx_align_top">06/2023</p>
</td>
</tr>
<tr id="S2.T1.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.1.1.1.1" class="ltx_p ltx_align_top"><math id="S2.T1.1.1.1.1.1.1.m1.1" class="ltx_centering" alttext="S^{3}" display="inline"><semantics id="S2.T1.1.1.1.1.1.1.m1.1a"><msup id="S2.T1.1.1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S2.T1.1.1.1.1.1.1.m1.1.1.2" xref="S2.T1.1.1.1.1.1.1.m1.1.1.2.cmml">S</mi><mn id="S2.T1.1.1.1.1.1.1.m1.1.1.3" xref="S2.T1.1.1.1.1.1.1.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.1.1.m1.1b"><apply id="S2.T1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.T1.1.1.1.1.1.1.m1.1.1">superscript</csymbol><ci id="S2.T1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S2.T1.1.1.1.1.1.1.m1.1.1.2">ğ‘†</ci><cn type="integer" id="S2.T1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.T1.1.1.1.1.1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.1.1.m1.1c">S^{3}</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.1.1.1.m1.1d">italic_S start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math><cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite></p>
</td>
<td id="S2.T1.1.1.1.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.1.2.1" class="ltx_p ltx_align_top">â‘¢</p>
</td>
<td id="S2.T1.1.1.1.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.1.3.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.1.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.1.4.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.1.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.1.5.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.1.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.1.6.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.1.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.1.7.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.1.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.1.8.1" class="ltx_p ltx_align_top">07/2023</p>
</td>
</tr>
<tr id="S2.T1.1.1.31.28" class="ltx_tr">
<td id="S2.T1.1.1.31.28.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.31.28.1.1" class="ltx_p ltx_align_top">ChatDev<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite></p>
</td>
<td id="S2.T1.1.1.31.28.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.31.28.2.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.31.28.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.31.28.3.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.31.28.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.31.28.4.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.31.28.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.31.28.5.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.31.28.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.31.28.6.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.31.28.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.31.28.7.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.31.28.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.31.28.8.1" class="ltx_p ltx_align_top">07/2023</p>
</td>
</tr>
<tr id="S2.T1.1.1.32.29" class="ltx_tr">
<td id="S2.T1.1.1.32.29.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.32.29.1.1" class="ltx_p ltx_align_top">ToolBench<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref">115</a>]</cite></p>
</td>
<td id="S2.T1.1.1.32.29.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.32.29.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.32.29.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.32.29.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.32.29.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.32.29.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.32.29.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.32.29.5.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.32.29.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.32.29.6.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.32.29.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.32.29.7.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.32.29.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.32.29.8.1" class="ltx_p ltx_align_top">07/2023</p>
</td>
</tr>
<tr id="S2.T1.1.1.33.30" class="ltx_tr">
<td id="S2.T1.1.1.33.30.1" class="ltx_td ltx_align_justify" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.33.30.1.1" class="ltx_p ltx_align_top">MemoryBank<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib158" title="" class="ltx_ref">158</a>]</cite></p>
</td>
<td id="S2.T1.1.1.33.30.2" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.33.30.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.33.30.3" class="ltx_td ltx_align_justify" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.33.30.3.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.33.30.4" class="ltx_td ltx_align_justify" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.33.30.4.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.33.30.5" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.33.30.5.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.33.30.6" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.33.30.6.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.33.30.7" class="ltx_td ltx_align_justify" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.33.30.7.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.33.30.8" class="ltx_td ltx_align_justify" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.33.30.8.1" class="ltx_p ltx_align_top">07/2023</p>
</td>
</tr>
<tr id="S2.T1.1.1.34.31" class="ltx_tr">
<td id="S2.T1.1.1.34.31.1" class="ltx_td ltx_align_justify ltx_border_b" style="width:96.7pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.34.31.1.1" class="ltx_p ltx_align_top">MetaGPT<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite></p>
</td>
<td id="S2.T1.1.1.34.31.2" class="ltx_td ltx_align_justify ltx_border_b" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.34.31.2.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S2.T1.1.1.34.31.3" class="ltx_td ltx_align_justify ltx_border_b" style="width:45.5pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.34.31.3.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.34.31.4" class="ltx_td ltx_align_justify ltx_border_b" style="width:37.0pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.34.31.4.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.34.31.5" class="ltx_td ltx_align_justify ltx_border_b" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.34.31.5.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.34.31.6" class="ltx_td ltx_align_justify ltx_border_b" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.34.31.6.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S2.T1.1.1.34.31.7" class="ltx_td ltx_align_justify ltx_border_b" style="width:22.8pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.34.31.7.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S2.T1.1.1.34.31.8" class="ltx_td ltx_align_justify ltx_border_b" style="width:34.1pt;padding-top:0.6pt;padding-bottom:0.6pt;">
<p id="S2.T1.1.1.34.31.8.1" class="ltx_p ltx_align_top">08/2023</p>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Learning Strategy</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">Learning stands as an essential mechanism for humans to attain both knowledge and skills, fostering the augmentation of their capabilitiesâ€”a significance extended profoundly to the realm of LLM-based agents. Through the process of learning, these agents are imbued with the capacity to demonstrate heightened mastery in adhering to instructions, deftly navigating intricate tasks, and seamlessly adapting to unprecedented and diverse environments. This transformative process empowers these agents to evolve beyond their initial programming, enabling them to perform tasks with greater finesse and flexibility. In this chapter, we will delve into various learning strategies employed by LLM-based agents and explore their far-reaching impacts.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">Learning from Example</span>: Learning from examples is a foundational process that underpins human and AI learning. In the realm of LLM-based agents, this principle is embodied in fine-tuning, where these agents refine their skills through exposure to real-world data.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.p3.1" class="ltx_p"><math id="S2.SS2.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS2.p3.1.m1.1a"><mo id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><ci id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_italic">Learning from Human-Annotations</span>. In the pursuit of harmony with human values, integrating human-generated feedback data becomes a cornerstone of fine-tuning LLMs. This practice is particularly crucial in shaping intelligent agents designed to complement or even replace human involvement in specific tasks. The CoH approach, proposed by Liu et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite>, involves a multi-step process where the LLM generates responses, assessed by human reviewers to differentiate favorable from unfavorable outcomes. This amalgamation of responses and evaluations contributes to the fine-tuning process, arming the LLM with a comprehensive understanding of errors and the ability to rectify them while staying aligned with human preferences. Despite the simplicity and directness of this approach, it is encumbered by substantial annotation costs and time, posing challenges in rapid adaptation to disparate scenarios. MIND2WEBÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> fine-tunes using human-annotated real-world website task data from diverse domains, resulting in a general agent that performs effectively across actual websites.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para ltx_noindent">
<p id="S2.SS2.p4.1" class="ltx_p"><math id="S2.SS2.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS2.p4.1.m1.1a"><mo id="S2.SS2.p4.1.m1.1.1" xref="S2.SS2.p4.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.1.m1.1b"><ci id="S2.SS2.p4.1.m1.1.1.cmml" xref="S2.SS2.p4.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p4.1.m1.1d">âˆ™</annotation></semantics></math> <span id="S2.SS2.p4.1.1" class="ltx_text ltx_font_italic">Learning from LLMsâ€™ Annotations</span>.
During pre-training, LLMs acquire a wealth of world knowledge from extensive training data. After fine-tuning and alignment with humans, they exhibit capabilities akin to human judgment, as exemplified by models like ChatGPT and GPT-4. Hence, we can utilize LLMs for annotation tasks, which can significantly reduce costs compared to human annotation, offering the potential for extensive data acquisition.
Liu et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite> proposed a stable alignment approach for fine-tuning LLMs based on social interaction. They devised a sandbox environment containing multiple agents, each responding to a probing question. These responses are then evaluated and scored by nearby agents and ChatGPT. Subsequently, the responding agent refines their answer based on these evaluations, which is then re-scored by ChatGPT. This iterative process yields a substantial corpus of interactive data, which is subsequently employed for fine-tuning LLMs using contrastive supervised learning. In RefinerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite>, the generator is asked to generate intermediate steps and a critic model is introduced to generate structured feedback. Then, the feedback records is used to fine-tune the generator model to improve inference ability.
In ToolFormerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite>, a pre-training corpus is marked with potential API calls using LLMs. Then, LLMs are fine-tuned on this annotated data to learn how and when to use APIs, and integrate API results into their text generation. Similarly, ToolBenchÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref">115</a>]</cite> is also a dataset entirely generated using ChatGPT, designed for fine-tuning and enhancing LLMsâ€™ proficiency in utilizing tools. ToolBench comprises an extensive collection of API descriptions, accompanied by directives that outline tasks to be accomplished using specific APIs, along with the corresponding sequences of actions to fulfill these directives. The fine-tuning process using ToolBench results in a model termed ToolLLaMA, which demonstrates performance comparable to ChatGPT. Notably, ToolLLaMA exhibits robust generalization capabilities even when confronted with previously unseen APIs.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para ltx_noindent">
<p id="S2.SS2.p5.1" class="ltx_p"><span id="S2.SS2.p5.1.1" class="ltx_text ltx_font_bold">Learning from Environment Feedback</span>:
In many cases, intelligent agents need to proactively explore their surroundings and interact with the environment. Therefore, they require the ability to adapt to the environment and enhance their capabilities from environmental feedback. In the field of reinforcement learning, agents learn by continuously exploring the environment and adapting based on environmental feedbackÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib82" title="" class="ltx_ref">82</a>, <a href="#bib.bib98" title="" class="ltx_ref">98</a>, <a href="#bib.bib152" title="" class="ltx_ref">152</a>]</cite>. This principle also holds for intelligent agents based on LLMs. VoyagerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite> follows an iterative prompting method, where agents perform actions, gather environment feedback, and continuously iterate until newly acquired skills are validated and added to the skill repository through self-verification. Similarly, LMA3Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> autonomously sets goals and executes actions in interactive environments, with LLM scoring its performance as a reward function. By iterating this process, LMA3 independently learns a wide range of skills. Meanwhile, GITMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite> and Inner MonologueÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> integrate environmental feedback into the closed-loop process of planning based on large-scale language models. Furthermore, creating an environment that closely mirrors reality also contributes significantly to enhancing the agentâ€™s performance. WebShopÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib149" title="" class="ltx_ref">149</a>]</cite> has developed a simulated e-commerce environment where the agent can engage in activities such as searching and making purchases, receiving corresponding rewards and feedback in return. In Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib145" title="" class="ltx_ref">145</a>]</cite>, an embodiment simulator is utilized to enable agents to interact within a simulated real-world environment, facilitating physical engagements that lead to the acquisition of embodied experiences. Subsequently, these experiences are employed to fine-tune the model, thereby enhancing its performance in downstream tasks.</p>
</div>
<div id="S2.SS2.p6" class="ltx_para ltx_noindent">
<p id="S2.SS2.p6.1" class="ltx_p">Contrastive to learning from annotations, learning from environmental feedback distinctly encapsulates the autonomy and independence characteristic of LLM-based agents. This divergence exemplifies a profound interplay between environmental responsiveness and autonomous learning, fostering a nuanced understanding of agent behavior and adaptation.</p>
</div>
<div id="S2.SS2.p7" class="ltx_para ltx_noindent">
<p id="S2.SS2.p7.1" class="ltx_p"><span id="S2.SS2.p7.1.1" class="ltx_text ltx_font_bold">Learning from Interactive Human Feedback</span>:
Interactive human feedback provides the agent with the opportunities to adapt, evolve, and refine their behaviors under human guidance in a dynamic manner.
Compared to one-shot feedback, interactive feedback is more aligned with real-world scenarios.
As the agents are learned in a dynamic process, they do more than just process static data - they participate in a continual refinement of their understanding, adaptation, and alignment with humans.
For example, Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib156" title="" class="ltx_ref">156</a>]</cite> incorporates a communication module that enables collaborative task completion via chat-based interaction and feedback from humans.
As highlighted by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite>, interactive feedback fosters key aspects such as reliability, transparency, immediacy, task characteristics, and the evolution of trust over time when learning the agents.</p>
</div>
<div id="S2.SS2.p8" class="ltx_para ltx_noindent">
<p id="S2.SS2.p8.1" class="ltx_p">In the above sections, we summarize the previous work based on the agent construction strategies, where we focus on two aspects including the architecture design and parameter optimization. We present the correspondence between the previous work and our taxonomy in TableÂ <a href="#S2.T1" title="Table 1 â€£ 2.1.4 Action Module â€£ 2.1 Agent Architecture Design â€£ 2 LLM-based Autonomous Agent Construction â€£ A Survey on Large Language Model based Autonomous Agents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S2.F3" class="ltx_figure">
<p id="S2.F3.1" class="ltx_p ltx_align_center"><span id="S2.F3.1.1" class="ltx_text ltx_framed_rectangle" style="border-color: black;">
<img src="x3.png" id="S2.F3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="830" height="242" alt="Refer to caption">
</span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The applications (left) and evaluation strategies (right) of LLM-based agents.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>LLM-based Autonomous Agent Application</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">The application of LLM-based autonomous agents across various fields represents a paradigm shift in how we approach problem-solving, decision-making, and innovation. These agents, endowed with the capabilities of language comprehension, reasoning, and adaptation, are revolutionizing industries and disciplines by offering unprecedented insights, assistance, and solutions. In this section, we explore the transformative impact of LLM-based autonomous agents in three distinct domains: social science, natural science, and engineering (see the left part of FigureÂ <a href="#S2.F3" title="Figure 3 â€£ 2.2 Learning Strategy â€£ 2 LLM-based Autonomous Agent Construction â€£ A Survey on Large Language Model based Autonomous Agents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> for a global overview).
</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Social Science</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">Computational social science involves the development and application of computational methods to analyze complex human behavioral data, often at a large scale, including data from simulated scenariosÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>.
Recently, LLMs have shown impressive human-like capabilities, which hold promise for research in social computational scienceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>.
In the following, we present many representative domains that LLM-based agents have been applied to.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Psychology</span>: LLM-based agents can be used in Psychology for conducting psychological experimentsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib95" title="" class="ltx_ref">95</a>, <a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>. InÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, LLM-based agents are leveraged to simulate psychological experiments including the ultimatum game, garden path sentence, Milgram shock experiment, and capacity for group intelligence. In the first three experiments, the LLM-based agent can reproduce current psychological findings, while the last experiment reveals "hyperaccuracy distortions" in some language models (including ChatGPT and GPT-4), which may affect downstream applications. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, the authors employ LLM-based agents to simulate two prototypical repeated games in the field of game theory: the Prisonerâ€™s dilemma and the battle of the sexes. They find that LLM-based agents show a psychological tendency to prioritize self-interest over coordination. As for the application on mental health,Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite> discusses the advantages and disadvantages associated with the utilization of LLM-based agent to provide mental health support.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">Political Science and Economy</span>:
Recent studies have employed LLM-based agents in the fields of political science and economicsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>. These agents are utilized to analyze partisan impressions and explore how political actors modify agendas, among other applications.
Additionally, LLM-based agents can be utilized for ideology detection and predicting voting patternsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
Furthermore, recent research endeavors have focused on understanding the discourse structure and persuasive elements of political speech through the assistance of LLM-based agentsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>.
In the study conducted by Horton et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>, LLM-based agents are provided with specific traits such as talents, preferences, and personalities. This allows researchers to explore economic behavior in simulated scenarios and gain novel insights into the field of economics.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.1" class="ltx_p"><span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_bold">Social Simulation</span>: Conducting experiments with human societies is often expensive, anti-ethical, anti-moral, or even unachievable. In contrast, agent-based simulation enables researchers to construct hypothetical scenarios under specific rules to simulate a range of social phenomena, e.g., the propagation of harmful information. The researcher engages in both observes and intervenes within the system at both macro and micro levels, which enables them to study counterfactual eventsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>, <a href="#bib.bib81" title="" class="ltx_ref">81</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib109" title="" class="ltx_ref">109</a>, <a href="#bib.bib89" title="" class="ltx_ref">89</a>, <a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib140" title="" class="ltx_ref">140</a>]</cite>. This process allows decision makers to develop more rational rules or policies.
For example, Social SimulacraÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite> simulates an online social community and explores the potential of utilizing LLM-based agent simulations to aid decision-makers to improve community regulations. Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> investigates the behavioral characteristics of LLM-based agents in social networks and the potential impact on social networks. In addition, Generative AgentsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite> and AgentSims<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> construct towns comprising multiple agents. SocialAI SchoolÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> employs simulations to investigate fundamental social cognitive skills that manifest during the course of child development.
S<math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="{}^{3}" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><msup id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml"><mi id="S3.SS1.p4.1.m1.1.1a" xref="S3.SS1.p4.1.m1.1.1.cmml"></mi><mn id="S3.SS1.p4.1.m1.1.1.1" xref="S3.SS1.p4.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><cn type="integer" id="S3.SS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> focuses on Propagation of information, emotion and attitude, whileÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib140" title="" class="ltx_ref">140</a>]</cite> concentrates on the transmission of infectious diseases.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para ltx_noindent">
<p id="S3.SS1.p5.1" class="ltx_p"><span id="S3.SS1.p5.1.1" class="ltx_text ltx_font_bold">Jurisprudence</span>: LLM-based agent can serve as aids in legal decision-making processes, facilitating judges in rendering more informed judgementsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>. Blind JudgementÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> employs several language models to simulate the decision-making processes of multiple judges. It gathers diverse opinions and consolidates the outcomes through a voting mechanism. ChatLawÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> is a prominent Chinese legal fine-tuned LLM. To address the issue of model hallucination, ChatLaw incorporates a fusion of database search and keyword search techniques to enhance accuracy. Concurrently, a self-attention mechanism is employed to augment the LLMâ€™s capability in mitigating the impact of inaccuracies in the reference data.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para ltx_noindent">
<p id="S3.SS1.p6.1" class="ltx_p"><span id="S3.SS1.p6.1.1" class="ltx_text ltx_font_bold">Social Science Research Assistant </span>: Apart from conducting specialized research within distinct domains of social computing, LLM-based agent can play the role of research assistantsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>. They have the potential to aid researchers in tasks like generating article abstracts, extracting keywords and generating scripts.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>. Additionally, LLM-based agent can serve as a writing aids, and they even possess the capability to identify novel research inquiries for social scientistsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para ltx_noindent">
<p id="S3.SS1.p7.1" class="ltx_p">The development of LLM-based agents holds great promise for bringing new research methods to the field of computational social science research. However, the application of LLM-based agent on social computing still presents several challenges and limitations Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib163" title="" class="ltx_ref">163</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Two primary concerns are bias and toxicity, as LLMs are trained from real-world datasets, which makes them susceptible to inherent biases, discrimination content, and unfairness. When LLM is introduced, it may produce biased information, which is further taken to train LLM, leading to the amplification of bias. Causality and interpretability pose another challenge, particularly in the context of social science where robust causal relationships are often required. Probability-based LLMs tend to lack clear interpretability.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Natural Science</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">The application of LLM-based agents in the field of natural sciences is on the rise due to the rapid advancement of large language models.
These agents bring new opportunities for scientific research in the natural sciences.
In the following, we present many representative domains, where LLM-based agents can play important roles.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Documentation and Data Management</span>:
In the realm of natural scientific research, a substantial amount of literature and data often necessitates meticulous collection, organization, and extraction, entailing significant time and human resources. LLM-based agents exhibit robust natural language processing capabilities, enabling them to effectively access a wide array of tools to browse the Internet, documents, databases, and other information sources. This capacity empowers them to attain vast amounts of data, seamlessly integrate and manage it, and thereby provide valuable assistance in scientific researchÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
By utilizing APIs to access the Internet, agents inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> can efficiently query and retrieve real-time, relevant information, aiding in tasks such as question answering and experiment planning. ChatMOFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> leverages LLMs to extract key points from human-written textual descriptions and formulate a plan to invoke necessary toolkits in order to predict properties and structures of metal-organic frameworks.
The utilization of databases further enhances the agentâ€™s performance in specific domains, owing to the wealth of tailored data they contain. For instance, when accessing chemistry-related databases, ChemCrowÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> can verify the accuracy of compound representations or identify hazardous substances, thereby contributing to more accurate and informed scientific investigations.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Natural Science Experiment Assistant</span>:
LLM-based agents can operate autonomously, conducting experiments independently, as well as serve as a valuable tool to support scientists in their research projectsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
For example, Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> introduces an innovative agent system that leverages LLMs to automate the design, planning, and execution of scientific experiments. When provided with the experimental objectives as input, the system accesses the Internet and retrieves relevant documents to acquire the necessary information. Subsequently, it employs Python code to perform the essential calculations and ultimately executes the sequential steps of the experiment.
Additionally, the ChemCrowÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> incorporates 17 meticulously crafted tools specifically designed to aid researchers in chemical research. Upon receiving the input objectives, ChemCrow offers insightful recommendations for experimental procedures while carefully highlighting potential safety risks associated with the proposed experiments.
</p>
</div>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold">Natural Science Education</span>:
Benefiting from the natural language capabilities, LLMs facilitate seamless communication with humans through natural language interactions, making them exciting educational tools that offer real-time question-answering and knowledge disseminationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib129" title="" class="ltx_ref">129</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.
For example, Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> proposes agent systems that serve as valuable educational tools for students and researchers to learn experimental design, methodologies, and analysis.
They help foster critical thinking and problem-solving skills while encouraging a deeper comprehension of scientific principles. Math Agents <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite> are entities that use artificial intelligence techniques to explore, discover, solve and prove mathematical problems. Math Agents can also communicate with humans and help them understand and use mathematics.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> utilize the capabilities of CodeXÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> to achieve human-level automatic solving, explanation, and generation of university-level mathematical problems through few-shot learning. This achievement bears significant implications for higher education, offering advantages such as curriculum design and analysis tools, as well as automated content generation.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para ltx_noindent">
<p id="S3.SS2.p5.1" class="ltx_p">The utilization of LLM-based agents in supporting natural scientific research also entails certain risks and challenges. On one hand, LLMs themselves may be susceptible to illusions and other issues, occasionally providing erroneous answers, leading to incorrect conclusions, experimental failures, or even posing risks to human safety in hazardous experiments. Therefore, during experimentation, users must possess the necessary expertise and knowledge to exercise appropriate caution. On the other hand, LLM-based agents could potentially be exploited for malicious purposes, such as the development of chemical weapons, necessitating the implementation of security measures, such as human alignment, to ensure responsible and ethical use.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Engineering</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">LLM-based autonomous agents have shown great potential in assisting and enhancing engineering research and applications. In this section, we review and summarize the applications of LLM-based agents in several major engineering domains.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">Civil Engineering</span>:
In civil engineering, LLM-based agents can be used to design and optimize complex structures such as buildings, bridges, dams, roads, etc. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite> proposes an interactive framework where human architects and AI agents collaborate to construct structures in a 3D simulation environment. The interactive agent can understand natural language instructions, place blocks, detect confusion, seek clarification, and incorporate human feedback, showing the potential for human-AI collaboration in engineering design.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_bold">Computer Science &amp; Software Engineering</span>:
In the field of computer science and software engineering, LLM-based agents offer potential for automating coding, testing, debugging, and documentation generationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref">115</a>, <a href="#bib.bib113" title="" class="ltx_ref">113</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. ChatDevÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite> proposes an end-to-end framework, where multiple agent roles communicate and collaborate through natural language conversations to complete the software development life cycle. This framework demonstrates efficient and cost-effective generation of executable software systems.
ToolBenchÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref">115</a>]</cite> can be used for tasks such as code autocompletion and code recommendation. For example, ToolBench can automatically complete function names and variable names in code, as well as to recommend code snippets.
MetaGPTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> abstracts multiple roles, such as product managers, architects, project managers, and engineers, to internally supervise code generation and enhance the quality of the final output code. This enables low-cost software development. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> presents a self-collaboration framework for code generation using LLMs, exemplified by ChatGPT.
In this framework, multiple LLMs assume distinct "expert" roles for specific subtasks within a complex task. They collaborate and interact according to specified instructions, forming a virtual team that facilitates each otherâ€™s work. Ultimately, the virtual team collaboratively addresses code generation tasks without requiring human intervention.
GPT-EngineerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, SmolModelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> and DemoGPTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> are open-source projects that focus on automating code generation through prompts to complete development tasks.
LLMs can also be applied to code bug testing and correction. LLIFTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> utilizes LLMs to aid in static analysis for detecting code vulnerabilities, striking a balance between precision and scalability.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS3.p4.1" class="ltx_p"><span id="S3.SS3.p4.1.1" class="ltx_text ltx_font_bold">Aerospace Engineering</span>:
In aerospace engineering, early work explores using LLM-based agents to model physics, solve complex differential equations, and optimize design. Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite> shows promising results in solving problems related to aerodynamics, aircraft design, trajectory optimization, etc. With further development, LLM-based agents may innovatively design spacecraft, simulate fluid flows, perform structural analysis, and even control autonomous vehicles by generating executable code that integrates with engineering systems.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para ltx_noindent">
<p id="S3.SS3.p5.1" class="ltx_p"><span id="S3.SS3.p5.1.1" class="ltx_text ltx_font_bold">Industrial Automation</span>: In the field of industrial automation, LLM-based agents can be used to achieve intelligent planning and control of production processes. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib144" title="" class="ltx_ref">144</a>]</cite> proposes a novel framework that integrates large language models (LLMs) with digital twin systems to accommodate flexible production needs. The framework leverages prompt engineering techniques to create LLM agents that can adapt to specific tasks based on the information provided by digital twins. These agents can coordinate a series of atomic functionalities and skills to complete production tasks at different levels within the automation pyramid. This research demonstrates the potential of integrating LLMs into industrial automation systems, providing innovative solutions for more agile, flexible and adaptive production processes.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para ltx_noindent">
<p id="S3.SS3.p6.1" class="ltx_p"><span id="S3.SS3.p6.1.1" class="ltx_text ltx_font_bold">Robotics &amp; Embodied Artificial Intelligence</span>:
Recent works have developed more efficient reinforcement learning agents for robotics and embodied artificial intelligenceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib160" title="" class="ltx_ref">160</a>, <a href="#bib.bib106" title="" class="ltx_ref">106</a>, <a href="#bib.bib143" title="" class="ltx_ref">143</a>, <a href="#bib.bib133" title="" class="ltx_ref">133</a>, <a href="#bib.bib161" title="" class="ltx_ref">161</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib142" title="" class="ltx_ref">142</a>, <a href="#bib.bib154" title="" class="ltx_ref">154</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The focus is on enhancing autonomous agentsâ€™ abilities for planning, reasoning, and collaboration in embodied environments.
Some approaches such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> combine complementary strengths into unified systems for embodied reasoning and task planning. High-level commands enable improved planning while low-level controllers translate commands into actions. Dialogue for information gathering as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib160" title="" class="ltx_ref">160</a>]</cite> can accelerate training.
Other works such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>, <a href="#bib.bib143" title="" class="ltx_ref">143</a>]</cite> employ autonomous agents for embodied decision-making and exploration guided by internal world models.
Considering physical constraints, agents can generate executable plans and accomplish long-term tasks requiring multiple skills.
In terms of control policies, SayCanÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> focuses on investigating a wide range of manipulation and navigation skills utilizing a mobile manipulator robot. Taking inspiration from typical tasks encountered in a kitchen environment, it present a comprehensive set of 551 skills that cover seven skill families and 17 objects. These skills encompass various actions such as picking, placing, pouring, grasping, and manipulating objects, among others.
Additional frameworks such as VOYAGARÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite> and GITMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite> propose autonomous agents that communicate, collaborate, and accomplish complex tasks. This demonstrates the promise of natural language understanding, motion planning, and human interaction for real-world robotics. As capabilities advance, adaptive autonomous agents may accomplish increasingly complex embodied tasks.
In summary, complementing conventional methods with reasoning and planning abilities as inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib142" title="" class="ltx_ref">142</a>, <a href="#bib.bib154" title="" class="ltx_ref">154</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> significantly improves autonomous agent performance in embodied environments. The focus is on holistic systems that enhance sample efficiency, generalization, and accomplish long-horizon tasks.</p>
</div>
<div id="S3.SS3.p7" class="ltx_para ltx_noindent">
<p id="S3.SS3.p7.1" class="ltx_p"><span id="S3.SS3.p7.1.1" class="ltx_text ltx_font_bold">General Autonomous AI Agent</span>: A number of open source projects based on LLM development have conducted preliminary explorations of Artificial General Intelligence(AGI), they are dedicated to the autonomous universal AI agent frameworkÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib114" title="" class="ltx_ref">114</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib141" title="" class="ltx_ref">141</a>]</cite>, enabling developers to build, manage, and run useful autonomous agents quickly and reliably. For example,
LangChainÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> is an open-source framework that automates coding, testing, debugging, and documentation generation tasks. By integrating language models with data sources and facilitating interaction with the environment, LangChain enables efficient and cost-effective software development through natural language communication and collaboration among multiple agent roles.
Based on LangChain, XLangÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> comes with a comprehensive set of tools, a complete user interface, and support for three different agent scenarios, namely data processing, plugin usage, and web agent.
AutoGPTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> is a fully automated, networkable agent that simply sets one or more goals and automatically breaks them down into corresponding tasks and cycles through them until the goal is reached.
WorkGPTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> is an agent framework similar to AutoGPT and LangChain. By providing it with an instruction and a set of APIs, it engages in back-and-forth conversations with AI until the instruction is completed.
AGiXTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> is a dynamic AI automation platform designed to orchestrate efficient AI command management and task execution across many providers.
AgentVerseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> is a versatile framework that helps researchers quickly create customized multiple LLM-based agent simulations.
GPT ResearcherÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> is an experimental application that leverages large language models to efficiently develop research questions, trigger web crawls to gather information, summarize sources, and aggregate summaries.
BMToolsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite> is an open-source repository that extends LLMs with tools and provides a platform for community-driven tool building and sharing. It supports various types of tools, enables simultaneous task execution using multiple tools, and offers a simple interface for loading plugins via URLs, fostering easy development and contribution to the BMTools ecosystem.
</p>
</div>
<div id="S3.SS3.p8" class="ltx_para ltx_noindent">
<p id="S3.SS3.p8.1" class="ltx_p">In summary, LLM-based autonomous agents are opening up new possibilities across diverse engineering domains to enhance human creativity and productivity. As LLMs continue to advance in their reasoning and generalization capabilities, we expect that symbiotic human-AI teams will unlock new horizons and capabilities in engineering innovation and discovery. However, questions around trust, transparency, and control remain when deploying LLM-based agents in safety-critical engineering systems. Finding the right balance between human and AI capabilities while ensuring robustness will enable realizing the full potential of this technology.</p>
</div>
<div id="S3.SS3.p9" class="ltx_para ltx_noindent">
<p id="S3.SS3.p9.1" class="ltx_p">In the above sections, we introduce the previous work on LLM-based autonomous agents according to their applications. For more clear understanding, we summarize the these applications in TableÂ <a href="#S3" title="3 LLM-based Autonomous Agent Application â€£ A Survey on Large Language Model based Autonomous Agents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Representative applications of LLM-based autonomous agents.</figcaption>
<div id="S3.T2.1" class="ltx_inline-block ltx_transformed_outer" style="width:682.4pt;height:380.9pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-25.7pt,14.3pt) scale(0.93,0.93) ;">
<table id="S3.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.2.1" class="ltx_tr">
<td id="S3.T2.1.1.2.1.1" class="ltx_td ltx_border_t" style="width:62.6pt;padding-top:1pt;padding-bottom:1pt;"></td>
<th id="S3.T2.1.1.2.1.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" style="width:142.3pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.2.1.2.1" class="ltx_p ltx_align_top">Domain</p>
</th>
<th id="S3.T2.1.1.2.1.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" style="width:184.9pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.2.1.3.1" class="ltx_p ltx_align_top">Work</p>
</th>
</tr>
<tr id="S3.T2.1.1.3.2" class="ltx_tr">
<td id="S3.T2.1.1.3.2.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:62.6pt;padding-top:1pt;padding-bottom:1pt;" rowspan="5"><span id="S3.T2.1.1.3.2.1.1" class="ltx_text ltx_align_center ltx_align_top">Social Science</span></td>
<td id="S3.T2.1.1.3.2.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.3.2.2.1" class="ltx_p ltx_align_top">Psychology</p>
</td>
<td id="S3.T2.1.1.3.2.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:184.9pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.3.2.3.1" class="ltx_p ltx_align_top">TE<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, Akata et al. <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, 
<br class="ltx_break ltx_centering">Ziems et al <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite></p>
</td>
</tr>
<tr id="S3.T2.1.1.4.3" class="ltx_tr">
<td id="S3.T2.1.1.4.3.1" class="ltx_td ltx_align_justify" style="width:142.3pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.4.3.1.1" class="ltx_p ltx_align_top">Political Science and Economy</p>
</td>
<td id="S3.T2.1.1.4.3.2" class="ltx_td ltx_align_justify" style="width:184.9pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.4.3.2.1" class="ltx_p ltx_align_top">Out of OneÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, Horton<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>, Ziems et al <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite></p>
</td>
</tr>
<tr id="S3.T2.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.2" class="ltx_td ltx_align_justify" style="width:142.3pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.1.2.1" class="ltx_p ltx_align_top">Social Simulation</p>
</td>
<td id="S3.T2.1.1.1.1" class="ltx_td ltx_align_justify" style="width:184.9pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.1.1.1.1" class="ltx_p ltx_align_top">Social SimulacraÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite>, Generative AgentsÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite>, SocialAI SchoolÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>,
AgentSimsÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>,
<math id="S3.T2.1.1.1.1.1.1.m1.1" class="ltx_centering" alttext="S^{3}" display="inline"><semantics id="S3.T2.1.1.1.1.1.1.m1.1a"><msup id="S3.T2.1.1.1.1.1.1.m1.1.1" xref="S3.T2.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S3.T2.1.1.1.1.1.1.m1.1.1.2" xref="S3.T2.1.1.1.1.1.1.m1.1.1.2.cmml">S</mi><mn id="S3.T2.1.1.1.1.1.1.m1.1.1.3" xref="S3.T2.1.1.1.1.1.1.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.1.1.1.m1.1b"><apply id="S3.T2.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T2.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T2.1.1.1.1.1.1.m1.1.1">superscript</csymbol><ci id="S3.T2.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S3.T2.1.1.1.1.1.1.m1.1.1.2">ğ‘†</ci><cn type="integer" id="S3.T2.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S3.T2.1.1.1.1.1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.1.1.1.m1.1c">S^{3}</annotation><annotation encoding="application/x-llamapun" id="S3.T2.1.1.1.1.1.1.m1.1d">italic_S start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math>Â <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, Williams et al.Â <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib140" title="" class="ltx_ref">140</a>]</cite>,
Li et al.Â <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>,
Chao et al.Â <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite></p>
</td>
</tr>
<tr id="S3.T2.1.1.5.4" class="ltx_tr">
<td id="S3.T2.1.1.5.4.1" class="ltx_td ltx_align_justify" style="width:142.3pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.5.4.1.1" class="ltx_p ltx_align_top">Jurisprudence</p>
</td>
<td id="S3.T2.1.1.5.4.2" class="ltx_td ltx_align_justify" style="width:184.9pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.5.4.2.1" class="ltx_p ltx_align_top">ChatLawÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, Blind JudgementÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite></p>
</td>
</tr>
<tr id="S3.T2.1.1.6.5" class="ltx_tr">
<td id="S3.T2.1.1.6.5.1" class="ltx_td ltx_align_justify" style="width:142.3pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.6.5.1.1" class="ltx_p ltx_align_top">Research Assistant</p>
</td>
<td id="S3.T2.1.1.6.5.2" class="ltx_td ltx_align_justify" style="width:184.9pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.6.5.2.1" class="ltx_p ltx_align_top">Ziems et al Â <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>, Bail et al.Â <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite></p>
</td>
</tr>
<tr id="S3.T2.1.1.7.6" class="ltx_tr">
<td id="S3.T2.1.1.7.6.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:62.6pt;padding-top:1pt;padding-bottom:1pt;" rowspan="3"><span id="S3.T2.1.1.7.6.1.1" class="ltx_text ltx_align_center ltx_align_top">Natural Science</span></td>
<td id="S3.T2.1.1.7.6.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.7.6.2.1" class="ltx_p ltx_align_top">Documentation, Data Managent</p>
</td>
<td id="S3.T2.1.1.7.6.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:184.9pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.7.6.3.1" class="ltx_p ltx_align_top">ChemCrow<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, Boiko et al.<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, ChatMOFÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite></p>
</td>
</tr>
<tr id="S3.T2.1.1.8.7" class="ltx_tr">
<td id="S3.T2.1.1.8.7.1" class="ltx_td ltx_align_justify" style="width:142.3pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.8.7.1.1" class="ltx_p ltx_align_top">Experiment Assistant</p>
</td>
<td id="S3.T2.1.1.8.7.2" class="ltx_td ltx_align_justify" style="width:184.9pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.8.7.2.1" class="ltx_p ltx_align_top">ChemCrowÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, Boiko et al.Â <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite></p>
</td>
</tr>
<tr id="S3.T2.1.1.9.8" class="ltx_tr">
<td id="S3.T2.1.1.9.8.1" class="ltx_td ltx_align_justify" style="width:142.3pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.9.8.1.1" class="ltx_p ltx_align_top">Science Education</p>
</td>
<td id="S3.T2.1.1.9.8.2" class="ltx_td ltx_align_justify" style="width:184.9pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.9.8.2.1" class="ltx_p ltx_align_top">ChemCrowÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, Boiko et al.Â <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>,
<br class="ltx_break ltx_centering">MathAgentÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite>, Drori et al.Â <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite></p>
</td>
</tr>
<tr id="S3.T2.1.1.10.9" class="ltx_tr">
<td id="S3.T2.1.1.10.9.1" class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" style="width:62.6pt;padding-top:1pt;padding-bottom:1pt;" rowspan="6"><span id="S3.T2.1.1.10.9.1.1" class="ltx_text ltx_align_center ltx_align_top">Engineering</span></td>
<td id="S3.T2.1.1.10.9.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.10.9.2.1" class="ltx_p ltx_align_top">Civil Engineering</p>
</td>
<td id="S3.T2.1.1.10.9.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:184.9pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.10.9.3.1" class="ltx_p ltx_align_top">IGLUÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite></p>
</td>
</tr>
<tr id="S3.T2.1.1.11.10" class="ltx_tr">
<td id="S3.T2.1.1.11.10.1" class="ltx_td ltx_align_justify" style="width:142.3pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.11.10.1.1" class="ltx_p ltx_align_top">CS &amp; SE</p>
</td>
<td id="S3.T2.1.1.11.10.2" class="ltx_td ltx_align_justify" style="width:184.9pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.11.10.2.1" class="ltx_p ltx_align_top">ToolBenchÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref">115</a>]</cite>, ChatDevÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite>, MetaGPTÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>, SCGÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, GPTEngineerÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, SmolModelsÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, DemoGPTÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite></p>
</td>
</tr>
<tr id="S3.T2.1.1.12.11" class="ltx_tr">
<td id="S3.T2.1.1.12.11.1" class="ltx_td ltx_align_justify" style="width:142.3pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.12.11.1.1" class="ltx_p ltx_align_top">Aerospace Engineering</p>
</td>
<td id="S3.T2.1.1.12.11.2" class="ltx_td ltx_align_justify" style="width:184.9pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.12.11.2.1" class="ltx_p ltx_align_top">IELLMÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite></p>
</td>
</tr>
<tr id="S3.T2.1.1.13.12" class="ltx_tr">
<td id="S3.T2.1.1.13.12.1" class="ltx_td ltx_align_justify" style="width:142.3pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.13.12.1.1" class="ltx_p ltx_align_top">Industrial Automation</p>
</td>
<td id="S3.T2.1.1.13.12.2" class="ltx_td ltx_align_justify" style="width:184.9pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.13.12.2.1" class="ltx_p ltx_align_top">GPT4IAÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib144" title="" class="ltx_ref">144</a>]</cite></p>
</td>
</tr>
<tr id="S3.T2.1.1.14.13" class="ltx_tr">
<td id="S3.T2.1.1.14.13.1" class="ltx_td ltx_align_justify" style="width:142.3pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.14.13.1.1" class="ltx_p ltx_align_top">Robotics &amp; Embodied AI</p>
</td>
<td id="S3.T2.1.1.14.13.2" class="ltx_td ltx_align_justify" style="width:184.9pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.14.13.2.1" class="ltx_p ltx_align_top">Planner-Actor-ReporterÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, Dialogue ShapingÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib160" title="" class="ltx_ref">160</a>]</cite>, DECKARDÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite>, TaPAÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref">143</a>]</cite>,
VoyagerÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite>, GITMÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite>,Â LLM4RL<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>, PETÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib142" title="" class="ltx_ref">142</a>]</cite>, REMEMBERERÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib154" title="" class="ltx_ref">154</a>]</cite>,
Unified AgentÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, SayCanÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite></p>
</td>
</tr>
<tr id="S3.T2.1.1.15.14" class="ltx_tr">
<td id="S3.T2.1.1.15.14.1" class="ltx_td ltx_align_justify ltx_border_b" style="width:142.3pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.15.14.1.1" class="ltx_p ltx_align_top">General Autonomous Agents</p>
</td>
<td id="S3.T2.1.1.15.14.2" class="ltx_td ltx_align_justify ltx_border_b" style="width:184.9pt;padding-top:1pt;padding-bottom:1pt;">
<p id="S3.T2.1.1.15.14.2.1" class="ltx_p ltx_align_top">AutoGPTÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, AgentGPTÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, AIlegionÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>,
AGiXTÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, AgentVerseÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, XLang<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>,
BabyAGIÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, LangChainÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, WorkGPTÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>,
LoopGPTÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, GPTresearcherÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, BMTools<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite>, TransformersAgentÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, DemoGPTÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, MiniAGIÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, SuperAGIÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>,
AutoGenÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib141" title="" class="ltx_ref">141</a>]</cite></p>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>LLM-based Autonomous Agent Evaluation</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">This section introduces the evaluation methods for assessing the effectiveness of LLM-based autonomous agents.
Similar to LLM itself, the evaluation of AI agent is not an easy problem.
Here, we present two commonly used evaluation strategies for assessing AI agents: subjective and objective evaluation. (Refer to the right part of FigureÂ <a href="#S2.F3" title="Figure 3 â€£ 2.2 Learning Strategy â€£ 2 LLM-based Autonomous Agent Construction â€£ A Survey on Large Language Model based Autonomous Agents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> for an overview.)</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Subjective Evaluation</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">LLM-based agents have a wide range of applications. However, in many scenarios, there lacks general metrics to evaluate the performance of agents. Some potential properties, like agentâ€™s intelligence and user-friendliness, cannot be measured by quantitative metrics as well. Therefore, subjective evaluation is indispensable for current research.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p">Subjective evaluation refers to the testing of the capabilities of LLM-based agents by humans through various means such as interaction, scoring, and so on. In this case, the participating testers are often recruited through crowdsourcing platforms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>, <a href="#bib.bib110" title="" class="ltx_ref">110</a>, <a href="#bib.bib109" title="" class="ltx_ref">109</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib156" title="" class="ltx_ref">156</a>]</cite>; while some researchers believe that crowdsourced personnel are unstable due to individual differences and use expert annotations to conduct the tests <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>. In the following, we present two commonly leveraged strategies.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Human Annotation</span>: In some studies, the human evaluators directly rank or score the generated results of the LLM-based agents based on some specific perspectives <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib163" title="" class="ltx_ref">163</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib156" title="" class="ltx_ref">156</a>]</cite>; Another evaluation type is user-centred, which asks human evaluators to response whether the LLM-based agents system is helpful to themÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite> and whether it is user-friendly Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>, and so on. In specific, one possible evaluation can be whether a social simulation system can effectively contribute to the enhancement of rule design for online communitiesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite>.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">Turing Test</span>: In this method, human evaluators are always asked to distinguish between agent and human behaviours. In Generative AgentsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite>, the first cohort of human evaluators are asked to assess agentsâ€™ key competencies in five areas by interviewing. After two days of play time, another cohort of human evaluators will be asked to differentiate between agents and human responses under the same conditions. In the Free-form Partisan Text experiment<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, human evaluators are asked to guess whether responses are from human or LLM-based agent.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para ltx_noindent">
<p id="S4.SS1.p5.1" class="ltx_p">Because the LLM-based agent system ultimately serves humans, manual evaluation plays an irreplaceable role at this stage, but it also suffers from expensive costs, low efficiency, and population bias. With advances in LLM, it can somewhat play the role of a human for assessment tasks.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para ltx_noindent">
<p id="S4.SS1.p6.1" class="ltx_p">In some of the current studies, additional LLM-agents can be employed as subjective evaluators of the results. In ChemCrowÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, EvaluatorGPT evaluates the outcomes of the experiment by assigning grades that consider both the successful completion of tasks and the accuracy of the underlying thought processes. ChatEvalÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> assembles a panel of multiple agent referees based on LLMs to evaluate the results generated by models through debates. We believe that with the progress of LLM, the results of model evaluation will be more credible and the application will be more extensive.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>
Summary on the evaluation strategies of LLM-based autonomous agents (more details can be seen on https://github.com/Paitesanshi/LLM-Agent-Survey).
For subjective evaluation, we use â‘  and â‘¡ to represent human annotation and the Turing test, respectively.
For objective evaluation, we use â‘ , â‘¡, â‘¢, â‘£, and â‘¤ to represent environment simulation, isolated reasoning, social evaluation, multi-task evaluation, and software testing, respectively.
We also summarize whether these agents use benchmarks for evaluation.</figcaption>
<div id="S4.T3.16" class="ltx_inline-block ltx_transformed_outer" style="width:397.3pt;height:518.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-15.0pt,19.5pt) scale(0.93,0.93) ;">
<table id="S4.T3.16.16" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.16.16.17.1" class="ltx_tr">
<th id="S4.T3.16.16.17.1.1" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.17.1.1.1" class="ltx_p ltx_align_top">Model</p>
</th>
<th id="S4.T3.16.16.17.1.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.17.1.2.1" class="ltx_p ltx_align_top">Subjective</p>
</th>
<th id="S4.T3.16.16.17.1.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.17.1.3.1" class="ltx_p ltx_align_top">Objective</p>
</th>
<th id="S4.T3.16.16.17.1.4" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.17.1.4.1" class="ltx_p ltx_align_top">Benchmark</p>
</th>
<th id="S4.T3.16.16.17.1.5" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.17.1.5.1" class="ltx_p ltx_align_top">Time</p>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.1.1.1.2.1" class="ltx_p ltx_align_top">WebShop <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib149" title="" class="ltx_ref">149</a>]</cite></p>
</td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.1.1.1.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.1.1.1.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.1.1.1.4.1" class="ltx_p ltx_align_top">â‘  â‘¡ â‘£</p>
</td>
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.1.1.1.1.1.1" class="ltx_p ltx_align_top"><math id="S4.T3.1.1.1.1.1.1.m1.1" class="ltx_centering" alttext="\checkmark" display="inline"><semantics id="S4.T3.1.1.1.1.1.1.m1.1a"><mi mathvariant="normal" id="S4.T3.1.1.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.1.1.m1.1.1.cmml">âœ“</mi><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.1.1.m1.1.1">âœ“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.1.1.1.m1.1d">âœ“</annotation></semantics></math></p>
</td>
<td id="S4.T3.1.1.1.5" class="ltx_td ltx_align_justify ltx_border_t" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.1.1.1.5.1" class="ltx_p ltx_align_top">07/2022</p>
</td>
</tr>
<tr id="S4.T3.16.16.18.1" class="ltx_tr">
<td id="S4.T3.16.16.18.1.1" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.18.1.1.1" class="ltx_p ltx_align_top">Social Simulacra <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite></p>
</td>
<td id="S4.T3.16.16.18.1.2" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.18.1.2.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S4.T3.16.16.18.1.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.18.1.3.1" class="ltx_p ltx_align_top">â‘¢</p>
</td>
<td id="S4.T3.16.16.18.1.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.18.1.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.16.16.18.1.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.18.1.5.1" class="ltx_p ltx_align_top">08/2022</p>
</td>
</tr>
<tr id="S4.T3.16.16.19.2" class="ltx_tr">
<td id="S4.T3.16.16.19.2.1" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.19.2.1.1" class="ltx_p ltx_align_top">TE <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite></p>
</td>
<td id="S4.T3.16.16.19.2.2" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.19.2.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.16.16.19.2.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.19.2.3.1" class="ltx_p ltx_align_top">â‘¢</p>
</td>
<td id="S4.T3.16.16.19.2.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.19.2.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.16.16.19.2.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.19.2.5.1" class="ltx_p ltx_align_top">08/2022</p>
</td>
</tr>
<tr id="S4.T3.16.16.20.3" class="ltx_tr">
<td id="S4.T3.16.16.20.3.1" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.20.3.1.1" class="ltx_p ltx_align_top">LIBRO <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite></p>
</td>
<td id="S4.T3.16.16.20.3.2" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.20.3.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.16.16.20.3.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.20.3.3.1" class="ltx_p ltx_align_top">â‘¤</p>
</td>
<td id="S4.T3.16.16.20.3.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.20.3.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.16.16.20.3.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.20.3.5.1" class="ltx_p ltx_align_top">09/2022</p>
</td>
</tr>
<tr id="S4.T3.2.2.2" class="ltx_tr">
<td id="S4.T3.2.2.2.2" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.2.2.2.2.1" class="ltx_p ltx_align_top">ReAct<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib151" title="" class="ltx_ref">151</a>]</cite></p>
</td>
<td id="S4.T3.2.2.2.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.2.2.2.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.2.2.2.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.2.2.2.4.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S4.T3.2.2.2.1" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.2.2.2.1.1.1" class="ltx_p ltx_align_top"><math id="S4.T3.2.2.2.1.1.1.m1.1" class="ltx_centering" alttext="\checkmark" display="inline"><semantics id="S4.T3.2.2.2.1.1.1.m1.1a"><mi mathvariant="normal" id="S4.T3.2.2.2.1.1.1.m1.1.1" xref="S4.T3.2.2.2.1.1.1.m1.1.1.cmml">âœ“</mi><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.1.1.m1.1b"><ci id="S4.T3.2.2.2.1.1.1.m1.1.1.cmml" xref="S4.T3.2.2.2.1.1.1.m1.1.1">âœ“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.1.1.1.m1.1d">âœ“</annotation></semantics></math></p>
</td>
<td id="S4.T3.2.2.2.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.2.2.2.5.1" class="ltx_p ltx_align_top">10/2022</p>
</td>
</tr>
<tr id="S4.T3.16.16.21.4" class="ltx_tr">
<td id="S4.T3.16.16.21.4.1" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.21.4.1.1" class="ltx_p ltx_align_top">Out of One, Many<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite></p>
</td>
<td id="S4.T3.16.16.21.4.2" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.21.4.2.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S4.T3.16.16.21.4.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.21.4.3.1" class="ltx_p ltx_align_top">â‘¡ â‘£</p>
</td>
<td id="S4.T3.16.16.21.4.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.21.4.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.16.16.21.4.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.21.4.5.1" class="ltx_p ltx_align_top">02/2023</p>
</td>
</tr>
<tr id="S4.T3.3.3.3" class="ltx_tr">
<td id="S4.T3.3.3.3.2" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.3.3.3.2.1" class="ltx_p ltx_align_top">DEPS<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib137" title="" class="ltx_ref">137</a>]</cite></p>
</td>
<td id="S4.T3.3.3.3.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.3.3.3.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.3.3.3.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.3.3.3.4.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S4.T3.3.3.3.1" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.3.3.3.1.1.1" class="ltx_p ltx_align_top"><math id="S4.T3.3.3.3.1.1.1.m1.1" class="ltx_centering" alttext="\checkmark" display="inline"><semantics id="S4.T3.3.3.3.1.1.1.m1.1a"><mi mathvariant="normal" id="S4.T3.3.3.3.1.1.1.m1.1.1" xref="S4.T3.3.3.3.1.1.1.m1.1.1.cmml">âœ“</mi><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.1.1.1.m1.1b"><ci id="S4.T3.3.3.3.1.1.1.m1.1.1.cmml" xref="S4.T3.3.3.3.1.1.1.m1.1.1">âœ“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.3.3.1.1.1.m1.1d">âœ“</annotation></semantics></math></p>
</td>
<td id="S4.T3.3.3.3.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.3.3.3.5.1" class="ltx_p ltx_align_top">02/2023</p>
</td>
</tr>
<tr id="S4.T3.16.16.22.5" class="ltx_tr">
<td id="S4.T3.16.16.22.5.1" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.22.5.1.1" class="ltx_p ltx_align_top">Jalil et al.<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite></p>
</td>
<td id="S4.T3.16.16.22.5.2" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.22.5.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.16.16.22.5.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.22.5.3.1" class="ltx_p ltx_align_top">â‘¤</p>
</td>
<td id="S4.T3.16.16.22.5.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.22.5.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.16.16.22.5.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.22.5.5.1" class="ltx_p ltx_align_top">02/2023</p>
</td>
</tr>
<tr id="S4.T3.16.16.23.6" class="ltx_tr">
<td id="S4.T3.16.16.23.6.1" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.23.6.1.1" class="ltx_p ltx_align_top">Reflexion<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite></p>
</td>
<td id="S4.T3.16.16.23.6.2" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.23.6.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.16.16.23.6.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.23.6.3.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S4.T3.16.16.23.6.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.23.6.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.16.16.23.6.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.23.6.5.1" class="ltx_p ltx_align_top">03/2023</p>
</td>
</tr>
<tr id="S4.T3.4.4.4" class="ltx_tr">
<td id="S4.T3.4.4.4.2" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.4.4.4.2.1" class="ltx_p ltx_align_top">IGLU<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite></p>
</td>
<td id="S4.T3.4.4.4.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.4.4.4.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.4.4.4.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.4.4.4.4.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S4.T3.4.4.4.1" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.4.4.4.1.1.1" class="ltx_p ltx_align_top"><math id="S4.T3.4.4.4.1.1.1.m1.1" class="ltx_centering" alttext="\checkmark" display="inline"><semantics id="S4.T3.4.4.4.1.1.1.m1.1a"><mi mathvariant="normal" id="S4.T3.4.4.4.1.1.1.m1.1.1" xref="S4.T3.4.4.4.1.1.1.m1.1.1.cmml">âœ“</mi><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.1.1.1.m1.1b"><ci id="S4.T3.4.4.4.1.1.1.m1.1.1.cmml" xref="S4.T3.4.4.4.1.1.1.m1.1.1">âœ“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.4.4.1.1.1.m1.1d">âœ“</annotation></semantics></math></p>
</td>
<td id="S4.T3.4.4.4.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.4.4.4.5.1" class="ltx_p ltx_align_top">04/2023</p>
</td>
</tr>
<tr id="S4.T3.16.16.24.7" class="ltx_tr">
<td id="S4.T3.16.16.24.7.1" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.24.7.1.1" class="ltx_p ltx_align_top">LLM+P<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite></p>
</td>
<td id="S4.T3.16.16.24.7.2" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.24.7.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.16.16.24.7.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.24.7.3.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S4.T3.16.16.24.7.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.24.7.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.16.16.24.7.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.24.7.5.1" class="ltx_p ltx_align_top">04/2023</p>
</td>
</tr>
<tr id="S4.T3.16.16.25.8" class="ltx_tr">
<td id="S4.T3.16.16.25.8.1" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.25.8.1.1" class="ltx_p ltx_align_top">Generative Agents<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite></p>
</td>
<td id="S4.T3.16.16.25.8.2" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.25.8.2.1" class="ltx_p ltx_align_top">â‘ â‘¡</p>
</td>
<td id="S4.T3.16.16.25.8.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.25.8.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.16.16.25.8.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.25.8.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.16.16.25.8.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.25.8.5.1" class="ltx_p ltx_align_top">04/2023</p>
</td>
</tr>
<tr id="S4.T3.5.5.5" class="ltx_tr">
<td id="S4.T3.5.5.5.2" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.5.5.5.2.1" class="ltx_p ltx_align_top">ToolBench<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite></p>
</td>
<td id="S4.T3.5.5.5.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.5.5.5.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.5.5.5.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.5.5.5.4.1" class="ltx_p ltx_align_top">â‘£</p>
</td>
<td id="S4.T3.5.5.5.1" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.5.5.5.1.1.1" class="ltx_p ltx_align_top"><math id="S4.T3.5.5.5.1.1.1.m1.1" class="ltx_centering" alttext="\checkmark" display="inline"><semantics id="S4.T3.5.5.5.1.1.1.m1.1a"><mi mathvariant="normal" id="S4.T3.5.5.5.1.1.1.m1.1.1" xref="S4.T3.5.5.5.1.1.1.m1.1.1.cmml">âœ“</mi><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.1.1.1.m1.1b"><ci id="S4.T3.5.5.5.1.1.1.m1.1.1.cmml" xref="S4.T3.5.5.5.1.1.1.m1.1.1">âœ“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.5.5.5.1.1.1.m1.1d">âœ“</annotation></semantics></math></p>
</td>
<td id="S4.T3.5.5.5.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.5.5.5.5.1" class="ltx_p ltx_align_top">04/2023</p>
</td>
</tr>
<tr id="S4.T3.6.6.6" class="ltx_tr">
<td id="S4.T3.6.6.6.2" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.6.6.6.2.1" class="ltx_p ltx_align_top">GITM<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite></p>
</td>
<td id="S4.T3.6.6.6.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.6.6.6.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.6.6.6.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.6.6.6.4.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S4.T3.6.6.6.1" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.6.6.6.1.1.1" class="ltx_p ltx_align_top"><math id="S4.T3.6.6.6.1.1.1.m1.1" class="ltx_centering" alttext="\checkmark" display="inline"><semantics id="S4.T3.6.6.6.1.1.1.m1.1a"><mi mathvariant="normal" id="S4.T3.6.6.6.1.1.1.m1.1.1" xref="S4.T3.6.6.6.1.1.1.m1.1.1.cmml">âœ“</mi><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.6.1.1.1.m1.1b"><ci id="S4.T3.6.6.6.1.1.1.m1.1.1.cmml" xref="S4.T3.6.6.6.1.1.1.m1.1.1">âœ“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.6.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.6.6.6.1.1.1.m1.1d">âœ“</annotation></semantics></math></p>
</td>
<td id="S4.T3.6.6.6.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.6.6.6.5.1" class="ltx_p ltx_align_top">05/2023</p>
</td>
</tr>
<tr id="S4.T3.16.16.26.9" class="ltx_tr">
<td id="S4.T3.16.16.26.9.1" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.26.9.1.1" class="ltx_p ltx_align_top">Two-Failures<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite></p>
</td>
<td id="S4.T3.16.16.26.9.2" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.26.9.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.16.16.26.9.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.26.9.3.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S4.T3.16.16.26.9.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.26.9.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.16.16.26.9.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.26.9.5.1" class="ltx_p ltx_align_top">05/2023</p>
</td>
</tr>
<tr id="S4.T3.7.7.7" class="ltx_tr">
<td id="S4.T3.7.7.7.2" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.7.7.7.2.1" class="ltx_p ltx_align_top">Voyager<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite></p>
</td>
<td id="S4.T3.7.7.7.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.7.7.7.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.7.7.7.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.7.7.7.4.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S4.T3.7.7.7.1" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.7.7.7.1.1.1" class="ltx_p ltx_align_top"><math id="S4.T3.7.7.7.1.1.1.m1.1" class="ltx_centering" alttext="\checkmark" display="inline"><semantics id="S4.T3.7.7.7.1.1.1.m1.1a"><mi mathvariant="normal" id="S4.T3.7.7.7.1.1.1.m1.1.1" xref="S4.T3.7.7.7.1.1.1.m1.1.1.cmml">âœ“</mi><annotation-xml encoding="MathML-Content" id="S4.T3.7.7.7.1.1.1.m1.1b"><ci id="S4.T3.7.7.7.1.1.1.m1.1.1.cmml" xref="S4.T3.7.7.7.1.1.1.m1.1.1">âœ“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.7.7.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.7.7.7.1.1.1.m1.1d">âœ“</annotation></semantics></math></p>
</td>
<td id="S4.T3.7.7.7.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.7.7.7.5.1" class="ltx_p ltx_align_top">05/2023</p>
</td>
</tr>
<tr id="S4.T3.8.8.8" class="ltx_tr">
<td id="S4.T3.8.8.8.2" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.8.8.8.2.1" class="ltx_p ltx_align_top">SocKET <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite></p>
</td>
<td id="S4.T3.8.8.8.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.8.8.8.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.8.8.8.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.8.8.8.4.1" class="ltx_p ltx_align_top">â‘¡ â‘¢ â‘£</p>
</td>
<td id="S4.T3.8.8.8.1" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.8.8.8.1.1.1" class="ltx_p ltx_align_top"><math id="S4.T3.8.8.8.1.1.1.m1.1" class="ltx_centering" alttext="\checkmark" display="inline"><semantics id="S4.T3.8.8.8.1.1.1.m1.1a"><mi mathvariant="normal" id="S4.T3.8.8.8.1.1.1.m1.1.1" xref="S4.T3.8.8.8.1.1.1.m1.1.1.cmml">âœ“</mi><annotation-xml encoding="MathML-Content" id="S4.T3.8.8.8.1.1.1.m1.1b"><ci id="S4.T3.8.8.8.1.1.1.m1.1.1.cmml" xref="S4.T3.8.8.8.1.1.1.m1.1.1">âœ“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.8.8.8.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.8.8.8.1.1.1.m1.1d">âœ“</annotation></semantics></math></p>
</td>
<td id="S4.T3.8.8.8.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.8.8.8.5.1" class="ltx_p ltx_align_top">05/2023</p>
</td>
</tr>
<tr id="S4.T3.9.9.9" class="ltx_tr">
<td id="S4.T3.9.9.9.2" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.9.9.9.2.1" class="ltx_p ltx_align_top">MobileEnv <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib155" title="" class="ltx_ref">155</a>]</cite></p>
</td>
<td id="S4.T3.9.9.9.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.9.9.9.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.9.9.9.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.9.9.9.4.1" class="ltx_p ltx_align_top">â‘  â‘¡ â‘£</p>
</td>
<td id="S4.T3.9.9.9.1" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.9.9.9.1.1.1" class="ltx_p ltx_align_top"><math id="S4.T3.9.9.9.1.1.1.m1.1" class="ltx_centering" alttext="\checkmark" display="inline"><semantics id="S4.T3.9.9.9.1.1.1.m1.1a"><mi mathvariant="normal" id="S4.T3.9.9.9.1.1.1.m1.1.1" xref="S4.T3.9.9.9.1.1.1.m1.1.1.cmml">âœ“</mi><annotation-xml encoding="MathML-Content" id="S4.T3.9.9.9.1.1.1.m1.1b"><ci id="S4.T3.9.9.9.1.1.1.m1.1.1.cmml" xref="S4.T3.9.9.9.1.1.1.m1.1.1">âœ“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.9.9.9.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.9.9.9.1.1.1.m1.1d">âœ“</annotation></semantics></math></p>
</td>
<td id="S4.T3.9.9.9.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.9.9.9.5.1" class="ltx_p ltx_align_top">05/2023</p>
</td>
</tr>
<tr id="S4.T3.10.10.10" class="ltx_tr">
<td id="S4.T3.10.10.10.2" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.10.10.10.2.1" class="ltx_p ltx_align_top">clembench <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite></p>
</td>
<td id="S4.T3.10.10.10.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.10.10.10.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.10.10.10.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.10.10.10.4.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S4.T3.10.10.10.1" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.10.10.10.1.1.1" class="ltx_p ltx_align_top"><math id="S4.T3.10.10.10.1.1.1.m1.1" class="ltx_centering" alttext="\checkmark" display="inline"><semantics id="S4.T3.10.10.10.1.1.1.m1.1a"><mi mathvariant="normal" id="S4.T3.10.10.10.1.1.1.m1.1.1" xref="S4.T3.10.10.10.1.1.1.m1.1.1.cmml">âœ“</mi><annotation-xml encoding="MathML-Content" id="S4.T3.10.10.10.1.1.1.m1.1b"><ci id="S4.T3.10.10.10.1.1.1.m1.1.1.cmml" xref="S4.T3.10.10.10.1.1.1.m1.1.1">âœ“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.10.10.10.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.10.10.10.1.1.1.m1.1d">âœ“</annotation></semantics></math></p>
</td>
<td id="S4.T3.10.10.10.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.10.10.10.5.1" class="ltx_p ltx_align_top">05/2023</p>
</td>
</tr>
<tr id="S4.T3.11.11.11" class="ltx_tr">
<td id="S4.T3.11.11.11.2" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.11.11.11.2.1" class="ltx_p ltx_align_top">DialopÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite></p>
</td>
<td id="S4.T3.11.11.11.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.11.11.11.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.11.11.11.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.11.11.11.4.1" class="ltx_p ltx_align_top">â‘¢</p>
</td>
<td id="S4.T3.11.11.11.1" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.11.11.11.1.1.1" class="ltx_p ltx_align_top"><math id="S4.T3.11.11.11.1.1.1.m1.1" class="ltx_centering" alttext="\checkmark" display="inline"><semantics id="S4.T3.11.11.11.1.1.1.m1.1a"><mi mathvariant="normal" id="S4.T3.11.11.11.1.1.1.m1.1.1" xref="S4.T3.11.11.11.1.1.1.m1.1.1.cmml">âœ“</mi><annotation-xml encoding="MathML-Content" id="S4.T3.11.11.11.1.1.1.m1.1b"><ci id="S4.T3.11.11.11.1.1.1.m1.1.1.cmml" xref="S4.T3.11.11.11.1.1.1.m1.1.1">âœ“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.11.11.11.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.11.11.11.1.1.1.m1.1d">âœ“</annotation></semantics></math></p>
</td>
<td id="S4.T3.11.11.11.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.11.11.11.5.1" class="ltx_p ltx_align_top">06/2023</p>
</td>
</tr>
<tr id="S4.T3.16.16.27.10" class="ltx_tr">
<td id="S4.T3.16.16.27.10.1" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.27.10.1.1" class="ltx_p ltx_align_top">ChatDB<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite></p>
</td>
<td id="S4.T3.16.16.27.10.2" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.27.10.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.16.16.27.10.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.27.10.3.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S4.T3.16.16.27.10.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.27.10.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.16.16.27.10.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.27.10.5.1" class="ltx_p ltx_align_top">06/2023</p>
</td>
</tr>
<tr id="S4.T3.16.16.28.11" class="ltx_tr">
<td id="S4.T3.16.16.28.11.1" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.28.11.1.1" class="ltx_p ltx_align_top">Feldt et al.<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite></p>
</td>
<td id="S4.T3.16.16.28.11.2" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.28.11.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.16.16.28.11.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.28.11.3.1" class="ltx_p ltx_align_top">â‘¤</p>
</td>
<td id="S4.T3.16.16.28.11.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.28.11.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.16.16.28.11.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.28.11.5.1" class="ltx_p ltx_align_top">06/2023</p>
</td>
</tr>
<tr id="S4.T3.16.16.29.12" class="ltx_tr">
<td id="S4.T3.16.16.29.12.1" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.29.12.1.1" class="ltx_p ltx_align_top">CO-LLM <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib156" title="" class="ltx_ref">156</a>]</cite></p>
</td>
<td id="S4.T3.16.16.29.12.2" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.29.12.2.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S4.T3.16.16.29.12.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.29.12.3.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S4.T3.16.16.29.12.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.29.12.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.16.16.29.12.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.29.12.5.1" class="ltx_p ltx_align_top">07/2023</p>
</td>
</tr>
<tr id="S4.T3.12.12.12" class="ltx_tr">
<td id="S4.T3.12.12.12.2" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.12.12.12.2.1" class="ltx_p ltx_align_top">Tachikuma<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite></p>
</td>
<td id="S4.T3.12.12.12.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.12.12.12.3.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S4.T3.12.12.12.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.12.12.12.4.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S4.T3.12.12.12.1" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.12.12.12.1.1.1" class="ltx_p ltx_align_top"><math id="S4.T3.12.12.12.1.1.1.m1.1" class="ltx_centering" alttext="\checkmark" display="inline"><semantics id="S4.T3.12.12.12.1.1.1.m1.1a"><mi mathvariant="normal" id="S4.T3.12.12.12.1.1.1.m1.1.1" xref="S4.T3.12.12.12.1.1.1.m1.1.1.cmml">âœ“</mi><annotation-xml encoding="MathML-Content" id="S4.T3.12.12.12.1.1.1.m1.1b"><ci id="S4.T3.12.12.12.1.1.1.m1.1.1.cmml" xref="S4.T3.12.12.12.1.1.1.m1.1.1">âœ“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.12.12.12.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.12.12.12.1.1.1.m1.1d">âœ“</annotation></semantics></math></p>
</td>
<td id="S4.T3.12.12.12.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.12.12.12.5.1" class="ltx_p ltx_align_top">07/2023</p>
</td>
</tr>
<tr id="S4.T3.16.16.30.13" class="ltx_tr">
<td id="S4.T3.16.16.30.13.1" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.30.13.1.1" class="ltx_p ltx_align_top">ChatDev<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite></p>
</td>
<td id="S4.T3.16.16.30.13.2" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.30.13.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.16.16.30.13.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.30.13.3.1" class="ltx_p ltx_align_top">â‘¡</p>
</td>
<td id="S4.T3.16.16.30.13.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.30.13.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.16.16.30.13.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.30.13.5.1" class="ltx_p ltx_align_top">07/2023</p>
</td>
</tr>
<tr id="S4.T3.13.13.13" class="ltx_tr">
<td id="S4.T3.13.13.13.2" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.13.13.13.2.1" class="ltx_p ltx_align_top">WebArenaÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib159" title="" class="ltx_ref">159</a>]</cite></p>
</td>
<td id="S4.T3.13.13.13.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.13.13.13.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.13.13.13.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.13.13.13.4.1" class="ltx_p ltx_align_top">â‘ </p>
</td>
<td id="S4.T3.13.13.13.1" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.13.13.13.1.1.1" class="ltx_p ltx_align_top"><math id="S4.T3.13.13.13.1.1.1.m1.1" class="ltx_centering" alttext="\checkmark" display="inline"><semantics id="S4.T3.13.13.13.1.1.1.m1.1a"><mi mathvariant="normal" id="S4.T3.13.13.13.1.1.1.m1.1.1" xref="S4.T3.13.13.13.1.1.1.m1.1.1.cmml">âœ“</mi><annotation-xml encoding="MathML-Content" id="S4.T3.13.13.13.1.1.1.m1.1b"><ci id="S4.T3.13.13.13.1.1.1.m1.1.1.cmml" xref="S4.T3.13.13.13.1.1.1.m1.1.1">âœ“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.13.13.13.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.13.13.13.1.1.1.m1.1d">âœ“</annotation></semantics></math></p>
</td>
<td id="S4.T3.13.13.13.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.13.13.13.5.1" class="ltx_p ltx_align_top">07/2023</p>
</td>
</tr>
<tr id="S4.T3.16.16.31.14" class="ltx_tr">
<td id="S4.T3.16.16.31.14.1" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.31.14.1.1" class="ltx_p ltx_align_top">AgentSims<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite></p>
</td>
<td id="S4.T3.16.16.31.14.2" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.31.14.2.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.16.16.31.14.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.31.14.3.1" class="ltx_p ltx_align_top">â‘¢</p>
</td>
<td id="S4.T3.16.16.31.14.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.31.14.4.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.16.16.31.14.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.31.14.5.1" class="ltx_p ltx_align_top">08/2023</p>
</td>
</tr>
<tr id="S4.T3.14.14.14" class="ltx_tr">
<td id="S4.T3.14.14.14.2" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.14.14.14.2.1" class="ltx_p ltx_align_top">AgentBench<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite></p>
</td>
<td id="S4.T3.14.14.14.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.14.14.14.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.14.14.14.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.14.14.14.4.1" class="ltx_p ltx_align_top">â‘£</p>
</td>
<td id="S4.T3.14.14.14.1" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.14.14.14.1.1.1" class="ltx_p ltx_align_top"><math id="S4.T3.14.14.14.1.1.1.m1.1" class="ltx_centering" alttext="\checkmark" display="inline"><semantics id="S4.T3.14.14.14.1.1.1.m1.1a"><mi mathvariant="normal" id="S4.T3.14.14.14.1.1.1.m1.1.1" xref="S4.T3.14.14.14.1.1.1.m1.1.1.cmml">âœ“</mi><annotation-xml encoding="MathML-Content" id="S4.T3.14.14.14.1.1.1.m1.1b"><ci id="S4.T3.14.14.14.1.1.1.m1.1.1.cmml" xref="S4.T3.14.14.14.1.1.1.m1.1.1">âœ“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.14.14.14.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.14.14.14.1.1.1.m1.1d">âœ“</annotation></semantics></math></p>
</td>
<td id="S4.T3.14.14.14.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.14.14.14.5.1" class="ltx_p ltx_align_top">08/2023</p>
</td>
</tr>
<tr id="S4.T3.15.15.15" class="ltx_tr">
<td id="S4.T3.15.15.15.2" class="ltx_td ltx_align_justify" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.15.15.15.2.1" class="ltx_p ltx_align_top">BOLAA<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite></p>
</td>
<td id="S4.T3.15.15.15.3" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.15.15.15.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.15.15.15.4" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.15.15.15.4.1" class="ltx_p ltx_align_top">â‘  â‘£ â‘¤</p>
</td>
<td id="S4.T3.15.15.15.1" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.15.15.15.1.1.1" class="ltx_p ltx_align_top"><math id="S4.T3.15.15.15.1.1.1.m1.1" class="ltx_centering" alttext="\checkmark" display="inline"><semantics id="S4.T3.15.15.15.1.1.1.m1.1a"><mi mathvariant="normal" id="S4.T3.15.15.15.1.1.1.m1.1.1" xref="S4.T3.15.15.15.1.1.1.m1.1.1.cmml">âœ“</mi><annotation-xml encoding="MathML-Content" id="S4.T3.15.15.15.1.1.1.m1.1b"><ci id="S4.T3.15.15.15.1.1.1.m1.1.1.cmml" xref="S4.T3.15.15.15.1.1.1.m1.1.1">âœ“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.15.15.15.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.15.15.15.1.1.1.m1.1d">âœ“</annotation></semantics></math></p>
</td>
<td id="S4.T3.15.15.15.5" class="ltx_td ltx_align_justify" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.15.15.15.5.1" class="ltx_p ltx_align_top">08/2023</p>
</td>
</tr>
<tr id="S4.T3.16.16.16" class="ltx_tr">
<td id="S4.T3.16.16.16.2" class="ltx_td ltx_align_justify ltx_border_b" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.16.2.1" class="ltx_p ltx_align_top">GentopiaÂ <cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a href="#bib.bib146" title="" class="ltx_ref">146</a>]</cite></p>
</td>
<td id="S4.T3.16.16.16.3" class="ltx_td ltx_align_justify ltx_border_b" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.16.3.1" class="ltx_p ltx_align_top">-</p>
</td>
<td id="S4.T3.16.16.16.4" class="ltx_td ltx_align_justify ltx_border_b" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.16.4.1" class="ltx_p ltx_align_top">â‘¡ â‘£</p>
</td>
<td id="S4.T3.16.16.16.1" class="ltx_td ltx_align_justify ltx_border_b" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.16.1.1.1" class="ltx_p ltx_align_top"><math id="S4.T3.16.16.16.1.1.1.m1.1" class="ltx_centering" alttext="\checkmark" display="inline"><semantics id="S4.T3.16.16.16.1.1.1.m1.1a"><mi mathvariant="normal" id="S4.T3.16.16.16.1.1.1.m1.1.1" xref="S4.T3.16.16.16.1.1.1.m1.1.1.cmml">âœ“</mi><annotation-xml encoding="MathML-Content" id="S4.T3.16.16.16.1.1.1.m1.1b"><ci id="S4.T3.16.16.16.1.1.1.m1.1.1.cmml" xref="S4.T3.16.16.16.1.1.1.m1.1.1">âœ“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.16.16.16.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T3.16.16.16.1.1.1.m1.1d">âœ“</annotation></semantics></math></p>
</td>
<td id="S4.T3.16.16.16.5" class="ltx_td ltx_align_justify ltx_border_b" style="width:62.6pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<p id="S4.T3.16.16.16.5.1" class="ltx_p ltx_align_top">08/2023</p>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Objective Evaluation</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">Objective evaluation offers several benefits over human evaluations. Quantitative metrics enable clear comparisons between different methods and tracking of progress over time. Large-scale automated testing is feasible, allowing evaluation on thousands of tasks rather than a handfulÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Results are also more objective and reproducible. However, human evaluations can assess complementary qualities like naturalness, nuance, and social intelligence that are difficult to quantify objectively. The two approaches can therefore be used in conjunction.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p">Objective evaluation refers to assessing the capabilities of LLM-based autonomous agents using quantitative metrics that can be computed, compared and tracked over time. In contrast to subjective or human evaluations, objective metrics aim to provide concrete, measurable insights into agent performance. In this section, we review and synthesize objective evaluation approaches from the perspectives of metrics, strategies and benchmarks.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Metrics</span>:
In order to objectively evaluate the effectiveness of the agents, designing proper metrics is significant, which may influence the evaluation accuracy and comprehensiveness.
Ideal evaluation metrics should precisely reflect the quality of the agents, and align with the human feelings when using them in real-world scenarios.
In existing work, we can see the following representative evaluation metrics.
(1) <span id="S4.SS2.p3.1.2" class="ltx_text ltx_font_italic">Task success metrics:</span> These metrics measure how well an agent can complete tasks and achieve goals. Common metrics include success rateÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib156" title="" class="ltx_ref">156</a>, <a href="#bib.bib151" title="" class="ltx_ref">151</a>, <a href="#bib.bib125" title="" class="ltx_ref">125</a>, <a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>, reward/scoreÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib156" title="" class="ltx_ref">156</a>, <a href="#bib.bib151" title="" class="ltx_ref">151</a>, <a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>, coverageÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite>, and accuracyÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>. Higher values indicate greater task completion ability.
(2) <span id="S4.SS2.p3.1.3" class="ltx_text ltx_font_italic">Human similarity metrics:</span> These metrics quantify the degree to which agent behavior closely resembles that of humans.
Typical examples include trajectory/location accuracyÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite>, dialogue similaritiesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, and mimicry of human responsesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Higher similarity suggests more human-like reasoning.
(3) <span id="S4.SS2.p3.1.4" class="ltx_text ltx_font_italic">Efficiency metrics:</span>
In contrast to the aforementioned metrics used to evaluate agent effectiveness, these metrics assess agent efficiency from various perspectives. Typical metrics include planning lengthÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>, development costÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite>, inference speedÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>, <a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite>, and number of clarification dialoguesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>.
</p>
</div>
<div id="S4.SS2.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.p4.1" class="ltx_p"><span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_bold">Strategies</span>: Based on the methods employed for evaluation, we can identify several common strategies:
(1) <span id="S4.SS2.p4.1.2" class="ltx_text ltx_font_italic">Environmental simulation:</span>
In this method, the agents are assessed in immersive 3D environments such as games and interactive fiction using metrics for task success and human similarity, which incorporate factors like trajectories, language usage, and completed objectivesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib156" title="" class="ltx_ref">156</a>, <a href="#bib.bib161" title="" class="ltx_ref">161</a>, <a href="#bib.bib151" title="" class="ltx_ref">151</a>, <a href="#bib.bib133" title="" class="ltx_ref">133</a>, <a href="#bib.bib99" title="" class="ltx_ref">99</a>, <a href="#bib.bib137" title="" class="ltx_ref">137</a>, <a href="#bib.bib85" title="" class="ltx_ref">85</a>, <a href="#bib.bib149" title="" class="ltx_ref">149</a>, <a href="#bib.bib155" title="" class="ltx_ref">155</a>]</cite>.
This showcases the agentsâ€™ practical abilities in real-world scenarios.
(2) <span id="S4.SS2.p4.1.3" class="ltx_text ltx_font_italic">Isolated reasoning:</span>
In this method, the researchers concentrate on fundamental cognitive abilities by employing limited tasks such as accuracy, passage completion rate, and ablation measuresÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib125" title="" class="ltx_ref">125</a>, <a href="#bib.bib90" title="" class="ltx_ref">90</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib149" title="" class="ltx_ref">149</a>, <a href="#bib.bib155" title="" class="ltx_ref">155</a>]</cite>.
This approach simplifies the analysis of individual skills.
(3) <span id="S4.SS2.p4.1.4" class="ltx_text ltx_font_italic">Social evaluation:</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib89" title="" class="ltx_ref">89</a>, <a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> directly probe social intelligence using human studies and mimicry metrics. This assesses higher-order social cognition.
(4) <span id="S4.SS2.p4.1.5" class="ltx_text ltx_font_italic">Multi-task:</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib114" title="" class="ltx_ref">114</a>, <a href="#bib.bib93" title="" class="ltx_ref">93</a>, <a href="#bib.bib94" title="" class="ltx_ref">94</a>, <a href="#bib.bib149" title="" class="ltx_ref">149</a>, <a href="#bib.bib155" title="" class="ltx_ref">155</a>]</cite> use suites of diverse tasks from different domains with zero/few-shot evaluation. This measures generalizability.
(5) <span id="S4.SS2.p4.1.6" class="ltx_text ltx_font_italic">Software testing:</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> explore the use of LLMs for various software testing tasks, such as generating test cases, reproducing bugs, debugging code, and interacting with developers and external tools. They use metrics such as test coverage, bug detection rate, code quality, and reasoning ability to measure the effectiveness of LLM-based agents.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para ltx_noindent">
<p id="S4.SS2.p5.1" class="ltx_p"><span id="S4.SS2.p5.1.1" class="ltx_text ltx_font_bold">Benchmarks</span>: In addition to metrics, objective evaluation relies on benchmarks, controlled experiments, and statistical significance testing. Many papers construct benchmarks with datasets of tasks and environments to systematically test agents, such as ALFWorldÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib151" title="" class="ltx_ref">151</a>]</cite>, IGLUÂ  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>, and Minecraft Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>, <a href="#bib.bib133" title="" class="ltx_ref">133</a>, <a href="#bib.bib137" title="" class="ltx_ref">137</a>]</cite>.
ClembenchÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> is a game-based approach for evaluating chat-optimized language models as conversational agents, which explores the possibility of meaningfully evaluating LLMs by exposing them to restricted, game-like settings designed to challenge specific capabilities.
TachikumaÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> is a benchmark that leverages TRPG game logs to evaluate LLMsâ€™ ability to understand and infer complex interactions with multiple characters and novel objects.
AgentBenchÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> provides a comprehensive framework for evaluating LLMs as autonomous agents across diverse environments, which enables standardized benchmarking of LLM agents by adopting F1 as the primary metric. It represents the first systematic assessment of pretrained LLMs as agents on real-world challenges across diverse domains.
SocKETÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> is a comprehensive benchmark for evaluating the social knowledge capabilities of large language models (LLMs) across 58 tasks covering five categories of social information such as humor and sarcasm, emotions and feelings, credibility, etc.
AgentSimsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> is a versatile infrastructure for building testing sandboxes for large-scale language models, facilitating diverse evaluation tasks and applications in data generation and social science research.
ToolBenchÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite> is an open-source project that aims to facilitate the construction of powerful large language models (LLMs) with general tool-use capability by providing an open platform for training, serving, and evaluating LLMs for tool learning .
DialopÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite> was designed with three tasks: optimisation, planning, and mediation, to evaluate the decision-making ability of the LLM-based agent.
WebShopÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib149" title="" class="ltx_ref">149</a>]</cite> Benchmark evaluates LLM agents on product search and retrieval from a collection of 1.18 million real-world items through search queries and clicks, using rewards based on attribute overlap and recall performance.
Mobile-EnvÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib155" title="" class="ltx_ref">155</a>]</cite> is an easily-extendable interaction platform, which provides a foundation for assessing the multi-step interaction abilities of LLM-based agents in interacting with information user interfaces (InfoUI).
WebArenaÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib159" title="" class="ltx_ref">159</a>]</cite> has established a comprehensive website environment encompassing common domains. This environment serves as a platform for evaluating agents in an end-to-end manner, assessing the functional correctness of completed task.
GentBenchÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib146" title="" class="ltx_ref">146</a>]</cite> is a benchmark designed to evaluate various capabilities of agents, including reasoning, safety, efficiency, and more. Additionally, it supports the assessment of agentsâ€™ competence in utilizing tools to address complex tasks.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para ltx_noindent">
<p id="S4.SS2.p6.1" class="ltx_p">In summary, objective evaluation enables quantitative assessment of LLM-based agent capabilities through metrics like task success, human similarity, efficiency, and ablation studies. A diverse toolbox of objective techniques has emerged targeted at different competencies, from environmental simulation to social evaluation. While current techniques have limitations in measuring general capabilities, objective evaluation provides crucial insights complementing human assessment. Continued progress in objective evaluation benchmarks and methodology will further advance the development and understanding of LLM-based autonomous agents.</p>
</div>
<div id="S4.SS2.p7" class="ltx_para ltx_noindent">
<p id="S4.SS2.p7.1" class="ltx_p">In the above sections, we introduce both subjective and objective evaluation strategies for LLM-based autonomous agents.
The evaluation of the agents play significant roles in this domain. However, both subjective and objective evaluation have their own strengths and weakness. Maybe, in practice, they should be combined together to comprehensively evaluate the agents.
We summarize the correspondence between the previous work and these evaluation strategies in TableÂ <a href="#S4.T3" title="Table 3 â€£ 4.1 Subjective Evaluation â€£ 4 LLM-based Autonomous Agent Evaluation â€£ A Survey on Large Language Model based Autonomous Agents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Surveys</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">With the vigorous development of large language models, numerous comprehensive surveys have emerged, providing detailed insights into various aspects. Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib157" title="" class="ltx_ref">157</a>]</cite> extensively introduces the background, main findings, and mainstream technologies of LLMs, encompassing a vast array of existing works. On the other hand, Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib148" title="" class="ltx_ref">148</a>]</cite> primarily focus on the applications of LLMs in various downstream tasks and the challenges associated with their deployment. Aligning LLMs with human intelligence is an active area of research to address concerns such as biases and illusions. Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib136" title="" class="ltx_ref">136</a>]</cite> have compiled existing techniques for human alignment, including data collection and model training methodologies. Reasoning is a crucial aspect of intelligence, influencing decision-making, problem-solving, and other cognitive abilities.
Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> presents the current state of research on LLMsâ€™ reasoning abilities, exploring approaches to improve and evaluate their reasoning skills. Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> propose that language models can be enhanced with reasoning capabilities and the ability to utilize tools, termed Augmented Language Models (ALMs). They conduct a comprehensive review of the latest advancements in ALMs. As the utilization of large-scale models becomes more prevalent, evaluating their performance is increasingly critical.
Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> shed light on evaluating LLMs, addressing what to evaluate, where to evaluate, and how to assess their performance in downstream tasks and societal impact.
Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> also discusses the capabilities and limitations of LLMs in various downstream tasks.
The aforementioned research encompasses various aspects of large models, including training, application, and evaluation. However, prior to this paper, no work has specifically focused on the rapidly emerging and highly promising field of LLM-based Agents. In this study, we have compiled 100 relevant works on LLM-based Agents, covering their construction, applications, and evaluation processes.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Challenges</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">While previous work for LLM-based autonomous AI agent has shown many promising directions, this field is still at its initial stage, and many challenges exist on its development road.
In the follow, we present several important challenges.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Role-playing Capability</h3>

<div id="S6.SS1.p1" class="ltx_para ltx_noindent">
<p id="S6.SS1.p1.1" class="ltx_p">Different from traditional LLMs, AI agent usually has to play as specific roles (<em id="S6.SS1.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, program coder, researcher and chemist) for accomplishing different tasks.
Thus, the capability of the agent for role-playing is very important. While for many common roles (<em id="S6.SS1.p1.1.2" class="ltx_emph ltx_font_italic">e.g.</em>, movie reviewers), LLMs can well simulate them, there are still many roles and aspects that LLMs can be hard to capture.
To begin with, LLMs are usually trained based on web-corpus, thus for the roles which are seldom discussed on the web or the newly emerging roles, LLMs may not simulate them well.
In addition, previous researchÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> has shown that existing LLMs may not well model the human cognitive psychology characters, leading to the lack of self-awareness in conversation scenarios.
Potential solution to these problems may fine-tuning LLMs or carefully designing the agent prompts/architecturesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>.
For example, one can firstly collect real-human data for uncommon roles or psychology characters, and then leverage it to fine-tune LLMs.
However, how to ensure that fine-tuned model still perform well for the common roles may pose further challenges.
Beyond fine-tuning, one can also design tailored agent prompts/architectures to enhance the capability of LLM on role-playing.
However, finding the optimal prompts/architectures is not easy, since their designing spaces are too large.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Generalized Human Alignment</h3>

<div id="S6.SS2.p1" class="ltx_para ltx_noindent">
<p id="S6.SS2.p1.1" class="ltx_p">Human alignment has been discussed a lot for traditional LLMs.
In the field of autonomous AI agent, especially when the agents are leveraged for simulation, we believe this concept should be discussed more in depth.
In order to better serve human-beings, traditional LLMs are usually fine-tuned to be aligned with correct human values, for example, the agent should not plan to make a bomb for avenging society.
However, when the agents are leveraged for real-world simulation, an ideal simulator should be able to honestly depict diverse human traits, including the ones with incorrect values.
Actually, simulating the human negative aspects can be even more important, since an important goal of simulation is to discover and solve problems, and without negative aspects means no problem to be solved.
For example, to simulate the real-world society, we may have to allow the agent to plan for making a bomb, and observe how it will act to implement the plan as well as the influence of its behaviors.
Based on these observations, people can make better actions to stop similar behaviors in real-world society.
Inspired by the above case, maybe an important problem for agent-based simulation is how to conduct generalized human alignment, that is, for different purposes and applications, the agent should be able to align with diverse human values.
However, existing powerful LLMs including ChatGPT and GPT-4 are mostly aligned with unified human values.
Thus, an interesting direction is how to â€œrealignâ€ these models by designing proper prompting strategies.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Prompt Robustness</h3>

<div id="S6.SS3.p1" class="ltx_para ltx_noindent">
<p id="S6.SS3.p1.1" class="ltx_p">To ensure rational behavior in agents, designers often incorporate additional modules, such as memory and planning modules, into LLMs. However, the inclusion of these modules necessitates the development of more prompts in order to facilitate consistent operation and effective communication. Previous researchÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib162" title="" class="ltx_ref">162</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> has highlighted the lack of robustness in prompts for LLMs, as even minor alterations can yield substantially different outcomes. This issue becomes more pronounced when constructing autonomous agents, as they encompass not a single prompt but a prompt framework that considers all modules, wherein the prompt for one module has the potential to influence others.
Moreover, the prompt frameworks can vary significantly across different LLMs. Developing a unified and robust prompt framework that can be applied to various LLMs is an important yet unresolved issue.
There are two potential solutions to the aforementioned problems: (1) manually crafting the essential prompt elements through trial and error, or (2) automatically generating prompts using GPT.</p>
</div>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Hallucination</h3>

<div id="S6.SS4.p1" class="ltx_para ltx_noindent">
<p id="S6.SS4.p1.1" class="ltx_p">Hallucination poses a fundamental challenge for LLMs, wherein the model erroneously outputs false information confidently. This issue is also prevalent in autonomous agents. For instance, inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>, it was observed that when confronted with simplistic instructions during code generation tasks, the agent may exhibit hallucinatory behavior. Hallucination can lead to serious consequences such as incorrect or misleading code, security risks, and ethical issues <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>. To address this problem, one possible approach is to incorporate human correction feedback within the loop of human-agent interaction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>.
More discussions on the hallucination problem can be seen inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib157" title="" class="ltx_ref">157</a>]</cite>.</p>
</div>
</section>
<section id="S6.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5 </span>Knowledge Boundary</h3>

<div id="S6.SS5.p1" class="ltx_para ltx_noindent">
<p id="S6.SS5.p1.1" class="ltx_p">An important application of autonomous AI Agent is to simulate different real-world human behaviorsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite>.
The study of human simulation has a long history, and the recent surge in interest can be attributed to the remarkable advancements made by LLMs, which have demonstrated significant capabilities in simulating human behavior.
However, it is important to recognize that the power of LLMs may not always be advantageous. Specifically, an ideal simulation should accurately replicate human knowledge. In this regard, LLMs can exhibit excessive power, as they are trained on an extensive corpus of web knowledge that surpasses the scope of ordinary individuals.
The immense capabilities of LLMs can significantly impact the effectiveness of simulations. For instance, when attempting to simulate user selection behaviors for various movies, it is crucial to ensure that LLMs assume a position of having no prior knowledge of these movies. However, there is a possibility that LLMs have already acquired information about these movies. Without implementing appropriate strategies, LLMs may make decisions based on their extensive knowledge, even though real-world users would not have access to the contents of these movies beforehand.
Based on the above example, we may conclude that for building believable agent simulation environment, an important problem is how to constrain the utilization of user-unknown knowledge of LLM.</p>
</div>
</section>
<section id="S6.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.6 </span>Efficiency</h3>

<div id="S6.SS6.p1" class="ltx_para ltx_noindent">
<p id="S6.SS6.p1.1" class="ltx_p">Because of its auto-regressive architecture, LLMs typically have slow inference speeds.
However, the agent may need to query LLMs for each action multiple times, such as extracting information from the memory module, make plans before taking actions and so on.
Consequently, the efficiency of agent actions is greatly affected by the speed of LLM inference.
Deploying multiple agents with the same API key can further significantly increase the time cost.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para ltx_noindent">
<p id="S7.p1.1" class="ltx_p">In this survey, we systematically summarize existing research in the field of LLM-based autonomous agents.
We present and review these studies from three aspects
including the construction, application, and evaluation of the agents.
For each of these aspects, we provide a detailed taxonomy to draw connections among the existing research, summarizing the major techniques and their development histories.
In addition to reviewing the previous work, we also propose several challenges in this field, which are expected to guide potential future directions.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
GatiÂ V Aher, RosaÂ I Arriaga, and AdamÂ Tauman Kalai.

</span>
<span class="ltx_bibblock">Using large language models to simulate multiple humans and replicate
human subject studies.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
337â€“371. PMLR, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron
David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman,
etÂ al.

</span>
<span class="ltx_bibblock">Do as i can, not as i say: Grounding language in robotic affordances.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2204.01691</span>, 2022.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Elif Akata, Lion Schulz, Julian Coda-Forno, SeongÂ Joon Oh, Matthias Bethge, and
Eric Schulz.

</span>
<span class="ltx_bibblock">Playing repeated games with large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.16867</span>, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Model card and evaluations for claude models.

</span>
<span class="ltx_bibblock"><a href="https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf?ref=maginative.com" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf?ref=maginative.com</a>,
2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
LisaÂ P Argyle, EthanÂ C Busby, Nancy Fulda, JoshuaÂ R Gubler, Christopher
Rytting, and David Wingate.

</span>
<span class="ltx_bibblock">Out of one, many: Using language models to simulate human samples.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Political Analysis</span>, 31(3):337â€“351, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
ChristopherÂ A Bail.

</span>
<span class="ltx_bibblock">Can generative AI improve social science?

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">SocArXiv</span>, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
DaniilÂ A Boiko, Robert MacKnight, and Gabe Gomes.

</span>
<span class="ltx_bibblock">Emergent autonomous scientific research capabilities of large
language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.05332</span>, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
AndresÂ M Bran, Sam Cox, AndrewÂ D White, and Philippe Schwaller.

</span>
<span class="ltx_bibblock">ChemCrow: Augmenting large-language models with chemistry tools.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.05376</span>, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
TomÂ B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
etÂ al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2005.14165</span>, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou.

</span>
<span class="ltx_bibblock">Large language models as tool makers.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.17126</span>, 2023.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Kranti Chalamalasetti, Jana GÃ¶tze, Sherzod Hakimov, Brielen Madureira,
Philipp Sadler, and David Schlangen.

</span>
<span class="ltx_bibblock">clembench: Using game play to evaluate chat-optimized language models
as conversational agents.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.13455</span>, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang,
Jie Fu, and Zhiyuan Liu.

</span>
<span class="ltx_bibblock">ChatEval: Towards better LLM-based evaluators through multi-agent
debate.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2308.07201</span>, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
TylerÂ A Chang and BenjaminÂ K Bergen.

</span>
<span class="ltx_bibblock">Language model behavior: A comprehensive survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.11504</span>, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Yupeng Chang, XuÂ Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang,
Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, etÂ al.

</span>
<span class="ltx_bibblock">A survey on evaluation of large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.03109</span>, 2023.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Harrison Chase.

</span>
<span class="ltx_bibblock">langchain.

</span>
<span class="ltx_bibblock"><a href="https://docs.langchain.com/docs/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://docs.langchain.com/docs/</a>, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Angelica Chen, Jason Phang, Alicia Parrish, Vishakh Padmakumar, Chen Zhao,
SamuelÂ R Bowman, and Kyunghyun Cho.

</span>
<span class="ltx_bibblock">Two failures of self-consistency in the multi-step reasoning of
LLMs.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.14279</span>, 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Liting Chen, LuÂ Wang, Hang Dong, Yali Du, Jie Yan, Fangkai Yang, Shuang Li,
PuÂ Zhao, SiÂ Qin, Saravan Rajmohan, etÂ al.

</span>
<span class="ltx_bibblock">Introspective tips: Large language model for in-context decision
making.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.11598</span>, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde deÂ Oliveira
Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
Brockman, etÂ al.

</span>
<span class="ltx_bibblock">Evaluating large language models trained on code.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2107.03374</span>, 2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Xinshi Chen, Shuang Li, Hui Li, Shaohua Jiang, Yuan Qi, and LeÂ Song.

</span>
<span class="ltx_bibblock">Generative adversarial user model for reinforcement learning based
recommendation system.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
1052â€“1061. PMLR, 2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, WayneÂ Xin Zhao, and Ji-Rong
Wen.

</span>
<span class="ltx_bibblock">ChatCoT: Tool-augmented chain-of-thought reasoning on chat-based
large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.14323</span>, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Minje Choi, Jiaxin Pei, Sagar Kumar, Chang Shu, and David Jurgens.

</span>
<span class="ltx_bibblock">Do LLMs understand social knowledge? evaluating the sociability of
large language models with socket benchmark.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.14938</span>, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
CÃ©dric Colas, Laetitia Teodorescu, Pierre-Yves Oudeyer, Xingdi Yuan, and
Marc-Alexandre CÃ´tÃ©.

</span>
<span class="ltx_bibblock">Augmenting autotelic agents with large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.12487</span>, 2023.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and LiÂ Yuan.

</span>
<span class="ltx_bibblock">ChatLaw: Open-source legal large language model with integrated
external knowledge bases.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2306.16092</span>, 2023.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Gautier Dagan, Frank Keller, and Alex Lascarides.

</span>
<span class="ltx_bibblock">Dynamic planning with a LLM.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2308.06391</span>, 2023.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Ishita Dasgupta, Christine Kaeser-Chen, Kenneth Marino, Arun Ahuja, Sheila
Babayan, Felix Hill, and Rob Fergus.

</span>
<span class="ltx_bibblock">Collaborating with language models for embodied reasoning.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.00763</span>, 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Xiang Deng, YuÂ Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan
Sun, and YuÂ Su.

</span>
<span class="ltx_bibblock">Mind2web: Towards a generalist agent for the web.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2306.06070</span>, 2023.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and
Karthik Narasimhan.

</span>
<span class="ltx_bibblock">Toxicity in ChatGPT: Analyzing persona-assigned language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.05335</span>, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Norman DiÂ Palo, Arunkumar Byravan, Leonard Hasenclever, Markus Wulfmeier,
Nicolas Heess, and Martin Riedmiller.

</span>
<span class="ltx_bibblock">Towards a unified agent with foundation models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Workshop on Reincarnating Reinforcement Learning at ICLR
2023</span>, 2023.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Yihong Dong, Xue Jiang, Zhi Jin, and GeÂ Li.

</span>
<span class="ltx_bibblock">Self-collaboration code generation via ChatGPT.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.07590</span>, 2023.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth
Ke, Kevin Liu, Linda Chen, Sunny Tran, Newman Cheng, Roman Wang, Nikhil
Singh, TaylorÂ L. Patti, Jayson Lynch, Avi Shporer, Nakul Verma, Eugene Wu,
and Gilbert Strang.

</span>
<span class="ltx_bibblock">A neural network solves, explains, and generates university math
problems by program synthesis and few-shot learning at human level.

</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Proceedings of the National Academy of Sciences</span>, 119(32), aug
2022.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Yilun Du, Shuang Li, Antonio Torralba, JoshuaÂ B Tenenbaum, and Igor Mordatch.

</span>
<span class="ltx_bibblock">Improving factuality and reasoning in language models through
multiagent debate.

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.14325</span>, 2023.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
AlexÂ MacCaw etÂ al.

</span>
<span class="ltx_bibblock">WorkGPT.

</span>
<span class="ltx_bibblock"><a href="https://github.com/team-openpm/workgpt" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/team-openpm/workgpt</a>, 2023.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
AntonÂ Osika etÂ al.

</span>
<span class="ltx_bibblock">GPT engineer.

</span>
<span class="ltx_bibblock"><a href="https://github.com/AntonOsika/gpt-engineer" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/AntonOsika/gpt-engineer</a>, 2023.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
AssafÂ Elovic etÂ al.

</span>
<span class="ltx_bibblock">GPT-researcher.

</span>
<span class="ltx_bibblock"><a href="https://github.com/assafelovic/gpt-researcher" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/assafelovic/gpt-researcher</a>, 2023.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Chen etÂ al.

</span>
<span class="ltx_bibblock">Agentverse.

</span>
<span class="ltx_bibblock"><a href="https://github.com/OpenBMB/AgentVerse" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/OpenBMB/AgentVerse</a>, 2023.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Chen etÂ al.

</span>
<span class="ltx_bibblock">Xlang.

</span>
<span class="ltx_bibblock"><a href="https://github.com/xlang-ai/xlang" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/xlang-ai/xlang</a>, 2023.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Enricoros etÂ al.

</span>
<span class="ltx_bibblock">Miniagi.

</span>
<span class="ltx_bibblock"><a href="https://github.com/muellerberndt/mini-agi" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/muellerberndt/mini-agi</a>, 2023.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Eumemic etÂ al.

</span>
<span class="ltx_bibblock">Ai-legion.

</span>
<span class="ltx_bibblock"><a href="https://github.com/eumemic/ai-legion" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/eumemic/ai-legion</a>, 2023.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
FayazÂ Rahman etÂ al.

</span>
<span class="ltx_bibblock">LoopGPT.

</span>
<span class="ltx_bibblock"><a href="https://github.com/farizrahman4u/loopgpt" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/farizrahman4u/loopgpt</a>, 2023.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
JoshÂ XT etÂ al.

</span>
<span class="ltx_bibblock">Agixt.

</span>
<span class="ltx_bibblock"><a href="https://github.com/Josh-XT/AGiXT" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Josh-XT/AGiXT</a>, 2023.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
MelihÂ Unsal etÂ al.

</span>
<span class="ltx_bibblock">DemoGPT.

</span>
<span class="ltx_bibblock"><a href="https://github.com/melih-unsal/DemoGPT" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/melih-unsal/DemoGPT</a>, 2023.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Nakajima etÂ al.

</span>
<span class="ltx_bibblock">Babyagi.

</span>
<span class="ltx_bibblock"><a href="https://github.com/yoheinakajima" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/yoheinakajima</a>, 2023.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Reworkd etÂ al.

</span>
<span class="ltx_bibblock">AgentGPT.

</span>
<span class="ltx_bibblock"><a href="https://github.com/reworkd/AgentGPT" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/reworkd/AgentGPT</a>, 2023.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Swyxio etÂ al.

</span>
<span class="ltx_bibblock">Smolmodels.

</span>
<span class="ltx_bibblock"><a href="https://github.com/smol-ai/developer" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/smol-ai/developer</a>, 2023.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Torantulino etÂ al.

</span>
<span class="ltx_bibblock">Auto-GPT.

</span>
<span class="ltx_bibblock"><a href="https://github.com/Significant-Gravitas/Auto-GPT" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Significant-Gravitas/Auto-GPT</a>, 2023.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
TransformerOptimus etÂ al.

</span>
<span class="ltx_bibblock">Superagi.

</span>
<span class="ltx_bibblock"><a href="https://github.com/TransformerOptimus/SuperAGI" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/TransformerOptimus/SuperAGI</a>, 2023.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Hugging Face.

</span>
<span class="ltx_bibblock">transformers-agent.

</span>
<span class="ltx_bibblock"><a href="https://huggingface.co/docs/transformers/transformers_agents" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/docs/transformers/transformers_agents</a>,
2023.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Robert Feldt, Sungmin Kang, Juyeon Yoon, and Shin Yoo.

</span>
<span class="ltx_bibblock">Towards autonomous testing agents via conversational large language
models.

</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2306.05152</span>, 2023.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
KevinÂ A Fischer.

</span>
<span class="ltx_bibblock">Reflective linguistic programming (rlp): A stepping stone in
socially-aware agi (socialagi).

</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.12647</span>, 2023.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao, Huandong Wang,
Depeng Jin, and Yong Li.

</span>
<span class="ltx_bibblock">S3: Social-network simulation system with large language
model-empowered agents.

</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.14984</span>, 2023.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Yingqiang Ge, Wenyue Hua, Jianchao Ji, Juntao Tan, Shuyuan Xu, and Yongfeng
Zhang.

</span>
<span class="ltx_bibblock">OpenAGI: When llm meets domain experts.

</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.04370</span>, 2023.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Zorik Gekhman, Nadav Oved, Orgad Keller, Idan Szpektor, and Roi Reichart.

</span>
<span class="ltx_bibblock">On the robustness of dialogue history representation in
conversational question answering: a comprehensive study and a new
prompt-based method.

</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">Transactions of the Association for Computational Linguistics</span>,
11:351â€“366, 2023.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Maitrey Gramopadhye and Daniel Szafir.

</span>
<span class="ltx_bibblock">Generating executable action plans with environmentally-aware
language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2210.04964</span>, 2022.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Igor Grossmann, Matthew Feinberg, DawnÂ C Parker, NicholasÂ A Christakis,
PhilipÂ E Tetlock, and WilliamÂ A Cunningham.

</span>
<span class="ltx_bibblock">Ai and the transformation of social science research.

</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text ltx_font_italic">Science</span>, 380(6650):1108â€“1109, 2023.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.

</span>
<span class="ltx_bibblock">Soft actor-critic: Off-policy maximum entropy deep reinforcement
learning with a stochastic actor.

</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1801.01290</span>, 2018.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Sil Hamilton.

</span>
<span class="ltx_bibblock">Blind judgement: Agent-based supreme court modelling with GPT.

</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2301.05327</span>, 2023.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Shibo Hao, YiÂ Gu, Haodi Ma, JoshuaÂ Jiahua Hong, Zhen Wang, DaisyÂ Zhe Wang, and
Zhiting Hu.

</span>
<span class="ltx_bibblock">Reasoning with language model is planning with world model.

</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.14992</span>, 2023.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang,
Steven KaÂ Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, etÂ al.

</span>
<span class="ltx_bibblock">MetaGPT: Meta programming for multi-agent collaborative framework.

</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2308.00352</span>, 2023.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
JohnÂ J Horton.

</span>
<span class="ltx_bibblock">Large language models as simulated economic agents: What can we learn
from homo silicus?

</span>
<span class="ltx_bibblock">Technical report, National Bureau of Economic Research, 2023.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Bin Hu, Chenyang Zhao, PuÂ Zhang, Zihao Zhou, Yuanhang Yang, Zenglin Xu, and Bin
Liu.

</span>
<span class="ltx_bibblock">Enabling intelligent interactions between an agent and an llm: A
reinforcement learning approach.

</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2306.03604</span>, 2023.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao.

</span>
<span class="ltx_bibblock">Chatdb: Augmenting LLMs with databases as their symbolic memory.

</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2306.03901</span>, 2023.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Jie Huang and Kevin Chen-Chuan Chang.

</span>
<span class="ltx_bibblock">Towards reasoning in large language models: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2212.10403</span>, 2022.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.

</span>
<span class="ltx_bibblock">Language models as zero-shot planners: Extracting actionable
knowledge for embodied agents.

</span>
<span class="ltx_bibblock">In <span id="bib.bib63.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
9118â€“9147. PMLR, 2022.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy
Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, etÂ al.

</span>
<span class="ltx_bibblock">Inner monologue: Embodied reasoning through planning with language
models.

</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2207.05608</span>, 2022.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni,
Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard
Grave.

</span>
<span class="ltx_bibblock">Few-shot learning with retrieval augmented language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2208.03299</span>, 2022.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Sajed Jalil, Suzzana Rafi, ThomasÂ D LaToza, Kevin Moran, and Wing Lam.

</span>
<span class="ltx_bibblock">ChatGPT and software testing education: Promises &amp; perils.

</span>
<span class="ltx_bibblock">In <span id="bib.bib66.1.1" class="ltx_text ltx_font_italic">2023 IEEE International Conference on Software Testing,
Verification and Validation Workshops (ICSTW)</span>, pages 4130â€“4137. IEEE, 2023.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,
YeÂ Jin Bang, Andrea Madotto, and Pascale Fung.

</span>
<span class="ltx_bibblock">Survey of hallucination in natural language generation.

</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text ltx_font_italic">ACM Computing Surveys</span>, 55(12):1â€“38, 2023.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
LeslieÂ Pack Kaelbling, MichaelÂ L Littman, and AndrewÂ W Moore.

</span>
<span class="ltx_bibblock">Reinforcement learning: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text ltx_font_italic">Journal of artificial intelligence research</span>, 4:237â€“285, 1996.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Sungmin Kang, Juyeon Yoon, and Shin Yoo.

</span>
<span class="ltx_bibblock">Large language models are few-shot testers: Exploring LLM-based
general bug reproduction.

</span>
<span class="ltx_bibblock">In <span id="bib.bib69.1.1" class="ltx_text ltx_font_italic">2023 IEEE/ACM 45th International Conference on Software
Engineering (ICSE)</span>, pages 2312â€“2323. IEEE, 2023.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Yeonghun Kang and Jihan Kim.

</span>
<span class="ltx_bibblock">Chatmof: An autonomous ai system for predicting and generating
metal-organic frameworks.

</span>
<span class="ltx_bibblock"><span id="bib.bib70.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2308.01423</span>, 2023.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir
Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, etÂ al.

</span>
<span class="ltx_bibblock">Mrkl systems: A modular, neuro-symbolic architecture that combines
large language models, external knowledge sources and discrete reasoning.

</span>
<span class="ltx_bibblock"><span id="bib.bib71.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2205.00445</span>, 2022.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Takeshi Kojima, ShixiangÂ Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
Iwasawa.

</span>
<span class="ltx_bibblock">Large language models are zero-shot reasoners.

</span>
<span class="ltx_bibblock"><span id="bib.bib72.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>,
35:22199â€“22213, 2022.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Grgur KovaÄ, RÃ©my Portelas, PeterÂ Ford Dominey, and Pierre-Yves
Oudeyer.

</span>
<span class="ltx_bibblock">The socialai school: Insights from developmental psychology towards
artificial socio-cultural agents.

</span>
<span class="ltx_bibblock"><span id="bib.bib73.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.07871</span>, 2023.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
DavidÂ MJ Lazer, Alex Pentland, DuncanÂ J Watts, Sinan Aral, Susan Athey, Noshir
Contractor, Deen Freelon, Sandra Gonzalez-Bailon, Gary King, Helen Margetts,
etÂ al.

</span>
<span class="ltx_bibblock">Computational social science: Obstacles and opportunities.

</span>
<span class="ltx_bibblock"><span id="bib.bib74.1.1" class="ltx_text ltx_font_italic">Science</span>, 369(6507):1060â€“1062, 2020.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Mina Lee, Megha Srivastava, Amelia Hardy, John Thickstun, Esin Durmus, Ashwin
Paranjape, Ines Gerard-Ursin, XiangÂ Lisa Li, Faisal Ladhak, Frieda Rong,
etÂ al.

</span>
<span class="ltx_bibblock">Evaluating human-language model interaction.

</span>
<span class="ltx_bibblock"><span id="bib.bib75.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2212.09746</span>, 2022.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Chao Li, Xing Su, Chao Fan, Haoying Han, Cong Xue, and Chunmo Zheng.

</span>
<span class="ltx_bibblock">Quantifying the impact of large language models on collective opinion
dynamics.

</span>
<span class="ltx_bibblock"><span id="bib.bib76.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2308.03313</span>, 2023.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Cheng Li, Jindong Wang, Kaijie Zhu, Yixuan Zhang, Wenxin Hou, Jianxun Lian, and
Xing Xie.

</span>
<span class="ltx_bibblock">Emotionprompt: Leveraging psychology for large language models
enhancement via emotional stimulus.

</span>
<span class="ltx_bibblock"><span id="bib.bib77.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.11760</span>, 2023.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Guohao Li, Hasan Abed AlÂ Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and
Bernard Ghanem.

</span>
<span class="ltx_bibblock">Camel: Communicative agents for" mind" exploration of large scale
language model society.

</span>
<span class="ltx_bibblock"><span id="bib.bib78.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.17760</span>, 2023.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Haonan Li, YuÂ Hao, Yizhuo Zhai, and Zhiyun Qian.

</span>
<span class="ltx_bibblock">The hitchhikerâ€™s guide to program analysis: A journey with large
language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib79.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2308.00245</span>, 2023.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and
Yongbin Li.

</span>
<span class="ltx_bibblock">Api-bank: A benchmark for tool-augmented LLMs.

</span>
<span class="ltx_bibblock"><span id="bib.bib80.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.08244</span>, 2023.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Siyu Li, Jin Yang, and Kui Zhao.

</span>
<span class="ltx_bibblock">Are you in a masquerade? exploring the behavior and impact of large
language model driven social bots in online social networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib81.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.10337</span>, 2023.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Yuxi Li.

</span>
<span class="ltx_bibblock">Deep reinforcement learning: An overview.

</span>
<span class="ltx_bibblock"><span id="bib.bib82.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1701.07274</span>, 2017.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu
Yang, Zhaopeng Tu, and Shuming Shi.

</span>
<span class="ltx_bibblock">Encouraging divergent thinking in large language models through
multi-agent debate.

</span>
<span class="ltx_bibblock"><span id="bib.bib83.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.19118</span>, 2023.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, LuÂ Lu, Zejun Ma,
and Zhoujun Li.

</span>
<span class="ltx_bibblock">Unleashing infinite-length input capacity for large-scale language
models with self-controlled memory system.

</span>
<span class="ltx_bibblock"><span id="bib.bib84.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.13343</span>, 2023.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Yuanzhi Liang, Linchao Zhu, and YiÂ Yang.

</span>
<span class="ltx_bibblock">Tachikuma: Understading complex interactions with multi-character and
novel objects by large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib85.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.12573</span>, 2023.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
TimothyÂ P Lillicrap, JonathanÂ J Hunt, Alexander Pritzel, Nicolas Heess, Tom
Erez, Yuval Tassa, David Silver, and Daan Wierstra.

</span>
<span class="ltx_bibblock">Continuous control with deep reinforcement learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib86.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1509.02971</span>, 2015.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
BillÂ Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu, Faeze
Brahman, Shiyu Huang, Chandra Bhagavatula, Yejin Choi, and Xiang Ren.

</span>
<span class="ltx_bibblock">Swiftsage: A generative agent with fast and slow thinking for complex
interactive tasks.

</span>
<span class="ltx_bibblock"><span id="bib.bib87.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.17390</span>, 2023.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Jessy Lin, Nicholas Tomlin, Jacob Andreas, and Jason Eisner.

</span>
<span class="ltx_bibblock">Decision-oriented dialogue for human-ai collaboration.

</span>
<span class="ltx_bibblock"><span id="bib.bib88.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.20076</span>, 2023.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, and Qin Chen.

</span>
<span class="ltx_bibblock">Agentsims: An open-source sandbox for large language model
evaluation.

</span>
<span class="ltx_bibblock"><span id="bib.bib89.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2308.04026</span>, 2023.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
BoÂ Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas,
and Peter Stone.

</span>
<span class="ltx_bibblock">LLM+P: Empowering large language models with optimal planning
proficiency.

</span>
<span class="ltx_bibblock"><span id="bib.bib90.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.11477</span>, 2023.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
Hao Liu, Carmelo Sferrazza, and Pieter Abbeel.

</span>
<span class="ltx_bibblock">Chain of hindsight aligns language models with feedback.

</span>
<span class="ltx_bibblock"><span id="bib.bib91.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.02676</span>, 3, 2023.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Ruibo Liu, Ruixin Yang, Chenyan Jia, GeÂ Zhang, Denny Zhou, AndrewÂ M Dai, Diyi
Yang, and Soroush Vosoughi.

</span>
<span class="ltx_bibblock">Training socially aligned language models in simulated human society.

</span>
<span class="ltx_bibblock"><span id="bib.bib92.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.16960</span>, 2023.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, YuÂ Gu,
Hangliang Ding, Kaiwen Men, Kejuan Yang, etÂ al.

</span>
<span class="ltx_bibblock">Agentbench: Evaluating LLMs as agents.

</span>
<span class="ltx_bibblock"><span id="bib.bib93.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2308.03688</span>, 2023.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
Zhiwei Liu, Weiran Yao, Jianguo Zhang, LeÂ Xue, Shelby Heinecke, Rithesh Murthy,
Yihao Feng, Zeyuan Chen, JuanÂ Carlos Niebles, Devansh Arpit, etÂ al.

</span>
<span class="ltx_bibblock">BOLAA: Benchmarking and orchestrating LLM-augmented autonomous
agents.

</span>
<span class="ltx_bibblock"><span id="bib.bib94.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2308.05960</span>, 2023.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
Zilin Ma, Yiyang Mei, and Zhaoyuan Su.

</span>
<span class="ltx_bibblock">Understanding the benefits and challenges of using large language
model-based conversational agents for mental well-being support.

</span>
<span class="ltx_bibblock"><span id="bib.bib95.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.15810</span>, 2023.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang.

</span>
<span class="ltx_bibblock">Memory-assisted prompt editing to improve GPT-3 after deployment.

</span>
<span class="ltx_bibblock">In <span id="bib.bib96.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing</span>, pages 2833â€“2861, 2022.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah
Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, etÂ al.

</span>
<span class="ltx_bibblock">Self-refine: Iterative refinement with self-feedback.

</span>
<span class="ltx_bibblock"><span id="bib.bib97.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.17651</span>, 2023.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
Roger McFarlane.

</span>
<span class="ltx_bibblock">A survey of exploration strategies in reinforcement learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib98.1.1" class="ltx_text ltx_font_italic">McGill University</span>, 3:17â€“18, 2018.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
Nikhil Mehta, Milagro Teruel, PatricioÂ Figueroa Sanz, Xin Deng, AhmedÂ Hassan
Awadallah, and Julia Kiseleva.

</span>
<span class="ltx_bibblock">Improving grounded language understanding in a collaborative
environment by interacting with agents through help feedback.

</span>
<span class="ltx_bibblock"><span id="bib.bib99.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.10750</span>, 2023.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
GrÃ©goire Mialon, Roberto DessÃ¬, Maria Lomeli, Christoforos Nalmpantis,
Ram Pasunuru, Roberta Raileanu, Baptiste RoziÃ¨re, Timo Schick, Jane
Dwivedi-Yu, Asli Celikyilmaz, etÂ al.

</span>
<span class="ltx_bibblock">Augmented language models: a survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib100.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.07842</span>, 2023.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, AndreiÂ A Rusu, Joel Veness,
MarcÂ G Bellemare, Alex Graves, Martin Riedmiller, AndreasÂ K Fidjeland, Georg
Ostrovski, etÂ al.

</span>
<span class="ltx_bibblock">Human-level control through deep reinforcement learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib101.1.1" class="ltx_text ltx_font_italic">Nature</span>, 518(7540):529â€“533, 2015.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich SchÃ¼tze.

</span>
<span class="ltx_bibblock">RET-LLM: Towards a general read-write memory for large language
models.

</span>
<span class="ltx_bibblock"><span id="bib.bib102.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.14322</span>, 2023.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
Rithesh Murthy, Shelby Heinecke, JuanÂ Carlos Niebles, Zhiwei Liu, LeÂ Xue,
Weiran Yao, Yihao Feng, Zeyuan Chen, Akash Gokul, Devansh Arpit, etÂ al.

</span>
<span class="ltx_bibblock">Rex: Rapid exploration and exploitation for ai agents.

</span>
<span class="ltx_bibblock"><span id="bib.bib103.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.08962</span>, 2023.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
Varun Nair, Elliot Schumacher, Geoffrey Tso, and Anitha Kannan.

</span>
<span class="ltx_bibblock">Dera: enhancing large language model completions with dialog-enabled
resolving agents.

</span>
<span class="ltx_bibblock"><span id="bib.bib104.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.17071</span>, 2023.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina
Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders,
etÂ al.

</span>
<span class="ltx_bibblock">WebGPT: Browser-assisted question-answering with human feedback.

</span>
<span class="ltx_bibblock"><span id="bib.bib105.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2112.09332</span>, 2021.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh
Hajishirzi, Sameer Singh, and Roy Fox.

</span>
<span class="ltx_bibblock">Do embodied agents dream of pixelated sheep?: Embodied decision
making using language guided world modelling.

</span>
<span class="ltx_bibblock"><span id="bib.bib106.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2301.12050</span>, 2023.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
Oluwatosin Ogundare, Srinath Madasu, and Nathanial Wiggins.

</span>
<span class="ltx_bibblock">Industrial engineering with large language models: A case study of
ChatGPTâ€™s performance on oil &amp; gas problems.

</span>
<span class="ltx_bibblock"><span id="bib.bib107.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.14354</span>, 2023.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">GPT-4 technical report, 2023.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
JoonÂ Sung Park, JosephÂ C. Oâ€™Brien, CarrieÂ J. Cai, MeredithÂ Ringel Morris, Percy
Liang, and MichaelÂ S. Bernstein.

</span>
<span class="ltx_bibblock">Generative agents: Interactive simulacra of human behavior.

</span>
<span class="ltx_bibblock">In <span id="bib.bib109.1.1" class="ltx_text ltx_font_italic">In the 36th Annual ACM Symposium on User Interface Software
and Technology (UIST â€™23)</span>, UIST â€™23, New York, NY, USA, 2023. Association
for Computing Machinery.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
JoonÂ Sung Park, Lindsay Popowski, Carrie Cai, MeredithÂ Ringel Morris, Percy
Liang, and MichaelÂ S Bernstein.

</span>
<span class="ltx_bibblock">Social simulacra: Creating populated prototypes for social computing
systems.

</span>
<span class="ltx_bibblock">In <span id="bib.bib110.1.1" class="ltx_text ltx_font_italic">Proceedings of the 35th Annual ACM Symposium on User
Interface Software and Technology</span>, pages 1â€“18, 2022.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
ShishirÂ G Patil, Tianjun Zhang, Xin Wang, and JosephÂ E Gonzalez.

</span>
<span class="ltx_bibblock">Gorilla: Large language model connected with massive apis.

</span>
<span class="ltx_bibblock"><span id="bib.bib111.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.15334</span>, 2023.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine
Bosselut, Robert West, and Boi Faltings.

</span>
<span class="ltx_bibblock">Refiner: Reasoning feedback on intermediate representations.

</span>
<span class="ltx_bibblock"><span id="bib.bib112.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.01904</span>, 2023.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan
Liu, and Maosong Sun.

</span>
<span class="ltx_bibblock">Communicative agents for software development.

</span>
<span class="ltx_bibblock"><span id="bib.bib113.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.07924</span>, 2023.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni
Zeng, Yufei Huang, Chaojun Xiao, Chi Han, etÂ al.

</span>
<span class="ltx_bibblock">Tool learning with foundation models.

</span>
<span class="ltx_bibblock"><span id="bib.bib114.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.08354</span>, 2023.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin,
Xin Cong, Xiangru Tang, Bill Qian, etÂ al.

</span>
<span class="ltx_bibblock">ToolLLM: Facilitating large language models to master 16000+
real-world apis.

</span>
<span class="ltx_bibblock"><span id="bib.bib115.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.16789</span>, 2023.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><span id="bib.bib116.1.1" class="ltx_text ltx_font_italic">OpenAI blog</span>, 1(8):9, 2019.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
ShreyasÂ Sundara Raman, Vanya Cohen, Eric Rosen, Ifrah Idrees, David Paulius,
and Stefanie Tellex.

</span>
<span class="ltx_bibblock">Planning with large language models via corrective re-prompting.

</span>
<span class="ltx_bibblock"><span id="bib.bib117.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2211.09935</span>, 2022.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du,
Shiwei Shi, Hangyu Mao, Xingyu Zeng, and Rui Zhao.

</span>
<span class="ltx_bibblock">TPTU: Task planning and tool usage of large language model-based
AI agents.

</span>
<span class="ltx_bibblock"><span id="bib.bib118.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2308.03427</span>, 2023.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria
Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.

</span>
<span class="ltx_bibblock">Toolformer: Language models can teach themselves to use tools.

</span>
<span class="ltx_bibblock"><span id="bib.bib119.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.04761</span>, 2023.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.

</span>
<span class="ltx_bibblock">Proximal policy optimization algorithms.

</span>
<span class="ltx_bibblock"><span id="bib.bib120.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1707.06347</span>, 2017.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
Dale Schuurmans.

</span>
<span class="ltx_bibblock">Memory augmented large language models are computationally universal.

</span>
<span class="ltx_bibblock"><span id="bib.bib121.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2301.04589</span>, 2023.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
Sivan Schwartz, Avi Yaeli, and Segev Shlomov.

</span>
<span class="ltx_bibblock">Enhancing trust in LLM-based AI automation agents: New
considerations and future challenges.

</span>
<span class="ltx_bibblock"><span id="bib.bib122.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2308.05391</span>, 2023.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
Yongliang Shen, Kaitao Song, XuÂ Tan, Dongsheng Li, Weiming Lu, and Yueting
Zhuang.

</span>
<span class="ltx_bibblock">HuggingGPT: Solving ai tasks with ChatGPT and its friends in
huggingface.

</span>
<span class="ltx_bibblock"><span id="bib.bib123.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.17580</span>, 2023.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis,
Luke Zettlemoyer, and Wen-tau Yih.

</span>
<span class="ltx_bibblock">Replug: Retrieval-augmented black-box language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib124.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2301.12652</span>, 2023.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan,
and Shunyu Yao.

</span>
<span class="ltx_bibblock">Reflexion: Language agents with verbal reinforcement learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib125.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.11366</span>, 2023.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan
Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg.

</span>
<span class="ltx_bibblock">Progprompt: Generating situated robot task plans using large language
models, 2022.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
ChanÂ Hee Song, Jiaman Wu, Clayton Washington, BrianÂ M Sadler, Wei-Lun Chao, and
YuÂ Su.

</span>
<span class="ltx_bibblock">LLM-Planner: Few-shot grounded planning for embodied agents with
large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib127.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2212.04088</span>, 2022.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
DÃ­dac SurÃ­s, Sachit Menon, and Carl Vondrick.

</span>
<span class="ltx_bibblock">ViperGPT: Visual inference via python execution for reasoning.

</span>
<span class="ltx_bibblock"><span id="bib.bib128.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.08128</span>, 2023.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
Melanie Swan, Takashi Kido, Eric Roland, and Renato PÂ dos Santos.

</span>
<span class="ltx_bibblock">Math agents: Computational infrastructure, mathematical embedding,
and genomics.

</span>
<span class="ltx_bibblock"><span id="bib.bib129.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.02502</span>, 2023.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric
Hambro, Faisal Azhar, etÂ al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib130.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.13971</span>, 2023.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
etÂ al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><span id="bib.bib131.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.09288</span>, 2023.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
Shangqing Tu, Chunyang Li, Jifan Yu, Xiaozhi Wang, Lei Hou, and Juanzi Li.

</span>
<span class="ltx_bibblock">ChatLog: Recording and analyzing ChatGPT across time.

</span>
<span class="ltx_bibblock"><span id="bib.bib132.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.14106</span>, 2023.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu,
Linxi Fan, and Anima Anandkumar.

</span>
<span class="ltx_bibblock">Voyager: An open-ended embodied agent with large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib133.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.16291</span>, 2023.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
Lei Wang, Jingsen Zhang, XuÂ Chen, Yankai Lin, Ruihua Song, WayneÂ Xin Zhao, and
Ji-Rong Wen.

</span>
<span class="ltx_bibblock">Recagent: A novel simulation paradigm for recommender systems.

</span>
<span class="ltx_bibblock"><span id="bib.bib134.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2306.02552</span>, 2023.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, EdÂ Chi, Sharan Narang,
Aakanksha Chowdhery, and Denny Zhou.

</span>
<span class="ltx_bibblock">Self-consistency improves chain of thought reasoning in language
models.

</span>
<span class="ltx_bibblock"><span id="bib.bib135.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2203.11171</span>, 2022.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang,
Lifeng Shang, Xin Jiang, and Qun Liu.

</span>
<span class="ltx_bibblock">Aligning large language models with human: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib136.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.12966</span>, 2023.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang.

</span>
<span class="ltx_bibblock">Describe, explain, plan and select: Interactive planning with large
language models enables open-world multi-task agents.

</span>
<span class="ltx_bibblock"><span id="bib.bib137.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.01560</span>, 2023.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, EdÂ Chi, QuocÂ V
Le, Denny Zhou, etÂ al.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language
models.

</span>
<span class="ltx_bibblock"><span id="bib.bib138.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>,
35:24824â€“24837, 2022.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
Jimmy Wei, Kurt Shuster, Arthur Szlam, Jason Weston, Jack Urbanek, and Mojtaba
Komeili.

</span>
<span class="ltx_bibblock">Multi-party chat: Conversational agents in group settings with humans
and models.

</span>
<span class="ltx_bibblock"><span id="bib.bib139.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.13835</span>, 2023.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
Ross Williams, Niyousha Hosseinichimeh, Aritra Majumdar, and Navid
Ghaffarzadegan.

</span>
<span class="ltx_bibblock">Epidemic modeling with generative agents.

</span>
<span class="ltx_bibblock"><span id="bib.bib140.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.04986</span>, 2023.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu,
Beibin Li, LiÂ Jiang, Xiaoyun Zhang, and Chi Wang.

</span>
<span class="ltx_bibblock">AutoGen: Enabling next-gen LLM applications via multi-agent
conversation framework.

</span>
<span class="ltx_bibblock"><span id="bib.bib141.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2308.08155</span>, 2023.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
Yue Wu, SoÂ Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Yuanzhi
Li, Tom Mitchell, and Shrimai Prabhumoye.

</span>
<span class="ltx_bibblock">Plan, eliminate, and trackâ€“language models are good teachers for
embodied agents.

</span>
<span class="ltx_bibblock"><span id="bib.bib142.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.02412</span>, 2023.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, and Haibin Yan.

</span>
<span class="ltx_bibblock">Embodied task planning with large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib143.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.01848</span>, 2023.

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
Yuchen Xia, Manthan Shenoy, Nasser Jazdi, and Michael Weyrich.

</span>
<span class="ltx_bibblock">Towards autonomous system: flexible modular production system
enhanced with large language model agents.

</span>
<span class="ltx_bibblock"><span id="bib.bib144.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.14721</span>, 2023.

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
Jiannan Xiang, Tianhua Tao, YiÂ Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and
Zhiting Hu.

</span>
<span class="ltx_bibblock">Language models meet world models: Embodied experiences enhance
language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib145.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.10626</span>, 2023.

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
Binfeng Xu, Xukun Liu, Hua Shen, Zeyu Han, Yuhan Li, Murong Yue, Zhiyuan Peng,
Yuchen Liu, Ziyu Yao, and Dongkuan Xu.

</span>
<span class="ltx_bibblock">Gentopia: A collaborative platform for tool-augmented LLMs.

</span>
<span class="ltx_bibblock"><span id="bib.bib146.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2308.04030</span>, 2023.

</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
Binfeng Xu, Zhiyuan Peng, Bowen Lei, Subhabrata Mukherjee, Yuchen Liu, and
Dongkuan Xu.

</span>
<span class="ltx_bibblock">Rewoo: Decoupling reasoning from observations for efficient augmented
language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib147.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.18323</span>, 2023.

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming
Jiang, Bing Yin, and Xia Hu.

</span>
<span class="ltx_bibblock">Harnessing the power of LLMs in practice: A survey on ChatGPT and
beyond.

</span>
<span class="ltx_bibblock"><span id="bib.bib148.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.13712</span>, 2023.

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.

</span>
<span class="ltx_bibblock">Webshop: Towards scalable real-world web interaction with grounded
language agents.

</span>
<span class="ltx_bibblock"><span id="bib.bib149.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>,
35:20744â€“20757, 2022.

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, ThomasÂ L Griffiths, Yuan Cao,
and Karthik Narasimhan.

</span>
<span class="ltx_bibblock">Tree of thoughts: Deliberate problem solving with large language
models.

</span>
<span class="ltx_bibblock"><span id="bib.bib150.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.10601</span>, 2023.

</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
and Yuan Cao.

</span>
<span class="ltx_bibblock">React: Synergizing reasoning and acting in language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib151.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2210.03629</span>, 2022.

</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
Weiran Yao, Shelby Heinecke, JuanÂ Carlos Niebles, Zhiwei Liu, Yihao Feng,
LeÂ Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, etÂ al.

</span>
<span class="ltx_bibblock">Retroformer: Retrospective large language agents with policy gradient
optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib152.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2308.02151</span>, 2023.

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, CharlesÂ Robert
Jankowski, Deqing Yang, and Yanghua Xiao.

</span>
<span class="ltx_bibblock">Distilling script knowledge from large language models for
constrained language planning.

</span>
<span class="ltx_bibblock"><span id="bib.bib153.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.05252</span>, 2023.

</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock">
Danyang Zhang, LuÂ Chen, Situo Zhang, Hongshen Xu, Zihan Zhao, and Kai Yu.

</span>
<span class="ltx_bibblock">Large language model is semi-parametric reinforcement learning agent.

</span>
<span class="ltx_bibblock"><span id="bib.bib154.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2306.07929</span>, 2023.

</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock">
Danyang Zhang, LuÂ Chen, Zihan Zhao, Ruisheng Cao, and Kai Yu.

</span>
<span class="ltx_bibblock">Mobile-Env: An evaluation platform and benchmark for interactive
agents in LLM era.

</span>
<span class="ltx_bibblock"><span id="bib.bib155.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.08144</span>, 2023.

</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock">
Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, JoshuaÂ B
Tenenbaum, Tianmin Shu, and Chuang Gan.

</span>
<span class="ltx_bibblock">Building cooperative embodied agents modularly with large language
models.

</span>
<span class="ltx_bibblock"><span id="bib.bib156.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.02485</span>, 2023.

</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock">
WayneÂ Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, etÂ al.

</span>
<span class="ltx_bibblock">A survey of large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib157.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.18223</span>, 2023.

</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock">
Wanjun Zhong, Lianghong Guo, Qiqi Gao, and Yanlin Wang.

</span>
<span class="ltx_bibblock">Memorybank: Enhancing large language models with long-term memory.

</span>
<span class="ltx_bibblock"><span id="bib.bib158.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.10250</span>, 2023.

</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock">
Shuyan Zhou, FrankÂ F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar,
Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, etÂ al.

</span>
<span class="ltx_bibblock">Webarena: A realistic web environment for building autonomous agents.

</span>
<span class="ltx_bibblock"><span id="bib.bib159.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.13854</span>, 2023.

</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock">
Wei Zhou, Xiangyu Peng, and Mark Riedl.

</span>
<span class="ltx_bibblock">Dialogue shaping: Empowering agents through npc interaction.

</span>
<span class="ltx_bibblock"><span id="bib.bib160.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.15833</span>, 2023.

</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock">
Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao
Huang, Bin Li, Lewei Lu, Xiaogang Wang, etÂ al.

</span>
<span class="ltx_bibblock">Ghost in the minecraft: Generally capable agents for open-world
enviroments via large language models with text-based knowledge and memory.

</span>
<span class="ltx_bibblock"><span id="bib.bib161.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.17144</span>, 2023.

</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock">
TerryÂ Yue Zhuo, Zhuang Li, Yujin Huang, Yuan-Fang Li, Weiqing Wang, Gholamreza
Haffari, and Fatemeh Shiri.

</span>
<span class="ltx_bibblock">On robustness of prompt-based semantic parsing with large pre-trained
language model: An empirical study on codex.

</span>
<span class="ltx_bibblock"><span id="bib.bib162.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2301.12868</span>, 2023.

</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[163]</span>
<span class="ltx_bibblock">
Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi
Yang.

</span>
<span class="ltx_bibblock">Can large language models transform computational social science?

</span>
<span class="ltx_bibblock"><span id="bib.bib163.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.03514</span>, 2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Aug 28 04:10:36 2023 by <a href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"></a>
</div></footer>
</div>
</body>
</html>
