<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents</title>
<!--Generated on Mon Aug 14 22:36:03 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/dginev/ar5iv-css@0.7.6/css/ar5iv.min.css" type="text/css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">BOLAA: <span id="id19.id1" class="ltx_text ltx_framed_underline">B</span>enchmarking and <span id="id20.id2" class="ltx_text ltx_framed_underline">O</span>rchestrating <span id="id21.id3" class="ltx_text ltx_framed_underline">L</span>LM-augmented <span id="id22.id4" class="ltx_text ltx_framed_underline">A</span>utonomous <span id="id23.id5" class="ltx_text ltx_framed_underline">A</span>gents</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhiwei Liu<math id="id1.1.m1.1" class="ltx_Math" alttext="{}^{\dagger}" display="inline"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1a" xref="id1.1.m1.1.1.cmml"></mi><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><ci id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span><span class="ltx_author_notes">zhiweiliu@salesforce.com
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id24.2.id1" class="ltx_text">Weiran Yao</span><math id="id2.1.m1.1" class="ltx_Math" alttext="{}^{\dagger}" display="inline"><semantics id="id2.1.m1.1a"><msup id="id2.1.m1.1.1" xref="id2.1.m1.1.1.cmml"><mi id="id2.1.m1.1.1a" xref="id2.1.m1.1.1.cmml"></mi><mo id="id2.1.m1.1.1.1" xref="id2.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="id2.1.m1.1b"><apply id="id2.1.m1.1.1.cmml" xref="id2.1.m1.1.1"><ci id="id2.1.m1.1.1.1.cmml" xref="id2.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="id2.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id25.2.id1" class="ltx_text">Jianguo Zhang</span><math id="id3.1.m1.1" class="ltx_Math" alttext="{}^{\dagger}" display="inline"><semantics id="id3.1.m1.1a"><msup id="id3.1.m1.1.1" xref="id3.1.m1.1.1.cmml"><mi id="id3.1.m1.1.1a" xref="id3.1.m1.1.1.cmml"></mi><mo id="id3.1.m1.1.1.1" xref="id3.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="id3.1.m1.1b"><apply id="id3.1.m1.1.1.cmml" xref="id3.1.m1.1.1"><ci id="id3.1.m1.1.1.1.cmml" xref="id3.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="id3.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id26.2.id1" class="ltx_text">Le Xue</span><math id="id4.1.m1.1" class="ltx_Math" alttext="{}^{\dagger}" display="inline"><semantics id="id4.1.m1.1a"><msup id="id4.1.m1.1.1" xref="id4.1.m1.1.1.cmml"><mi id="id4.1.m1.1.1a" xref="id4.1.m1.1.1.cmml"></mi><mo id="id4.1.m1.1.1.1" xref="id4.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="id4.1.m1.1b"><apply id="id4.1.m1.1.1.cmml" xref="id4.1.m1.1.1"><ci id="id4.1.m1.1.1.1.cmml" xref="id4.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id4.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="id4.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id27.2.id1" class="ltx_text">Shelby Heinecke</span><math id="id5.1.m1.1" class="ltx_Math" alttext="{}^{\dagger}" display="inline"><semantics id="id5.1.m1.1a"><msup id="id5.1.m1.1.1" xref="id5.1.m1.1.1.cmml"><mi id="id5.1.m1.1.1a" xref="id5.1.m1.1.1.cmml"></mi><mo id="id5.1.m1.1.1.1" xref="id5.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="id5.1.m1.1b"><apply id="id5.1.m1.1.1.cmml" xref="id5.1.m1.1.1"><ci id="id5.1.m1.1.1.1.cmml" xref="id5.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id5.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="id5.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id28.2.id1" class="ltx_text">Rithesh Murthy</span><math id="id6.1.m1.1" class="ltx_Math" alttext="{}^{\dagger}" display="inline"><semantics id="id6.1.m1.1a"><msup id="id6.1.m1.1.1" xref="id6.1.m1.1.1.cmml"><mi id="id6.1.m1.1.1a" xref="id6.1.m1.1.1.cmml"></mi><mo id="id6.1.m1.1.1.1" xref="id6.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="id6.1.m1.1b"><apply id="id6.1.m1.1.1.cmml" xref="id6.1.m1.1.1"><ci id="id6.1.m1.1.1.1.cmml" xref="id6.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id6.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="id6.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id29.2.id1" class="ltx_text">Yihao Feng</span><math id="id7.1.m1.1" class="ltx_Math" alttext="{}^{\dagger}" display="inline"><semantics id="id7.1.m1.1a"><msup id="id7.1.m1.1.1" xref="id7.1.m1.1.1.cmml"><mi id="id7.1.m1.1.1a" xref="id7.1.m1.1.1.cmml"></mi><mo id="id7.1.m1.1.1.1" xref="id7.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="id7.1.m1.1b"><apply id="id7.1.m1.1.1.cmml" xref="id7.1.m1.1.1"><ci id="id7.1.m1.1.1.1.cmml" xref="id7.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id7.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="id7.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id30.2.id1" class="ltx_text">Zeyuan Chen</span><math id="id8.1.m1.1" class="ltx_Math" alttext="{}^{\dagger}" display="inline"><semantics id="id8.1.m1.1a"><msup id="id8.1.m1.1.1" xref="id8.1.m1.1.1.cmml"><mi id="id8.1.m1.1.1a" xref="id8.1.m1.1.1.cmml"></mi><mo id="id8.1.m1.1.1.1" xref="id8.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="id8.1.m1.1b"><apply id="id8.1.m1.1.1.cmml" xref="id8.1.m1.1.1"><ci id="id8.1.m1.1.1.1.cmml" xref="id8.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id8.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="id8.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id31.2.id1" class="ltx_text">Juan Carlos Niebles</span><math id="id9.1.m1.1" class="ltx_Math" alttext="{}^{\dagger}" display="inline"><semantics id="id9.1.m1.1a"><msup id="id9.1.m1.1.1" xref="id9.1.m1.1.1.cmml"><mi id="id9.1.m1.1.1a" xref="id9.1.m1.1.1.cmml"></mi><mo id="id9.1.m1.1.1.1" xref="id9.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="id9.1.m1.1b"><apply id="id9.1.m1.1.1.cmml" xref="id9.1.m1.1.1"><ci id="id9.1.m1.1.1.1.cmml" xref="id9.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id9.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="id9.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id32.2.id1" class="ltx_text">Devansh Arpit</span><math id="id10.1.m1.1" class="ltx_Math" alttext="{}^{\dagger}" display="inline"><semantics id="id10.1.m1.1a"><msup id="id10.1.m1.1.1" xref="id10.1.m1.1.1.cmml"><mi id="id10.1.m1.1.1a" xref="id10.1.m1.1.1.cmml"></mi><mo id="id10.1.m1.1.1.1" xref="id10.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="id10.1.m1.1b"><apply id="id10.1.m1.1.1.cmml" xref="id10.1.m1.1.1"><ci id="id10.1.m1.1.1.1.cmml" xref="id10.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id10.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="id10.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id33.2.id1" class="ltx_text">Ran Xu</span><math id="id11.1.m1.1" class="ltx_Math" alttext="{}^{\dagger}" display="inline"><semantics id="id11.1.m1.1a"><msup id="id11.1.m1.1.1" xref="id11.1.m1.1.1.cmml"><mi id="id11.1.m1.1.1a" xref="id11.1.m1.1.1.cmml"></mi><mo id="id11.1.m1.1.1.1" xref="id11.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="id11.1.m1.1b"><apply id="id11.1.m1.1.1.cmml" xref="id11.1.m1.1.1"><ci id="id11.1.m1.1.1.1.cmml" xref="id11.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id11.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="id11.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id34.2.id1" class="ltx_text">Phil Mui</span><math id="id12.1.m1.1" class="ltx_Math" alttext="{}^{\diamond}" display="inline"><semantics id="id12.1.m1.1a"><msup id="id12.1.m1.1.1" xref="id12.1.m1.1.1.cmml"><mi id="id12.1.m1.1.1a" xref="id12.1.m1.1.1.cmml"></mi><mo id="id12.1.m1.1.1.1" xref="id12.1.m1.1.1.1.cmml">⋄</mo></msup><annotation-xml encoding="MathML-Content" id="id12.1.m1.1b"><apply id="id12.1.m1.1.1.cmml" xref="id12.1.m1.1.1"><ci id="id12.1.m1.1.1.1.cmml" xref="id12.1.m1.1.1.1">⋄</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id12.1.m1.1c">{}^{\diamond}</annotation><annotation encoding="application/x-llamapun" id="id12.1.m1.1d">start_FLOATSUPERSCRIPT ⋄ end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id35.2.id1" class="ltx_text">Huan Wang</span><math id="id13.1.m1.1" class="ltx_Math" alttext="{}^{\dagger\blacklozenge}" display="inline"><semantics id="id13.1.m1.1a"><msup id="id13.1.m1.1.1" xref="id13.1.m1.1.1.cmml"><mi id="id13.1.m1.1.1a" xref="id13.1.m1.1.1.cmml"></mi><mrow id="id13.1.m1.1.1.1" xref="id13.1.m1.1.1.1.cmml"><mi id="id13.1.m1.1.1.1.2" xref="id13.1.m1.1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="id13.1.m1.1.1.1.1" xref="id13.1.m1.1.1.1.1.cmml">†</mo><mi mathvariant="normal" id="id13.1.m1.1.1.1.3" xref="id13.1.m1.1.1.1.3.cmml">◆</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="id13.1.m1.1b"><apply id="id13.1.m1.1.1.cmml" xref="id13.1.m1.1.1"><apply id="id13.1.m1.1.1.1.cmml" xref="id13.1.m1.1.1.1"><ci id="id13.1.m1.1.1.1.1.cmml" xref="id13.1.m1.1.1.1.1">†</ci><csymbol cd="latexml" id="id13.1.m1.1.1.1.2.cmml" xref="id13.1.m1.1.1.1.2">absent</csymbol><ci id="id13.1.m1.1.1.1.3.cmml" xref="id13.1.m1.1.1.1.3">◆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id13.1.m1.1c">{}^{\dagger\blacklozenge}</annotation><annotation encoding="application/x-llamapun" id="id13.1.m1.1d">start_FLOATSUPERSCRIPT † ◆ end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id36.2.id1" class="ltx_text">Caiming Xiong</span><math id="id14.1.m1.1" class="ltx_Math" alttext="{}^{\dagger\blacklozenge}" display="inline"><semantics id="id14.1.m1.1a"><msup id="id14.1.m1.1.1" xref="id14.1.m1.1.1.cmml"><mi id="id14.1.m1.1.1a" xref="id14.1.m1.1.1.cmml"></mi><mrow id="id14.1.m1.1.1.1" xref="id14.1.m1.1.1.1.cmml"><mi id="id14.1.m1.1.1.1.2" xref="id14.1.m1.1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="id14.1.m1.1.1.1.1" xref="id14.1.m1.1.1.1.1.cmml">†</mo><mi mathvariant="normal" id="id14.1.m1.1.1.1.3" xref="id14.1.m1.1.1.1.3.cmml">◆</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="id14.1.m1.1b"><apply id="id14.1.m1.1.1.cmml" xref="id14.1.m1.1.1"><apply id="id14.1.m1.1.1.1.cmml" xref="id14.1.m1.1.1.1"><ci id="id14.1.m1.1.1.1.1.cmml" xref="id14.1.m1.1.1.1.1">†</ci><csymbol cd="latexml" id="id14.1.m1.1.1.1.2.cmml" xref="id14.1.m1.1.1.1.2">absent</csymbol><ci id="id14.1.m1.1.1.1.3.cmml" xref="id14.1.m1.1.1.1.3">◆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id14.1.m1.1c">{}^{\dagger\blacklozenge}</annotation><annotation encoding="application/x-llamapun" id="id14.1.m1.1d">start_FLOATSUPERSCRIPT † ◆ end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id37.5.id1" class="ltx_text">Silvio Savarese</span><math id="id15.1.m1.1" class="ltx_Math" alttext="{}^{\dagger\blacklozenge}" display="inline"><semantics id="id15.1.m1.1a"><msup id="id15.1.m1.1.1" xref="id15.1.m1.1.1.cmml"><mi id="id15.1.m1.1.1a" xref="id15.1.m1.1.1.cmml"></mi><mrow id="id15.1.m1.1.1.1" xref="id15.1.m1.1.1.1.cmml"><mi id="id15.1.m1.1.1.1.2" xref="id15.1.m1.1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="id15.1.m1.1.1.1.1" xref="id15.1.m1.1.1.1.1.cmml">†</mo><mi mathvariant="normal" id="id15.1.m1.1.1.1.3" xref="id15.1.m1.1.1.1.3.cmml">◆</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="id15.1.m1.1b"><apply id="id15.1.m1.1.1.cmml" xref="id15.1.m1.1.1"><apply id="id15.1.m1.1.1.1.cmml" xref="id15.1.m1.1.1.1"><ci id="id15.1.m1.1.1.1.1.cmml" xref="id15.1.m1.1.1.1.1">†</ci><csymbol cd="latexml" id="id15.1.m1.1.1.1.2.cmml" xref="id15.1.m1.1.1.1.2">absent</csymbol><ci id="id15.1.m1.1.1.1.3.cmml" xref="id15.1.m1.1.1.1.3">◆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id15.1.m1.1c">{}^{\dagger\blacklozenge}</annotation><annotation encoding="application/x-llamapun" id="id15.1.m1.1d">start_FLOATSUPERSCRIPT † ◆ end_FLOATSUPERSCRIPT</annotation></semantics></math> 
<br class="ltx_break"><math id="id16.2.m2.1" class="ltx_Math" alttext="{}^{\dagger}" display="inline"><semantics id="id16.2.m2.1a"><msup id="id16.2.m2.1.1" xref="id16.2.m2.1.1.cmml"><mi id="id16.2.m2.1.1a" xref="id16.2.m2.1.1.cmml"></mi><mo id="id16.2.m2.1.1.1" xref="id16.2.m2.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="id16.2.m2.1b"><apply id="id16.2.m2.1.1.cmml" xref="id16.2.m2.1.1"><ci id="id16.2.m2.1.1.1.cmml" xref="id16.2.m2.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id16.2.m2.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="id16.2.m2.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>Salesforce Research, USA
<br class="ltx_break"><math id="id17.3.m3.1" class="ltx_Math" alttext="{}^{\diamond}" display="inline"><semantics id="id17.3.m3.1a"><msup id="id17.3.m3.1.1" xref="id17.3.m3.1.1.cmml"><mi id="id17.3.m3.1.1a" xref="id17.3.m3.1.1.cmml"></mi><mo id="id17.3.m3.1.1.1" xref="id17.3.m3.1.1.1.cmml">⋄</mo></msup><annotation-xml encoding="MathML-Content" id="id17.3.m3.1b"><apply id="id17.3.m3.1.1.cmml" xref="id17.3.m3.1.1"><ci id="id17.3.m3.1.1.1.cmml" xref="id17.3.m3.1.1.1">⋄</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id17.3.m3.1c">{}^{\diamond}</annotation><annotation encoding="application/x-llamapun" id="id17.3.m3.1d">start_FLOATSUPERSCRIPT ⋄ end_FLOATSUPERSCRIPT</annotation></semantics></math>CTO Office, Salesforce, USA
<br class="ltx_break"><math id="id18.4.m4.1" class="ltx_Math" alttext="{}^{\blacklozenge}" display="inline"><semantics id="id18.4.m4.1a"><msup id="id18.4.m4.1.1" xref="id18.4.m4.1.1.cmml"><mi id="id18.4.m4.1.1a" xref="id18.4.m4.1.1.cmml"></mi><mi mathvariant="normal" id="id18.4.m4.1.1.1" xref="id18.4.m4.1.1.1.cmml">◆</mi></msup><annotation-xml encoding="MathML-Content" id="id18.4.m4.1b"><apply id="id18.4.m4.1.1.cmml" xref="id18.4.m4.1.1"><ci id="id18.4.m4.1.1.1.cmml" xref="id18.4.m4.1.1.1">◆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id18.4.m4.1c">{}^{\blacklozenge}</annotation><annotation encoding="application/x-llamapun" id="id18.4.m4.1d">start_FLOATSUPERSCRIPT ◆ end_FLOATSUPERSCRIPT</annotation></semantics></math>Corresponding Authors: <span id="id38.6.id2" class="ltx_text ltx_font_typewriter">{huan.wang, cxiong, ssavarese}@salesforce.com</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id39.id1" class="ltx_p">The massive successes of large language models (LLMs) encourage the emerging exploration of LLM-augmented Autonomous Agents (LAAs).
An LAA is able to generate actions with its core LLM and interact with environments, which facilitates the ability to resolve complex tasks by conditioning on past interactions such as observations and actions.
Since the investigation of LAA is still very recent, limited explorations are available.
Therefore, we provide a comprehensive comparison of LAA in terms of both agent architectures and LLM backbones.
Additionally, we propose a new strategy to orchestrate multiple LAAs such that each labor LAA focuses on one type of action, <span id="id39.id1.1" class="ltx_text ltx_font_italic">i.e.</span> BOLAA, where a controller manages the communication among multiple agents.
We conduct simulations on both decision-making and multi-step reasoning environments, which comprehensively justify the capacity of LAAs.
Our performance results provide quantitative suggestions for designing LAA architectures and the optimal choice of LLMs, as well as the compatibility of both.
We release our implementation code of LAAs to the public at <a href="https://github.com/salesforce/BOLAA" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/salesforce/BOLAA</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Recent booming successes of large language models (LLMs) <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib17" title="" class="ltx_ref">2023</a>; Touvron et al., <a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite> motivate emerging exploration of employing LLM to tackle various complex tasks <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib34" title="" class="ltx_ref">2023</a>)</cite>, amongst which
<span id="S1.p1.1.1" class="ltx_text ltx_font_bold">L</span>LM-augmented <span id="S1.p1.1.2" class="ltx_text ltx_font_bold">A</span>utonomous <span id="S1.p1.1.3" class="ltx_text ltx_font_bold">A</span>gents (LAAs) <cite class="ltx_cite ltx_citemacro_citep">(Shinn et al., <a href="#bib.bib24" title="" class="ltx_ref">2023</a>; Madaan et al., <a href="#bib.bib13" title="" class="ltx_ref">2023b</a>; Huang et al., <a href="#bib.bib8" title="" class="ltx_ref">2022</a>; Kim et al., <a href="#bib.bib9" title="" class="ltx_ref">2023</a>; Paul et al., <a href="#bib.bib20" title="" class="ltx_ref">2023</a>; Yao et al., <a href="#bib.bib31" title="" class="ltx_ref">2023a</a>)</cite> stand with most spotlights.
LAA extends the intelligence of LLM to sequential action executions, exhibiting superiority in interacting with environments and resolving complex tasks via collecting observations.
To name a few, BabyAGI<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a href="https://github.com/yoheinakajima/babyagi" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/yoheinakajima/babyagi</a></span></span></span> proposes an AI-powered task management system, which leverages OpenAI LLM<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a href="https://platform.openai.com/docs/api-reference" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://platform.openai.com/docs/api-reference</a></span></span></span> to create, prioritize, and execute tasks.
AutoGPT<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a href="https://github.com/Significant-Gravitas/Auto-GPT" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Significant-Gravitas/Auto-GPT</a></span></span></span> is another popular open-source LAA framework that enables the API calling capability of LLMs.
ReAct <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a href="#bib.bib31" title="" class="ltx_ref">2023a</a>)</cite> is a recently proposed LAA method to interact with environments then consecutively generate the next action.
Langchain<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a href="https://github.com/langchain-ai/langchain" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/langchain-ai/langchain</a></span></span></span> is a recently released open-source framework for developing LAA.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Due to the initial investigation, LAA is rather under-explored.
Firstly, the optimal agent architecture is undetermined.
ReAct <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a href="#bib.bib31" title="" class="ltx_ref">2023a</a>)</cite> prompts the agents with pre-defined examples such that the LLM learns to generate the next action via in-context learning.
Moreover, ReAct argues that an agent should have intermediate reasoning steps before action executions.
ReWOO <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite> introduces additional planning steps for LAA.
Langchain generalizes the ReAct agent with zero-shot tool usage ability.
Intrinsically, the optimal architecture of agents should be aligned with both tasks and the associated LLM backbone, which is less explored in the existing works.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">Secondly, understanding the efficacy of the existing LLMs in LAA is far from comprehensive. The existing preliminary works only compare the performances of a few LLM backbones.
ReAct adopts the PaLM <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et al., <a href="#bib.bib2" title="" class="ltx_ref">2022</a>)</cite> as the backbone LLM.
ReWOO employs OpenAI text-davinci-003 model for instruction-tuning Alpaca model <cite class="ltx_cite ltx_citemacro_citep">(Taori et al., <a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite> for agent planning.
MIND2Web <cite class="ltx_cite ltx_citemacro_citep">(Deng et al., <a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite> compares Flan-T5 and OpenAI GPT3.5/4 for generalist web agent.
Nevertheless, few current works comprehensively compare the performance of LAA with regard to various pre-trained LLMs.
A very recent work <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite> releases a benchmark for evaluating LLMs as Agents. Nevertheless, they fail to jointly consider the agent architectures along with their LLM backbones.
Selecting the optimal LLMs from both efficacy and efficiency perspectives advances the current exploration of LAA.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">Thirdly, the increasing complexity of tasks may require the orchestration of multiple agents.
ReWOO recently identifies that decoupling reasoning from observation improves the efficiency for LAA.
In this paper, we argue that as the task complexity increases, especially in open-domain environments,
it is better to coordinate multiple agents to complete one task.
For example, regarding the web navigation task, we could employ one <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">click agent</span> to interact with clickable buttons and request another <span id="S1.p4.1.2" class="ltx_text ltx_font_italic">search agent</span> to retrieve additional resources.
Nonetheless, there are few works discussing how to orchestrate multiple agents and investigating the impacts of orchestration.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">To address these research gaps, this paper proposes to comprehensively compare the performances of LAAs.
We dive deep into the agent architecture of LAAs and the LLM backbones.
Specifically, we construct agent benchmarks from the existing environments to evaluate the performances of various agent architectures built upon various LLM backbones.
The tasks in our agent benchmarks are associated with different task complexity levels, which enables the agent performance analyses w.r.t. task complexity.
Those agent architectures are designed to extensively verify the existing design choices.
Regarding the orchestration of multiple LAAs, we propose a novel LAA architecture BOLAA<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>For easy memorizing, we intentionally name it the same as paper title.</span></span></span>, which has a controller module on top of multiple collaborated agents, for enabling the selection and communication between multiple labor LAA.</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<p id="S1.p6.1" class="ltx_p">The contributions of this paper are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We develop 6 different LAA agent architecture. We combine them with various backbone LLMs to justify the designing intuition of LAA from prompting, self-thinking, and planning.
We also develop BOLAA for orchestrating multi-agent strategy, which enhances the action interaction ability of solo agents.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We conduct extensive experiments on both decision-making web navigation environment and knowledge reasoning task environment.
We report the performance in terms of final sparse rewards and intermediate recalls, which provides qualitative indications for the optimal choice of LAAs as well as their compatible LLMs.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i3.p1.1" class="ltx_p">BOLAA on the WebShop environment consistently yields the best performance compared with other LAA architectures.
Our results demonstrate that the importance of designing specialist agents to collaborate on resolving complex task, which should be as equally important as training a large LLM with high generalization ability.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Augmented Language Agent Architecture</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">The completion of a complex task typically entails multiple stages. An agent must possess an understanding of these stages and plan accordingly. Chain-of-Thoughts, also known as CoT <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite>, is a groundbreaking work that prompts the agent to deconstruct challenging reasoning tasks into smaller, more manageable steps. On the other hand, ReAct <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a href="#bib.bib31" title="" class="ltx_ref">2023a</a>)</cite> proposes leveraging this aptitude for reasoning and action within Language and Learning Models (LLMs) to foster interactive engagement with the environment, such as utilizing the Wikipedia search API, by mapping observations to the generation of reasoning and action traces or API calls in natural language. This agent architecture has given rise to various applications, including HuggingGPT <cite class="ltx_cite ltx_citemacro_citep">(Shen et al., <a href="#bib.bib22" title="" class="ltx_ref">2023</a>)</cite>, Generative Agents <cite class="ltx_cite ltx_citemacro_citep">(Park et al., <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>, WebGPT <cite class="ltx_cite ltx_citemacro_citep">(Nakano et al., <a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite>, AutoGPT <cite class="ltx_cite ltx_citemacro_citep">(Gravitas, <a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>, BabyAGI <cite class="ltx_cite ltx_citemacro_citep">(Nakajima, <a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite>, and Langchain <cite class="ltx_cite ltx_citemacro_citep">(Chase, <a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.1" class="ltx_p">However, these approaches neglect to incorporate valuable feedback, such as environment rewards, to enhance the agent’s behaviors, resulting in performances that rely solely on the quality of the pre-trained Language and Learning Model (LLM). Self-refine <cite class="ltx_cite ltx_citemacro_citep">(Madaan et al., <a href="#bib.bib12" title="" class="ltx_ref">2023a</a>)</cite> tackles this limitation by employing a single LLM as a generator, refiner, and provider of feedback, enabling iterative refinement of outputs. However, it is not specifically tailored for real-world task-based interaction with the environment. On the other hand, REX <cite class="ltx_cite ltx_citemacro_citep">(Murthy et al., <a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite> and RAP <cite class="ltx_cite ltx_citemacro_citep">(Hao et al., <a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite> repurpose the LLM to function as both a comprehensive world model and a reasoning agent. They incorporate Monte Carlo Tree Search for strategic exploration within the vast realm of reasoning with environment rewards. This approach facilitates effective navigation and decision-making in intricate domains. <cite class="ltx_cite ltx_citemacro_citet">Shinn et al. (<a href="#bib.bib24" title="" class="ltx_ref">2023</a>)</cite> presents Reflexion, a framework that equips agents with dynamic memory and self-reflection capabilities, enhancing their reasoning skills. Self-reflection plays a pivotal role, allowing autonomous agents to iteratively refine past actions, make improvements, and prevent repetitive errors. Recently, <cite class="ltx_cite ltx_citemacro_citet">Yao et al. (<a href="#bib.bib33" title="" class="ltx_ref">2023b</a>)</cite> proposes a framework, namely Retroformer, which leverages policy gradient optimization to align the agent’s behaviors with environment-specific rewards by learning a plug-in retrospective language model.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Web Agent</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">Web navigation is the foundation for humans to collect information and communicate.
Before the boom of LLM, previous endeavours <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib10" title="" class="ltx_ref">2018</a>; Shi et al., <a href="#bib.bib23" title="" class="ltx_ref">2017</a>)</cite> already explored how to train web agent in a web simulation environment.
Very recently, a series of works have been devoted to developing LAA to tackle complex web navigation tasks.
Though action space of web navigation is almost infinite due to numerous available elements online, these action can be divided into a few operation types, such as <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">click</span>, <span id="S2.SS2.p1.1.2" class="ltx_text ltx_font_italic">type</span> and <span id="S2.SS2.p1.1.3" class="ltx_text ltx_font_italic">select</span>.
MIND2Web <cite class="ltx_cite ltx_citemacro_citep">(Deng et al., <a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite> collects a web browser data to fine-tune LLM to generate executable actions, which functions as a Web LAA.
WebAgent <cite class="ltx_cite ltx_citemacro_citep">(Gur et al., <a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite> is able to decompose task instruction into sub-tasks, which directly generates executable python program for web navigation.
WebArena <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite> supports realistic tasks simulation for designing Web LAA.
Langchain and ChatGPT both provide convenient web plugin such that the LLM behaves as Web LAA.
We believe that the web navigation is the next fundamental task for LAA to shine its superiority.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Tool Agent</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">The evolution of LLM and their interactions with various tools has been a focal point of recent research. The concept of a “Tool Agent” encapsulates the idea of LLMs leveraging external tools to enhance their capabilities and solve complex tasks. One of the pioneering works in this domain is the introduction of “Gorilla” <cite class="ltx_cite ltx_citemacro_citep">(Patil et al., <a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite>.
This model is adept at writing API calls and exhibits the ability to adapt test-time document changes.
Another noteworthy work is the “ToolLLM” framework <cite class="ltx_cite ltx_citemacro_citep">(Qin et al., <a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite>.
This open-source framework incorporates LLMs to efficiently engage with a myriad of tools, particularly APIs, to execute intricate tasks. The framework encompasses ToolBench, an instruction-tuning dataset tailored for tool utilization
More recently, a paradigm shift in teaching LLMs to use new tools has been discussed in <cite class="ltx_cite ltx_citemacro_citep">(Hsieh et al., <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>, which champions the use of tool documentation.
The authors present empirical evidence suggesting that tool documentation offers detailed descriptions of tool usage, which is a more effective and scalable approach. Notably, their research indicates that zero-shot prompts, which are exclusively based on tool documentation, can rival the performance of few-shot prompts.
</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Agent Architectures</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">In this section, we compare various LAA architectures.
We first present how to design different solo LAA based on the intuition of existing work. We then present the our orchestration designing of multiple LAAs, <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">i.e.</span> BOLAA.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Solo Agents</h3>

<figure id="S3.F1" class="ltx_figure"><img src="x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="244" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The LAA architectures for Zeroshot-LAA (ZS-LAA), ZeroshotThink LAA (ZST-LAA) and ReAct LAA. ZS-LAA generates actions from LLM with zeroshot prompt. ZST-LAA extends ZS-LAA with self-think. ReAct LAA advances ZST-LAA with fewshot prompt. They all resolve a given task by interacting with environment via actions to collect observations. Better view in colors.</figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">Hereafter, we present 5 different LAAs.
Each type of LAA is able to interact with the environment with its own interaction strategy.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Zeroshot LAA</span> (ZS-LAA)
directly extends the LLM to be action executor.
Specifically, the prompt for LLMs to function as the action executor consists of detailed descriptions for those actions.
For example, if we prompt LAA to understand the <span id="S3.SS1.p2.1.2" class="ltx_text ltx_font_italic">click</span> action with “<span id="S3.SS1.p2.1.3" class="ltx_text ltx_font_italic">click: using this action to click observed [button], the clickable buttons are in [].</span>”, it may behave as a web navigation agent.
We present the architecture of ZS-LAA in Figure <a href="#S3.F1" title="Figure 1 ‣ 3.1 Solo Agents ‣ 3 Agent Architectures ‣ BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(a).
The working flow is as follows:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Initial step</span>: firstly, the ZS-LAA receives the task instruction and constructs the zeroshot prompt. Then, the LLM layer generates a possible response, which is parsed to output a feasible action. After that, the observation from environment is appended into the agent memory.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Working teps</span>: the agent checks whether the task is finished. If not, ZS-LAA retrieves the previous actions and observations from memory, and constructs the prompts for LLM to generate the next executable actions. ZS-LAA continues the working stage until reaching the maximum steps or completing the task.</p>
</div>
</li>
</ul>
<p id="S3.SS1.p2.2" class="ltx_p">ZS-LAA is a minimum LAA architecture. It enables the action generation ability of LLM via zeroshot prompt layer, which is easy to generalize to new environments and requires no examples.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">ZeroshotThink LAA</span> (ZST-LAA) is an extended version of ZS-LAA. Different from ZS-LAA, ZST-LAA has an additional self-think flow.
The architecture of ZST-LAA is presented in Figure <a href="#S3.F1" title="Figure 1 ‣ 3.1 Solo Agents ‣ 3 Agent Architectures ‣ BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(b), where we denote the self-think flow as in pink arrow lines.
Self-think is running in intermediate steps of action generations flow, which enables the Chain-of-Thought (CoT) reasoning ability.</p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para ltx_noindent">
<p id="S3.I2.i1.p1.1" class="ltx_p"><span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_italic">Self-think Step</span>: before generating the next action, ZST-LAA collect observations and previous actions to construct the <span id="S3.I2.i1.p1.1.2" class="ltx_text ltx_font_italic">think</span> prompt. Then, the <span id="S3.I2.i1.p1.1.3" class="ltx_text ltx_font_italic">thought</span> is stored into memory.</p>
</div>
</li>
</ul>
<p id="S3.SS1.p3.2" class="ltx_p">Self-think step is generally useful when given reasoning tasks.
Note that the think prompt is also in a zero-shot format, such as <span id="S3.SS1.p3.2.1" class="ltx_text ltx_font_italic">“think: using this action to plan your actions and reasoning”</span>.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.1" class="ltx_p"><span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_bold">ReAct LAA</span> additionally advances ZST-LAA in the prompt layer, where fewshot examples are provided.
The architecture of ReAct LAA is illustrated in Figure <a href="#S3.F1" title="Figure 1 ‣ 3.1 Solo Agents ‣ 3 Agent Architectures ‣ BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(c).
ReAct LAA is able to leverage successful running examples to improve the action generation ability of LLM and enhance the environment interaction of LAA, because those fewshot examples endows the in-context learning ability of LLM.
However, the drawback for ReAct LAA is that, due to the limited context length, fewer token spaces are available after the occupancy of fewshot examples in the prompt.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="290" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The LAA architectures for PlanAct LAA and PlanReAct LAA. </figcaption>
</figure>
<div id="S3.SS1.p5" class="ltx_para ltx_noindent">
<p id="S3.SS1.p5.1" class="ltx_p"><span id="S3.SS1.p5.1.1" class="ltx_text ltx_font_bold">PlanAct LAA</span> is designed to facilitate the planning ability of LAA. PlanAct LAA differs from ZS-LAA in two parts: 1) the planning flow and 2) the fewshot prompt.
The architecture is depicted in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1 Solo Agents ‣ 3 Agent Architectures ‣ BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
The planning flow is executed before the initial action generation step, which has additional plan prompt to construct the input for the core LLM.</p>
<ul id="S3.I3" class="ltx_itemize">
<li id="S3.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I3.i1.p1" class="ltx_para ltx_noindent">
<p id="S3.I3.i1.p1.1" class="ltx_p"><span id="S3.I3.i1.p1.1.1" class="ltx_text ltx_font_italic">Planning Step</span>: PlanAct LAA generates a plan for a given task before interacting with environments.
The plan is memorized and will be retrieved to construct prompts.</p>
</div>
</li>
</ul>
<p id="S3.SS1.p5.2" class="ltx_p">It is worth noting that the plan prompt in this paper is in fewshot way, which allows LAA to generate plans based on previous successful plans.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para ltx_noindent">
<p id="S3.SS1.p6.1" class="ltx_p"><span id="S3.SS1.p6.1.1" class="ltx_text ltx_font_bold">PlanReAct LAA</span> extends PlanAct LAA with additional self-think flow, which also enables the CoT ability.
The architecture of PlanReAct LAA is presented in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1 Solo Agents ‣ 3 Agent Architectures ‣ BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
Intuitively, since the Planning flow is executed before the LAA observes the environment, self-think flow alleviates the hallucination incurred from incorrect plans.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para ltx_noindent">
<p id="S3.SS1.p7.1" class="ltx_p">Next, we introduce our multi-agent orchestrating architecture, <span id="S3.SS1.p7.1.1" class="ltx_text ltx_font_italic">i.e.</span> BOLAA.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>BOLAA: Orchestrating Multiple Agents.</h3>

<figure id="S3.F3" class="ltx_figure"><img src="x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="225" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The BOLAA architecture, which employs a controller to orchestrate multiple LAAs.</figcaption>
</figure>
<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">Though the success of the existing LLMs in completing various language understanding tasks, plenty of issues are still under-explored, such as the context length constraints, in-context learning and generalization ability, and etc.
Hence, it is challenging to employ a solo LAA to complete all tasks, especially when tasks are of high complexity.
Therefore, we propose a new agent architecture for orchestrating multiple LAAs, which is illustrated in Figure <a href="#S3.F3" title="Figure 3 ‣ 3.2 BOLAA: Orchestrating Multiple Agents. ‣ 3 Agent Architectures ‣ BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
BOLAA has two main modules, the labor agents pool and the controller.
The labor agents pool manages multiple LAAs.
Each LAA may only focus on generating one type of actions.
For example, in the web navigation environment, we could establish <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">click</span> LAA and <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_italic">search</span> LAA.
In this way, the former only generates the next button to click, while the later only outputs search query, which divides a complex task into feasible tasks.
The controller is devised to selectively call LAAs from agents pool.
Controller has the agents selection layer for choosing the most relevant LAA to call.
Then, the controller constructs the message for the selected LAA and builds the communication.
After obtaining the response from the labor LAA, the controller parses it to an executable action and then interacts with the environment.
Note that we can also design those labor LAAs to be think/plan agent.
In this way, the self-think and plan work flows are also retained.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Environment Benchmark</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">We construct the evaluation benchmarks from two environments, <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">i.e.,</span> the WebShop <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a href="#bib.bib32" title="" class="ltx_ref">preprint</a>)</cite> and HotPotQA <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a href="#bib.bib30" title="" class="ltx_ref">2018</a>)</cite> with Wikipedia API usage <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a href="#bib.bib31" title="" class="ltx_ref">2023a</a>)</cite>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.2" class="ltx_p">WebShop is a recently proposed online shopping website environment with 1.18M real-world products and human instructions.
Each instruction is associated with one ground-truth product, and contains attribute requirements, <span id="S4.SS1.p2.2.1" class="ltx_text ltx_font_italic">e.g. I’m looking for a travel monopod camera tripod with quick release and easy to carry, and price lower than 130.00 dollars.</span>
This instruction includes 3 attribute requirements <span id="S4.SS1.p2.2.2" class="ltx_text ltx_font_italic">i.e.</span> “quick release”, “camera tripod” and “easy carry” attributes.
We define the complexity of an instruction using the number of attribute requirements.
Thus, this instruction example above is of complexity <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mn id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><cn type="integer" id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">3</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">3</annotation></semantics></math>.
We equally sample 150 instructions regarding each complexity level.
Since we have fewer than 150 instructions for complexity larger than 6, we only include instructions from complexity in <math id="S4.SS1.p2.2.m2.4" class="ltx_Math" alttext="\{1,2,\dots,6\}" display="inline"><semantics id="S4.SS1.p2.2.m2.4a"><mrow id="S4.SS1.p2.2.m2.4.5.2" xref="S4.SS1.p2.2.m2.4.5.1.cmml"><mo stretchy="false" id="S4.SS1.p2.2.m2.4.5.2.1" xref="S4.SS1.p2.2.m2.4.5.1.cmml">{</mo><mn id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">1</mn><mo id="S4.SS1.p2.2.m2.4.5.2.2" xref="S4.SS1.p2.2.m2.4.5.1.cmml">,</mo><mn id="S4.SS1.p2.2.m2.2.2" xref="S4.SS1.p2.2.m2.2.2.cmml">2</mn><mo id="S4.SS1.p2.2.m2.4.5.2.3" xref="S4.SS1.p2.2.m2.4.5.1.cmml">,</mo><mi mathvariant="normal" id="S4.SS1.p2.2.m2.3.3" xref="S4.SS1.p2.2.m2.3.3.cmml">…</mi><mo id="S4.SS1.p2.2.m2.4.5.2.4" xref="S4.SS1.p2.2.m2.4.5.1.cmml">,</mo><mn id="S4.SS1.p2.2.m2.4.4" xref="S4.SS1.p2.2.m2.4.4.cmml">6</mn><mo stretchy="false" id="S4.SS1.p2.2.m2.4.5.2.5" xref="S4.SS1.p2.2.m2.4.5.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.4b"><set id="S4.SS1.p2.2.m2.4.5.1.cmml" xref="S4.SS1.p2.2.m2.4.5.2"><cn type="integer" id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">1</cn><cn type="integer" id="S4.SS1.p2.2.m2.2.2.cmml" xref="S4.SS1.p2.2.m2.2.2">2</cn><ci id="S4.SS1.p2.2.m2.3.3.cmml" xref="S4.SS1.p2.2.m2.3.3">…</ci><cn type="integer" id="S4.SS1.p2.2.m2.4.4.cmml" xref="S4.SS1.p2.2.m2.4.4">6</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.4c">\{1,2,\dots,6\}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.2.m2.4d">{ 1 , 2 , … , 6 }</annotation></semantics></math>, which sums up to 900 tasks for benchmark evaluation in the WebShop environment.
In the WebShop environment, an agent operates either <span id="S4.SS1.p2.2.3" class="ltx_text ltx_font_smallcaps">search[query]</span> or <span id="S4.SS1.p2.2.4" class="ltx_text ltx_font_smallcaps">click[element]</span> actions to interact the environment, for evaluating the interactive decision making ability of LAA.
The observation from WebShop is simplified web browser, which includes the clickable buttons and associated page content.
LAA interacts with the WebShop environment as a web navigation agent.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p">HotPotQA with Wikipedia API is another environment considered in this paper,
which contains multi-hop questions answering tasks that requires reasoning
over two or more Wikipedia passages.
This simulation environment serves as a powerful tool for evaluating the multi-step planning and comprehension capabilities and information retrieval skills of AI models, ensuring they are proficient in sourcing reliable information from vast online resources. With its unique blend of real-world internet browsing scenarios and text analysis, HotpotQA is an invaluable asset for the advancement of augmented large language agent systems.
In HotPotQA environment, an agent has three types of actions, <span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_italic">i.e.</span>, <span id="S4.SS1.p3.1.2" class="ltx_text ltx_font_smallcaps">search[entity]</span>, <span id="S4.SS1.p3.1.3" class="ltx_text ltx_font_smallcaps">lookup[string]</span> and <span id="S4.SS1.p3.1.4" class="ltx_text ltx_font_smallcaps">finish[answer]</span> to interact with HotPotQA environment.
HotPotQA environment aims at evaluate the knowledge reasoning ability of LAA.
We randomly sample 100 questions from easy, medium and hard levels, which constitutes the final 300 benchmark questions for evaluating LAAs.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation Metrics</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">We mainly use the <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">reward</span> score in each environment to evaluate the performances of LAAs.
In the WebShop environment, the reward is defined as the attribute overlapping ratio between the bought item and ground truth item.
In HotPotQA environment, the reward is defined as the F1 score grading between agent answer and ground-truth answer. Additionally, we develop the <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_italic">Recall</span> performance for WebShop environment, which is defined as 1 if the ground truth item is retrieved and 0 if not during one task session.
The Recall is reported as the average recall scores across all tasks in WebShop environment.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>LLM Utilization</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p">The core component of LAA is the LLM backbone. We compare different LLMs with various choices of model size and context length.
We reported the results w.r.t. open LLM models such as fastchat-3b, vicuna-3b/13b/33b <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al., <a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite>, Llama-2-7b/13b/70b<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>All Llama-2 models are -chat-hf version.</span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite>, MPT-7b/30b <cite class="ltx_cite ltx_citemacro_citep">(Team, <a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite>, xgen-8k-7b, longchat-16k-7b/13b and OpenAI API LLMs, including text-davinci-003, gpt-3.5-turbo and
gpt-3.5-turbo-16k.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Decision-making Simulation</h3>

<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Average reward in the WebShop environment. Len denotes the maximum context length. <span id="S4.T1.5.1" class="ltx_text ltx_font_bold">Bold</span> results denote the best results in one row, <span id="S4.T1.6.2" class="ltx_text ltx_font_italic">i.e.</span> best LAA architecture w.r.t. one LLM. <span id="S4.T1.7.3" class="ltx_text ltx_framed_underline">Underline</span> results denote the best performance in one column, <span id="S4.T1.8.4" class="ltx_text ltx_font_italic">i.e.</span> best LLM regarding one LAA architecture.</figcaption>
<table id="S4.T1.9" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.9.1.1" class="ltx_tr">
<th id="S4.T1.9.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T1.9.1.1.1.1" class="ltx_text">LLM</span></th>
<th id="S4.T1.9.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T1.9.1.1.2.1" class="ltx_text">Len.</span></th>
<td id="S4.T1.9.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="6">LAA Architecture</td>
</tr>
<tr id="S4.T1.9.2.2" class="ltx_tr">
<td id="S4.T1.9.2.2.1" class="ltx_td ltx_align_center ltx_border_t">ZS</td>
<td id="S4.T1.9.2.2.2" class="ltx_td ltx_align_center ltx_border_t">ZST</td>
<td id="S4.T1.9.2.2.3" class="ltx_td ltx_align_center ltx_border_t">ReAct</td>
<td id="S4.T1.9.2.2.4" class="ltx_td ltx_align_center ltx_border_t">PlanAct</td>
<td id="S4.T1.9.2.2.5" class="ltx_td ltx_align_center ltx_border_t">PlanReAct</td>
<td id="S4.T1.9.2.2.6" class="ltx_td ltx_align_center ltx_border_t">BOLAA</td>
</tr>
<tr id="S4.T1.9.3.3" class="ltx_tr">
<th id="S4.T1.9.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">fastchat-t5-3b</th>
<th id="S4.T1.9.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">2k</th>
<td id="S4.T1.9.3.3.3" class="ltx_td ltx_align_center ltx_border_t">0.3971</td>
<td id="S4.T1.9.3.3.4" class="ltx_td ltx_align_center ltx_border_t">0.2832</td>
<td id="S4.T1.9.3.3.5" class="ltx_td ltx_align_center ltx_border_t">0.3098</td>
<td id="S4.T1.9.3.3.6" class="ltx_td ltx_align_center ltx_border_t">0.3837</td>
<td id="S4.T1.9.3.3.7" class="ltx_td ltx_align_center ltx_border_t">0.1507</td>
<td id="S4.T1.9.3.3.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.9.3.3.8.1" class="ltx_text ltx_font_bold">0.5169</span></td>
</tr>
<tr id="S4.T1.9.4.4" class="ltx_tr">
<th id="S4.T1.9.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">vicuna-7b</th>
<th id="S4.T1.9.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">2k</th>
<td id="S4.T1.9.4.4.3" class="ltx_td ltx_align_center">0.0012</td>
<td id="S4.T1.9.4.4.4" class="ltx_td ltx_align_center">0.0002</td>
<td id="S4.T1.9.4.4.5" class="ltx_td ltx_align_center"><span id="S4.T1.9.4.4.5.1" class="ltx_text ltx_font_bold">0.1033</span></td>
<td id="S4.T1.9.4.4.6" class="ltx_td ltx_align_center">0.0555</td>
<td id="S4.T1.9.4.4.7" class="ltx_td ltx_align_center">0.0674</td>
<td id="S4.T1.9.4.4.8" class="ltx_td ltx_align_center">0.0604</td>
</tr>
<tr id="S4.T1.9.5.5" class="ltx_tr">
<th id="S4.T1.9.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">vicuna-13b</th>
<th id="S4.T1.9.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">2k</th>
<td id="S4.T1.9.5.5.3" class="ltx_td ltx_align_center">0.0340</td>
<td id="S4.T1.9.5.5.4" class="ltx_td ltx_align_center">0.0451</td>
<td id="S4.T1.9.5.5.5" class="ltx_td ltx_align_center">0.1509</td>
<td id="S4.T1.9.5.5.6" class="ltx_td ltx_align_center">0.3120</td>
<td id="S4.T1.9.5.5.7" class="ltx_td ltx_align_center">0.4127</td>
<td id="S4.T1.9.5.5.8" class="ltx_td ltx_align_center"><span id="S4.T1.9.5.5.8.1" class="ltx_text ltx_font_bold">0.5350</span></td>
</tr>
<tr id="S4.T1.9.6.6" class="ltx_tr">
<th id="S4.T1.9.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">vicuna-33b</th>
<th id="S4.T1.9.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">2k</th>
<td id="S4.T1.9.6.6.3" class="ltx_td ltx_align_center">0.1356</td>
<td id="S4.T1.9.6.6.4" class="ltx_td ltx_align_center">0.2049</td>
<td id="S4.T1.9.6.6.5" class="ltx_td ltx_align_center">0.1887</td>
<td id="S4.T1.9.6.6.6" class="ltx_td ltx_align_center">0.3692</td>
<td id="S4.T1.9.6.6.7" class="ltx_td ltx_align_center">0.3125</td>
<td id="S4.T1.9.6.6.8" class="ltx_td ltx_align_center"><span id="S4.T1.9.6.6.8.1" class="ltx_text ltx_font_bold">0.5612</span></td>
</tr>
<tr id="S4.T1.9.7.7" class="ltx_tr">
<th id="S4.T1.9.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">llama-2-7b</th>
<th id="S4.T1.9.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">4k</th>
<td id="S4.T1.9.7.7.3" class="ltx_td ltx_align_center">0.0042</td>
<td id="S4.T1.9.7.7.4" class="ltx_td ltx_align_center">0.0068</td>
<td id="S4.T1.9.7.7.5" class="ltx_td ltx_align_center">0.1248</td>
<td id="S4.T1.9.7.7.6" class="ltx_td ltx_align_center">0.3156</td>
<td id="S4.T1.9.7.7.7" class="ltx_td ltx_align_center">0.2761</td>
<td id="S4.T1.9.7.7.8" class="ltx_td ltx_align_center"><span id="S4.T1.9.7.7.8.1" class="ltx_text ltx_font_bold">0.4648</span></td>
</tr>
<tr id="S4.T1.9.8.8" class="ltx_tr">
<th id="S4.T1.9.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">llama-2-13b</th>
<th id="S4.T1.9.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">4k</th>
<td id="S4.T1.9.8.8.3" class="ltx_td ltx_align_center">0.0662</td>
<td id="S4.T1.9.8.8.4" class="ltx_td ltx_align_center">0.0420</td>
<td id="S4.T1.9.8.8.5" class="ltx_td ltx_align_center">0.2568</td>
<td id="S4.T1.9.8.8.6" class="ltx_td ltx_align_center"><span id="S4.T1.9.8.8.6.1" class="ltx_text ltx_font_bold ltx_framed_underline">0.4892</span></td>
<td id="S4.T1.9.8.8.7" class="ltx_td ltx_align_center">0.4091</td>
<td id="S4.T1.9.8.8.8" class="ltx_td ltx_align_center">0.3716</td>
</tr>
<tr id="S4.T1.9.9.9" class="ltx_tr">
<th id="S4.T1.9.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">llama-2-70b</th>
<th id="S4.T1.9.9.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">4k</th>
<td id="S4.T1.9.9.9.3" class="ltx_td ltx_align_center">0.0122</td>
<td id="S4.T1.9.9.9.4" class="ltx_td ltx_align_center">0.0080</td>
<td id="S4.T1.9.9.9.5" class="ltx_td ltx_align_center">0.4426</td>
<td id="S4.T1.9.9.9.6" class="ltx_td ltx_align_center">0.2979</td>
<td id="S4.T1.9.9.9.7" class="ltx_td ltx_align_center">0.3770</td>
<td id="S4.T1.9.9.9.8" class="ltx_td ltx_align_center"><span id="S4.T1.9.9.9.8.1" class="ltx_text ltx_font_bold">0.5040</span></td>
</tr>
<tr id="S4.T1.9.10.10" class="ltx_tr">
<th id="S4.T1.9.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">mpt-7b-instruct</th>
<th id="S4.T1.9.10.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">8k</th>
<td id="S4.T1.9.10.10.3" class="ltx_td ltx_align_center">0.0001</td>
<td id="S4.T1.9.10.10.4" class="ltx_td ltx_align_center">0.0001</td>
<td id="S4.T1.9.10.10.5" class="ltx_td ltx_align_center">0.0573</td>
<td id="S4.T1.9.10.10.6" class="ltx_td ltx_align_center">0.0656</td>
<td id="S4.T1.9.10.10.7" class="ltx_td ltx_align_center"><span id="S4.T1.9.10.10.7.1" class="ltx_text ltx_font_bold">0.1574</span></td>
<td id="S4.T1.9.10.10.8" class="ltx_td ltx_align_center">0.0632</td>
</tr>
<tr id="S4.T1.9.11.11" class="ltx_tr">
<th id="S4.T1.9.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">mpt-30b-instruct</th>
<th id="S4.T1.9.11.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">8k</th>
<td id="S4.T1.9.11.11.3" class="ltx_td ltx_align_center">0.1664</td>
<td id="S4.T1.9.11.11.4" class="ltx_td ltx_align_center">0.1255</td>
<td id="S4.T1.9.11.11.5" class="ltx_td ltx_align_center">0.3119</td>
<td id="S4.T1.9.11.11.6" class="ltx_td ltx_align_center">0.3060</td>
<td id="S4.T1.9.11.11.7" class="ltx_td ltx_align_center">0.3198</td>
<td id="S4.T1.9.11.11.8" class="ltx_td ltx_align_center"><span id="S4.T1.9.11.11.8.1" class="ltx_text ltx_font_bold">0.4381</span></td>
</tr>
<tr id="S4.T1.9.12.12" class="ltx_tr">
<th id="S4.T1.9.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">xgen-8k-7b-instruct</th>
<th id="S4.T1.9.12.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">8k</th>
<td id="S4.T1.9.12.12.3" class="ltx_td ltx_align_center">0.0001</td>
<td id="S4.T1.9.12.12.4" class="ltx_td ltx_align_center">0.0015</td>
<td id="S4.T1.9.12.12.5" class="ltx_td ltx_align_center">0.0685</td>
<td id="S4.T1.9.12.12.6" class="ltx_td ltx_align_center">0.1574</td>
<td id="S4.T1.9.12.12.7" class="ltx_td ltx_align_center">0.1004</td>
<td id="S4.T1.9.12.12.8" class="ltx_td ltx_align_center"><span id="S4.T1.9.12.12.8.1" class="ltx_text ltx_font_bold">0.3697</span></td>
</tr>
<tr id="S4.T1.9.13.13" class="ltx_tr">
<th id="S4.T1.9.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">longchat-7b-16k</th>
<th id="S4.T1.9.13.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">16k</th>
<td id="S4.T1.9.13.13.3" class="ltx_td ltx_align_center">0.0165</td>
<td id="S4.T1.9.13.13.4" class="ltx_td ltx_align_center">0.0171</td>
<td id="S4.T1.9.13.13.5" class="ltx_td ltx_align_center">0.069</td>
<td id="S4.T1.9.13.13.6" class="ltx_td ltx_align_center">0.0917</td>
<td id="S4.T1.9.13.13.7" class="ltx_td ltx_align_center">0.1322</td>
<td id="S4.T1.9.13.13.8" class="ltx_td ltx_align_center"><span id="S4.T1.9.13.13.8.1" class="ltx_text ltx_font_bold">0.1964</span></td>
</tr>
<tr id="S4.T1.9.14.14" class="ltx_tr">
<th id="S4.T1.9.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">longchat-13b-16k</th>
<th id="S4.T1.9.14.14.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">16k</th>
<td id="S4.T1.9.14.14.3" class="ltx_td ltx_align_center">0.0007</td>
<td id="S4.T1.9.14.14.4" class="ltx_td ltx_align_center">0.0007</td>
<td id="S4.T1.9.14.14.5" class="ltx_td ltx_align_center">0.2373</td>
<td id="S4.T1.9.14.14.6" class="ltx_td ltx_align_center">0.3978</td>
<td id="S4.T1.9.14.14.7" class="ltx_td ltx_align_center"><span id="S4.T1.9.14.14.7.1" class="ltx_text ltx_font_bold">0.4019</span></td>
<td id="S4.T1.9.14.14.8" class="ltx_td ltx_align_center">0.3205</td>
</tr>
<tr id="S4.T1.9.15.15" class="ltx_tr">
<th id="S4.T1.9.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">text-davinci-003</th>
<th id="S4.T1.9.15.15.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">4k</th>
<td id="S4.T1.9.15.15.3" class="ltx_td ltx_align_center ltx_border_t">0.5292</td>
<td id="S4.T1.9.15.15.4" class="ltx_td ltx_align_center ltx_border_t">0.5395</td>
<td id="S4.T1.9.15.15.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.9.15.15.5.1" class="ltx_text ltx_framed_underline">0.5474</span></td>
<td id="S4.T1.9.15.15.6" class="ltx_td ltx_align_center ltx_border_t">0.4751</td>
<td id="S4.T1.9.15.15.7" class="ltx_td ltx_align_center ltx_border_t">0.4912</td>
<td id="S4.T1.9.15.15.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.9.15.15.8.1" class="ltx_text ltx_font_bold">0.6341</span></td>
</tr>
<tr id="S4.T1.9.16.16" class="ltx_tr">
<th id="S4.T1.9.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">gpt-3.5-turbo</th>
<th id="S4.T1.9.16.16.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">4k</th>
<td id="S4.T1.9.16.16.3" class="ltx_td ltx_align_center">0.5061</td>
<td id="S4.T1.9.16.16.4" class="ltx_td ltx_align_center">0.5057</td>
<td id="S4.T1.9.16.16.5" class="ltx_td ltx_align_center">0.5383</td>
<td id="S4.T1.9.16.16.6" class="ltx_td ltx_align_center">0.4667</td>
<td id="S4.T1.9.16.16.7" class="ltx_td ltx_align_center">0.5483</td>
<td id="S4.T1.9.16.16.8" class="ltx_td ltx_align_center"><span id="S4.T1.9.16.16.8.1" class="ltx_text ltx_framed_underline"><span id="S4.T1.9.16.16.8.1.1" class="ltx_text ltx_font_bold">0.6567</span></span></td>
</tr>
<tr id="S4.T1.9.17.17" class="ltx_tr">
<th id="S4.T1.9.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">gpt-3.5-turbo-16k</th>
<th id="S4.T1.9.17.17.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">16k</th>
<td id="S4.T1.9.17.17.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.9.17.17.3.1" class="ltx_text ltx_framed_underline">0.5657</span></td>
<td id="S4.T1.9.17.17.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.9.17.17.4.1" class="ltx_text ltx_framed_underline">0.5642</span></td>
<td id="S4.T1.9.17.17.5" class="ltx_td ltx_align_center ltx_border_bb">0.4898</td>
<td id="S4.T1.9.17.17.6" class="ltx_td ltx_align_center ltx_border_bb">0.4565</td>
<td id="S4.T1.9.17.17.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.9.17.17.7.1" class="ltx_text ltx_framed_underline">0.5607</span></td>
<td id="S4.T1.9.17.17.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.9.17.17.8.1" class="ltx_text ltx_font_bold">0.6541</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.p1.1" class="ltx_p">In this section, we present and compare the decision-making performances of LAAs in the WebShop environment.
The performance regarding the average reward is reported in Table <a href="#S4.T1" title="Table 1 ‣ 4.4 Decision-making Simulation ‣ 4 Experiment ‣ BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
The agent prompts are constructed based on the maximum context length of different LLM models.
Regarding BOLAA, we devise one search LAA and one click LAA to generate search query and click elements, respectively.
We have the following observation:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">BOLAA performs the best compared with the other LAA architectures, especially when built on the high performing LLMs. BOLAA is able to actively select the appropriate LAA and yield qualitative communication, which stabilizes the action generation.
We observe that BOLAA, when paired with a 3b fastchat-t5 LLM, performs comparably to other LAA architectures with more powerful LLMs.
The superiority of BOLAA indicates that orchestrating multiple smaller-sized LAAs is a better choice if the computing resources are limited.
This further exemplifies the potential for fine-tuning multiple smaller-sized specialised LAAs rather than fine-tuning one large generalized LAA.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Pairing the LLM with the optimal LAA architecture is crucial. For example, Llama-2-13b performs best under PlanAct LAA arch while Llama-2-70b performs best under the BOLAA arch. Also, Longchat-13b-16K performs best when using PlanAct and PlanReAct, which may indicate the extraordinary planning ability of longchat-13b-16k models.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Increasing the context length alone may not necessarily improve the LAA performances. For example, when comparing longchat-13b-16k with llama-2-13b models, the latter yields better performances though with less context length. By checking the running log of those LAAs, we observe more occurrence of hallucinated generation when the LAA runs for more steps, which in the end degrades the benefits of longer context.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">A powerful LLM is able to generalize under the zeroshot LAA arch. The best performance of OpenAI API-based models are actually under ZS and ZST arch. This indicates the great potential of developing a generic LAA with powerful LLM.
Actually, this is currently what open-source projects are working towards, directly calling OpenAI API and tuning the zeroshot agent prompt instead.
Our benchmark results quantitatively justify that using only a ZS LAA can already achieve comparable or even better performances than LAA arch with additional Plan or Self-think flow. However, for other less powerful LLMs, fewshot prompts are necessary for LAAs.</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i5.p1" class="ltx_para ltx_noindent">
<p id="S4.I1.i5.p1.1" class="ltx_p">Plan flow generally improves the performances when the agent is built on open-source LLMs.
By comparing the performances of ReAct, PlanAct and PlanReAct, we observe a performance gain on most LLM cases when using plan flow. However, planning and thinking require the LLM to be able to reason in steps, which may be challenging for small size LLMs.
For example, fastchat-t5-3b performs above average on ZS LAA arch. But the performance degrades by a large margin under PlanReAct arch.</p>
</div>
</li>
</ul>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Average recall in the WebShop environment. Len denotes the maximum context length. <span id="S4.T2.5.1" class="ltx_text ltx_font_bold">Bold</span> results denote the best results in one row, <span id="S4.T2.6.2" class="ltx_text ltx_font_italic">i.e.</span> best LAA architecture w.r.t. one LLM. <span id="S4.T2.7.3" class="ltx_text ltx_framed_underline">Underline</span> results denote the best performance in one column, <span id="S4.T2.8.4" class="ltx_text ltx_font_italic">i.e.</span> best LLM regarding one LAA architecture.</figcaption>
<table id="S4.T2.9" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.9.1.1" class="ltx_tr">
<th id="S4.T2.9.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T2.9.1.1.1.1" class="ltx_text">LLM</span></th>
<th id="S4.T2.9.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T2.9.1.1.2.1" class="ltx_text">Len.</span></th>
<td id="S4.T2.9.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="6">LAA Architecture</td>
</tr>
<tr id="S4.T2.9.2.2" class="ltx_tr">
<td id="S4.T2.9.2.2.1" class="ltx_td ltx_align_center ltx_border_t">ZS</td>
<td id="S4.T2.9.2.2.2" class="ltx_td ltx_align_center ltx_border_t">ZST</td>
<td id="S4.T2.9.2.2.3" class="ltx_td ltx_align_center ltx_border_t">ReAct</td>
<td id="S4.T2.9.2.2.4" class="ltx_td ltx_align_center ltx_border_t">PlanAct</td>
<td id="S4.T2.9.2.2.5" class="ltx_td ltx_align_center ltx_border_t">PlanReAct</td>
<td id="S4.T2.9.2.2.6" class="ltx_td ltx_align_center ltx_border_t">BOLAA</td>
</tr>
<tr id="S4.T2.9.3.3" class="ltx_tr">
<th id="S4.T2.9.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">fastchat-t5-3b</th>
<th id="S4.T2.9.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">2k</th>
<td id="S4.T2.9.3.3.3" class="ltx_td ltx_align_center ltx_border_t">0.3533</td>
<td id="S4.T2.9.3.3.4" class="ltx_td ltx_align_center ltx_border_t">0.3122</td>
<td id="S4.T2.9.3.3.5" class="ltx_td ltx_align_center ltx_border_t">0.3800</td>
<td id="S4.T2.9.3.3.6" class="ltx_td ltx_align_center ltx_border_t">0.3700</td>
<td id="S4.T2.9.3.3.7" class="ltx_td ltx_align_center ltx_border_t">0.3722</td>
<td id="S4.T2.9.3.3.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.9.3.3.8.1" class="ltx_text ltx_font_bold">0.3867</span></td>
</tr>
<tr id="S4.T2.9.4.4" class="ltx_tr">
<th id="S4.T2.9.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">vicuna-7b</th>
<th id="S4.T2.9.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">2k</th>
<td id="S4.T2.9.4.4.3" class="ltx_td ltx_align_center">0.0833</td>
<td id="S4.T2.9.4.4.4" class="ltx_td ltx_align_center">0.0500</td>
<td id="S4.T2.9.4.4.5" class="ltx_td ltx_align_center">0.3600</td>
<td id="S4.T2.9.4.4.6" class="ltx_td ltx_align_center">0.3233</td>
<td id="S4.T2.9.4.4.7" class="ltx_td ltx_align_center">0.3278</td>
<td id="S4.T2.9.4.4.8" class="ltx_td ltx_align_center"><span id="S4.T2.9.4.4.8.1" class="ltx_text ltx_font_bold">0.3522</span></td>
</tr>
<tr id="S4.T2.9.5.5" class="ltx_tr">
<th id="S4.T2.9.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">vicuna-13b</th>
<th id="S4.T2.9.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">2k</th>
<td id="S4.T2.9.5.5.3" class="ltx_td ltx_align_center">0.0867</td>
<td id="S4.T2.9.5.5.4" class="ltx_td ltx_align_center">0.0644</td>
<td id="S4.T2.9.5.5.5" class="ltx_td ltx_align_center">0.3622</td>
<td id="S4.T2.9.5.5.6" class="ltx_td ltx_align_center">0.3444</td>
<td id="S4.T2.9.5.5.7" class="ltx_td ltx_align_center">0.2367</td>
<td id="S4.T2.9.5.5.8" class="ltx_td ltx_align_center"><span id="S4.T2.9.5.5.8.1" class="ltx_text ltx_font_bold">0.3700</span></td>
</tr>
<tr id="S4.T2.9.6.6" class="ltx_tr">
<th id="S4.T2.9.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">vicuna-33b</th>
<th id="S4.T2.9.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">2k</th>
<td id="S4.T2.9.6.6.3" class="ltx_td ltx_align_center">0.3600</td>
<td id="S4.T2.9.6.6.4" class="ltx_td ltx_align_center">0.3411</td>
<td id="S4.T2.9.6.6.5" class="ltx_td ltx_align_center">0.3822</td>
<td id="S4.T2.9.6.6.6" class="ltx_td ltx_align_center">0.3733</td>
<td id="S4.T2.9.6.6.7" class="ltx_td ltx_align_center">0.3567</td>
<td id="S4.T2.9.6.6.8" class="ltx_td ltx_align_center"><span id="S4.T2.9.6.6.8.1" class="ltx_text ltx_font_bold">0.3956</span></td>
</tr>
<tr id="S4.T2.9.7.7" class="ltx_tr">
<th id="S4.T2.9.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">llama-2-7b</th>
<th id="S4.T2.9.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">4k</th>
<td id="S4.T2.9.7.7.3" class="ltx_td ltx_align_center">0.0678</td>
<td id="S4.T2.9.7.7.4" class="ltx_td ltx_align_center">0.0311</td>
<td id="S4.T2.9.7.7.5" class="ltx_td ltx_align_center">0.3744</td>
<td id="S4.T2.9.7.7.6" class="ltx_td ltx_align_center">0.3400</td>
<td id="S4.T2.9.7.7.7" class="ltx_td ltx_align_center">0.3578</td>
<td id="S4.T2.9.7.7.8" class="ltx_td ltx_align_center"><span id="S4.T2.9.7.7.8.1" class="ltx_text ltx_font_bold">0.3856</span></td>
</tr>
<tr id="S4.T2.9.8.8" class="ltx_tr">
<th id="S4.T2.9.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">llama-2-13b</th>
<th id="S4.T2.9.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">4k</th>
<td id="S4.T2.9.8.8.3" class="ltx_td ltx_align_center">0.2856</td>
<td id="S4.T2.9.8.8.4" class="ltx_td ltx_align_center">0.2211</td>
<td id="S4.T2.9.8.8.5" class="ltx_td ltx_align_center">0.3844</td>
<td id="S4.T2.9.8.8.6" class="ltx_td ltx_align_center">0.3278</td>
<td id="S4.T2.9.8.8.7" class="ltx_td ltx_align_center">0.3500</td>
<td id="S4.T2.9.8.8.8" class="ltx_td ltx_align_center"><span id="S4.T2.9.8.8.8.1" class="ltx_text ltx_font_bold ltx_framed_underline">0.4078</span></td>
</tr>
<tr id="S4.T2.9.9.9" class="ltx_tr">
<th id="S4.T2.9.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">llama-2-70b</th>
<th id="S4.T2.9.9.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">4k</th>
<td id="S4.T2.9.9.9.3" class="ltx_td ltx_align_center">0.3344</td>
<td id="S4.T2.9.9.9.4" class="ltx_td ltx_align_center">0.3244</td>
<td id="S4.T2.9.9.9.5" class="ltx_td ltx_align_center">0.3789</td>
<td id="S4.T2.9.9.9.6" class="ltx_td ltx_align_center">0.3400</td>
<td id="S4.T2.9.9.9.7" class="ltx_td ltx_align_center">0.3600</td>
<td id="S4.T2.9.9.9.8" class="ltx_td ltx_align_center"><span id="S4.T2.9.9.9.8.1" class="ltx_text ltx_font_bold">0.4011</span></td>
</tr>
<tr id="S4.T2.9.10.10" class="ltx_tr">
<th id="S4.T2.9.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">mpt-7b-instruct</th>
<th id="S4.T2.9.10.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">8k</th>
<td id="S4.T2.9.10.10.3" class="ltx_td ltx_align_center">0.0144</td>
<td id="S4.T2.9.10.10.4" class="ltx_td ltx_align_center">0.0322</td>
<td id="S4.T2.9.10.10.5" class="ltx_td ltx_align_center"><span id="S4.T2.9.10.10.5.1" class="ltx_text ltx_font_bold">0.3644</span></td>
<td id="S4.T2.9.10.10.6" class="ltx_td ltx_align_center">0.3200</td>
<td id="S4.T2.9.10.10.7" class="ltx_td ltx_align_center">0.3400</td>
<td id="S4.T2.9.10.10.8" class="ltx_td ltx_align_center">0.3600</td>
</tr>
<tr id="S4.T2.9.11.11" class="ltx_tr">
<th id="S4.T2.9.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">mpt-30b-instruct</th>
<th id="S4.T2.9.11.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">8k</th>
<td id="S4.T2.9.11.11.3" class="ltx_td ltx_align_center">0.2973</td>
<td id="S4.T2.9.11.11.4" class="ltx_td ltx_align_center">0.3372</td>
<td id="S4.T2.9.11.11.5" class="ltx_td ltx_align_center">0.3333</td>
<td id="S4.T2.9.11.11.6" class="ltx_td ltx_align_center">0.3575</td>
<td id="S4.T2.9.11.11.7" class="ltx_td ltx_align_center">0.3412</td>
<td id="S4.T2.9.11.11.8" class="ltx_td ltx_align_center"><span id="S4.T2.9.11.11.8.1" class="ltx_text ltx_font_bold">0.3900</span></td>
</tr>
<tr id="S4.T2.9.12.12" class="ltx_tr">
<th id="S4.T2.9.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">xgen-8k-7b-instruct</th>
<th id="S4.T2.9.12.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">8k</th>
<td id="S4.T2.9.12.12.3" class="ltx_td ltx_align_center">0.0667</td>
<td id="S4.T2.9.12.12.4" class="ltx_td ltx_align_center">0.1400</td>
<td id="S4.T2.9.12.12.5" class="ltx_td ltx_align_center">0.3711</td>
<td id="S4.T2.9.12.12.6" class="ltx_td ltx_align_center">0.3400</td>
<td id="S4.T2.9.12.12.7" class="ltx_td ltx_align_center">0.3278</td>
<td id="S4.T2.9.12.12.8" class="ltx_td ltx_align_center"><span id="S4.T2.9.12.12.8.1" class="ltx_text ltx_font_bold">0.3800</span></td>
</tr>
<tr id="S4.T2.9.13.13" class="ltx_tr">
<th id="S4.T2.9.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">longchat-7b-16k</th>
<th id="S4.T2.9.13.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">16k</th>
<td id="S4.T2.9.13.13.3" class="ltx_td ltx_align_center">0.1344</td>
<td id="S4.T2.9.13.13.4" class="ltx_td ltx_align_center">0.1856</td>
<td id="S4.T2.9.13.13.5" class="ltx_td ltx_align_center">0.3644</td>
<td id="S4.T2.9.13.13.6" class="ltx_td ltx_align_center">0.3622</td>
<td id="S4.T2.9.13.13.7" class="ltx_td ltx_align_center">0.3622</td>
<td id="S4.T2.9.13.13.8" class="ltx_td ltx_align_center"><span id="S4.T2.9.13.13.8.1" class="ltx_text ltx_font_bold">0.3811</span></td>
</tr>
<tr id="S4.T2.9.14.14" class="ltx_tr">
<th id="S4.T2.9.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">longchat-13b-16k</th>
<th id="S4.T2.9.14.14.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">16k</th>
<td id="S4.T2.9.14.14.3" class="ltx_td ltx_align_center">0.0756</td>
<td id="S4.T2.9.14.14.4" class="ltx_td ltx_align_center">0.0867</td>
<td id="S4.T2.9.14.14.5" class="ltx_td ltx_align_center">0.3678</td>
<td id="S4.T2.9.14.14.6" class="ltx_td ltx_align_center">0.3467</td>
<td id="S4.T2.9.14.14.7" class="ltx_td ltx_align_center">0.3471</td>
<td id="S4.T2.9.14.14.8" class="ltx_td ltx_align_center"><span id="S4.T2.9.14.14.8.1" class="ltx_text ltx_font_bold">0.3789</span></td>
</tr>
<tr id="S4.T2.9.15.15" class="ltx_tr">
<th id="S4.T2.9.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">text-davinci-003</th>
<th id="S4.T2.9.15.15.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">4k</th>
<td id="S4.T2.9.15.15.3" class="ltx_td ltx_align_center ltx_border_t">0.3800</td>
<td id="S4.T2.9.15.15.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.9.15.15.4.1" class="ltx_text ltx_framed_underline">0.3856</span></td>
<td id="S4.T2.9.15.15.5" class="ltx_td ltx_align_center ltx_border_t">0.3767</td>
<td id="S4.T2.9.15.15.6" class="ltx_td ltx_align_center ltx_border_t">0.3711</td>
<td id="S4.T2.9.15.15.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.9.15.15.7.1" class="ltx_text ltx_framed_underline">0.3889</span></td>
<td id="S4.T2.9.15.15.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.9.15.15.8.1" class="ltx_text ltx_font_bold">0.3956</span></td>
</tr>
<tr id="S4.T2.9.16.16" class="ltx_tr">
<th id="S4.T2.9.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">gpt-3.5-turbo</th>
<th id="S4.T2.9.16.16.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">4k</th>
<td id="S4.T2.9.16.16.3" class="ltx_td ltx_align_center"><span id="S4.T2.9.16.16.3.1" class="ltx_text ltx_framed_underline">0.3889</span></td>
<td id="S4.T2.9.16.16.4" class="ltx_td ltx_align_center">0.3756</td>
<td id="S4.T2.9.16.16.5" class="ltx_td ltx_align_center"><span id="S4.T2.9.16.16.5.1" class="ltx_text ltx_font_bold">0.3933</span></td>
<td id="S4.T2.9.16.16.6" class="ltx_td ltx_align_center"><span id="S4.T2.9.16.16.6.1" class="ltx_text ltx_framed_underline">0.3789</span></td>
<td id="S4.T2.9.16.16.7" class="ltx_td ltx_align_center">0.3867</td>
<td id="S4.T2.9.16.16.8" class="ltx_td ltx_align_center">0.3929</td>
</tr>
<tr id="S4.T2.9.17.17" class="ltx_tr">
<th id="S4.T2.9.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">gpt-3.5-turbo-16k-0613</th>
<th id="S4.T2.9.17.17.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">16k</th>
<td id="S4.T2.9.17.17.3" class="ltx_td ltx_align_center ltx_border_bb">0.3856</td>
<td id="S4.T2.9.17.17.4" class="ltx_td ltx_align_center ltx_border_bb">0.3833</td>
<td id="S4.T2.9.17.17.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.9.17.17.5.1" class="ltx_text ltx_font_bold ltx_framed_underline">0.4011</span></td>
<td id="S4.T2.9.17.17.6" class="ltx_td ltx_align_center ltx_border_bb">0.3756</td>
<td id="S4.T2.9.17.17.7" class="ltx_td ltx_align_center ltx_border_bb">0.3811</td>
<td id="S4.T2.9.17.17.8" class="ltx_td ltx_align_center ltx_border_bb">0.3933</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS4.p2.1" class="ltx_p">We also report the intermediate Recall performances for all LAAs, which are illustrated in Table <a href="#S4.T2" title="Table 2 ‣ 4.4 Decision-making Simulation ‣ 4 Experiment ‣ BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
Recall is mainly related to the search action.
High recall performances indicate that the LAA is capable of generating a precise search query.
High recalls usually lead to better rewards. But they are not tightly related.
For example, Llama-2-70b has a recall performance of nearly 0.3344 on ZS LAA, which is comparable to the best LAA.
However, the reward performance in Table <a href="#S4.T1" title="Table 1 ‣ 4.4 Decision-making Simulation ‣ 4 Experiment ‣ BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> of ZS LAA Llama-2-70b is only 0.0122.
The reason is that generating the search query requires a different LLM ability from generating the correct click action, where the latter is more challenging.
Another observation is that our proposed BOLAA generally performs the best on all LLMs, which indicates that separating the search agent from the click agent improves the accuracy of the search action, leading to a higher recall value.
</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p"><span id="S4.SS4.p3.1.1" class="ltx_text ltx_font_bold">LAA performance w.r.t. Complexity</span>.
After the overall performances of those LAAs and LLMs are compared, we conduct more details investigation of the performance w.r.t. the task complexity.
Due to the space limitation, we only report the performance of text-davinci-003 and llama-2-70b. The reward performance is illustrated in Figure <a href="#S4.F4" title="Figure 4 ‣ 4.4 Decision-making Simulation ‣ 4 Experiment ‣ BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
The BOLAA model consistently performs better on all complexity levels.
We also observe the degraded performances when the task complexity is increased, which follows the intuition.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">

<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S4.F3.sf1" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="x4.png" id="S4.F3.sf1.g1" class="ltx_graphics ltx_img_landscape" width="761" height="570" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>text-davinci-003</figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S4.F3.sf2" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="x5.png" id="S4.F3.sf2.g1" class="ltx_graphics ltx_img_landscape" width="761" height="570" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Llama-2-70b</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The reward w.r.t. task complexity in WebShop. Each bar represents one LAA.</figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">

<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S4.F4.sf1" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="x6.png" id="S4.F4.sf1.g1" class="ltx_graphics ltx_img_landscape" width="761" height="570" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>text-davinci-003</figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S4.F4.sf2" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="x7.png" id="S4.F4.sf2.g1" class="ltx_graphics ltx_img_landscape" width="761" height="570" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Llama-2-70b</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The recall w.r.t. task complexity in WebShop. Each bar represents one LAA.</figcaption>
</figure>
<div id="S4.SS4.p4" class="ltx_para ltx_noindent">
<p id="S4.SS4.p4.1" class="ltx_p">Surprisingly, we find out that further increasing the complexity of tasks greater than 4 will not further degrade the performances. The reason is that the recall performance increases when the task is of higher complexity, which we demonstrated in Figure <a href="#S4.F5" title="Figure 5 ‣ 4.4 Decision-making Simulation ‣ 4 Experiment ‣ BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. This is due to the fact that high-complexity task instruction provides more additional context information for the LAA. As such, the <span id="S4.SS4.p4.1.1" class="ltx_text ltx_font_italic">search</span> action can be more specific and accurate under high complexity levels.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Knowledge Reasoning Simulation</h3>

<div id="S4.SS5.p1" class="ltx_para ltx_noindent">
<p id="S4.SS5.p1.1" class="ltx_p">We benchmark on the HotPotQA environment to evaluate the multi-step reasoning ability of LAAs.
Since the available search, lookup and finish operations are all related to knowledge reasoning in this environment and hard to separate, we therefore leave the BOLAA arch for future work and only compare the performance on other agent arch.
The results are in Table <a href="#S4.T3" title="Table 3 ‣ 4.5 Knowledge Reasoning Simulation ‣ 4 Experiment ‣ BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
In general, ReAct agent arch achieves the best performances, which can be interpreted in twofold.
Firstly, fewshot prompt is necessary to enable the action generation and reasoning ability for LAA, especially when experimenting with those small-size language models.
Secondly, comparing ReAct, PlanAct, and PlanReAct, we would conclude that planning flow of LAA hinders performance the in knowledge reasoning environment and tasks.
The reason is that knowledge reasoning tasks require contextualized information to conduct reasoning, whereas planning flow is executed ahead of interactions.
Thus, those generated plans tend to lead to more hallucination of LAA.
Thirdly, regarding this knowledge reasoning task, model size is much more important than the context length.
Large-sized model has better abilities in reasoning, thus performing better.
Additionally, the superior reasoning ability of OpenAI gpt-3.5 models is again verified.
We also observe the best performance of Llama-2-70b on all open-source LLMs, which suggests that potential future fine-tuning can be applied on Llama-2 models.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Average reward in the HotPotQA environment. Len denotes the maximum context length. <span id="S4.T3.5.1" class="ltx_text ltx_font_bold">Bold</span> results denote the best results in one row, <span id="S4.T3.6.2" class="ltx_text ltx_font_italic">i.e.</span> best LAA architecture w.r.t. one LLM. <span id="S4.T3.7.3" class="ltx_text ltx_framed_underline">Underline</span> results denote the best performance in one column, <span id="S4.T3.8.4" class="ltx_text ltx_font_italic">i.e.</span> best LLM regarding one LAA architecture.</figcaption>
<table id="S4.T3.9" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.9.1.1" class="ltx_tr">
<th id="S4.T3.9.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T3.9.1.1.1.1" class="ltx_text">LLM</span></th>
<th id="S4.T3.9.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T3.9.1.1.2.1" class="ltx_text">Len.</span></th>
<td id="S4.T3.9.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="5">LAA Architecture</td>
</tr>
<tr id="S4.T3.9.2.2" class="ltx_tr">
<td id="S4.T3.9.2.2.1" class="ltx_td ltx_align_center ltx_border_t">ZS</td>
<td id="S4.T3.9.2.2.2" class="ltx_td ltx_align_center ltx_border_t">ZST</td>
<td id="S4.T3.9.2.2.3" class="ltx_td ltx_align_center ltx_border_t">ReAct</td>
<td id="S4.T3.9.2.2.4" class="ltx_td ltx_align_center ltx_border_t">PlanAct</td>
<td id="S4.T3.9.2.2.5" class="ltx_td ltx_align_center ltx_border_t">PlanReAct</td>
</tr>
<tr id="S4.T3.9.3.3" class="ltx_tr">
<th id="S4.T3.9.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">fastchat-t5-3b</th>
<th id="S4.T3.9.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">2k</th>
<td id="S4.T3.9.3.3.3" class="ltx_td ltx_align_center ltx_border_t">0.0252</td>
<td id="S4.T3.9.3.3.4" class="ltx_td ltx_align_center ltx_border_t">0.0067</td>
<td id="S4.T3.9.3.3.5" class="ltx_td ltx_align_center ltx_border_t">0.0692</td>
<td id="S4.T3.9.3.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.9.3.3.6.1" class="ltx_text ltx_font_bold">0.1155</span></td>
<td id="S4.T3.9.3.3.7" class="ltx_td ltx_align_center ltx_border_t">0.0834</td>
</tr>
<tr id="S4.T3.9.4.4" class="ltx_tr">
<th id="S4.T3.9.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">vicuna-7b</th>
<th id="S4.T3.9.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">2k</th>
<td id="S4.T3.9.4.4.3" class="ltx_td ltx_align_center"><span id="S4.T3.9.4.4.3.1" class="ltx_text ltx_font_bold">0.1339</span></td>
<td id="S4.T3.9.4.4.4" class="ltx_td ltx_align_center">0.0797</td>
<td id="S4.T3.9.4.4.5" class="ltx_td ltx_align_center">0.0318</td>
<td id="S4.T3.9.4.4.6" class="ltx_td ltx_align_center">0.0868</td>
<td id="S4.T3.9.4.4.7" class="ltx_td ltx_align_center">0.0956</td>
</tr>
<tr id="S4.T3.9.5.5" class="ltx_tr">
<th id="S4.T3.9.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">vicuna-13b</th>
<th id="S4.T3.9.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">2k</th>
<td id="S4.T3.9.5.5.3" class="ltx_td ltx_align_center">0.1541</td>
<td id="S4.T3.9.5.5.4" class="ltx_td ltx_align_center">0.0910</td>
<td id="S4.T3.9.5.5.5" class="ltx_td ltx_align_center"><span id="S4.T3.9.5.5.5.1" class="ltx_text ltx_font_bold">0.2637</span></td>
<td id="S4.T3.9.5.5.6" class="ltx_td ltx_align_center">0.1754</td>
<td id="S4.T3.9.5.5.7" class="ltx_td ltx_align_center">0.2075</td>
</tr>
<tr id="S4.T3.9.6.6" class="ltx_tr">
<th id="S4.T3.9.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">vicuna-33b</th>
<th id="S4.T3.9.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">2k</th>
<td id="S4.T3.9.6.6.3" class="ltx_td ltx_align_center">0.2180</td>
<td id="S4.T3.9.6.6.4" class="ltx_td ltx_align_center">0.2223</td>
<td id="S4.T3.9.6.6.5" class="ltx_td ltx_align_center"><span id="S4.T3.9.6.6.5.1" class="ltx_text ltx_font_bold">0.2602</span></td>
<td id="S4.T3.9.6.6.6" class="ltx_td ltx_align_center">0.1333</td>
<td id="S4.T3.9.6.6.7" class="ltx_td ltx_align_center">0.2016</td>
</tr>
<tr id="S4.T3.9.7.7" class="ltx_tr">
<th id="S4.T3.9.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">llama-2-7b</th>
<th id="S4.T3.9.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">4k</th>
<td id="S4.T3.9.7.7.3" class="ltx_td ltx_align_center">0.0395</td>
<td id="S4.T3.9.7.7.4" class="ltx_td ltx_align_center">0.0207</td>
<td id="S4.T3.9.7.7.5" class="ltx_td ltx_align_center"><span id="S4.T3.9.7.7.5.1" class="ltx_text ltx_font_bold">0.2624</span></td>
<td id="S4.T3.9.7.7.6" class="ltx_td ltx_align_center">0.1780</td>
<td id="S4.T3.9.7.7.7" class="ltx_td ltx_align_center">0.1417</td>
</tr>
<tr id="S4.T3.9.8.8" class="ltx_tr">
<th id="S4.T3.9.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">llama-2-13b</th>
<th id="S4.T3.9.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">4k</th>
<td id="S4.T3.9.8.8.3" class="ltx_td ltx_align_center">0.1731</td>
<td id="S4.T3.9.8.8.4" class="ltx_td ltx_align_center">0.2313</td>
<td id="S4.T3.9.8.8.5" class="ltx_td ltx_align_center"><span id="S4.T3.9.8.8.5.1" class="ltx_text ltx_font_bold">0.2521</span></td>
<td id="S4.T3.9.8.8.6" class="ltx_td ltx_align_center">0.2192</td>
<td id="S4.T3.9.8.8.7" class="ltx_td ltx_align_center">0.2177</td>
</tr>
<tr id="S4.T3.9.9.9" class="ltx_tr">
<th id="S4.T3.9.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">llama-2-70b</th>
<th id="S4.T3.9.9.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">4k</th>
<td id="S4.T3.9.9.9.3" class="ltx_td ltx_align_center">0.2809</td>
<td id="S4.T3.9.9.9.4" class="ltx_td ltx_align_center">0.3207</td>
<td id="S4.T3.9.9.9.5" class="ltx_td ltx_align_center"><span id="S4.T3.9.9.9.5.1" class="ltx_text ltx_font_bold">0.3558</span></td>
<td id="S4.T3.9.9.9.6" class="ltx_td ltx_align_center">0.1424</td>
<td id="S4.T3.9.9.9.7" class="ltx_td ltx_align_center">0.1797</td>
</tr>
<tr id="S4.T3.9.10.10" class="ltx_tr">
<th id="S4.T3.9.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">mpt-7b-instruct</th>
<th id="S4.T3.9.10.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">8k</th>
<td id="S4.T3.9.10.10.3" class="ltx_td ltx_align_center">0.0982</td>
<td id="S4.T3.9.10.10.4" class="ltx_td ltx_align_center">0.0483</td>
<td id="S4.T3.9.10.10.5" class="ltx_td ltx_align_center"><span id="S4.T3.9.10.10.5.1" class="ltx_text ltx_font_bold">0.1707</span></td>
<td id="S4.T3.9.10.10.6" class="ltx_td ltx_align_center">0.1147</td>
<td id="S4.T3.9.10.10.7" class="ltx_td ltx_align_center">0.1195</td>
</tr>
<tr id="S4.T3.9.11.11" class="ltx_tr">
<th id="S4.T3.9.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">mpt-30b-instruct</th>
<th id="S4.T3.9.11.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">8k</th>
<td id="S4.T3.9.11.11.3" class="ltx_td ltx_align_center">0.1562</td>
<td id="S4.T3.9.11.11.4" class="ltx_td ltx_align_center">0.2141</td>
<td id="S4.T3.9.11.11.5" class="ltx_td ltx_align_center"><span id="S4.T3.9.11.11.5.1" class="ltx_text ltx_font_bold">0.3261</span></td>
<td id="S4.T3.9.11.11.6" class="ltx_td ltx_align_center">0.2224</td>
<td id="S4.T3.9.11.11.7" class="ltx_td ltx_align_center">0.2315</td>
</tr>
<tr id="S4.T3.9.12.12" class="ltx_tr">
<th id="S4.T3.9.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">xgen-8k-7b-instruct</th>
<th id="S4.T3.9.12.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">8k</th>
<td id="S4.T3.9.12.12.3" class="ltx_td ltx_align_center">0.1502</td>
<td id="S4.T3.9.12.12.4" class="ltx_td ltx_align_center">0.1244</td>
<td id="S4.T3.9.12.12.5" class="ltx_td ltx_align_center"><span id="S4.T3.9.12.12.5.1" class="ltx_text ltx_font_bold">0.1937</span></td>
<td id="S4.T3.9.12.12.6" class="ltx_td ltx_align_center">0.1116</td>
<td id="S4.T3.9.12.12.7" class="ltx_td ltx_align_center">0.1096</td>
</tr>
<tr id="S4.T3.9.13.13" class="ltx_tr">
<th id="S4.T3.9.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">longchat-7b-16k</th>
<th id="S4.T3.9.13.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">16k</th>
<td id="S4.T3.9.13.13.3" class="ltx_td ltx_align_center">0.0791</td>
<td id="S4.T3.9.13.13.4" class="ltx_td ltx_align_center">0.0672</td>
<td id="S4.T3.9.13.13.5" class="ltx_td ltx_align_center"><span id="S4.T3.9.13.13.5.1" class="ltx_text ltx_font_bold">0.2161</span></td>
<td id="S4.T3.9.13.13.6" class="ltx_td ltx_align_center">0.1296</td>
<td id="S4.T3.9.13.13.7" class="ltx_td ltx_align_center">0.0971</td>
</tr>
<tr id="S4.T3.9.14.14" class="ltx_tr">
<th id="S4.T3.9.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">longchat-13b-16k</th>
<th id="S4.T3.9.14.14.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">16k</th>
<td id="S4.T3.9.14.14.3" class="ltx_td ltx_align_center">0.1083</td>
<td id="S4.T3.9.14.14.4" class="ltx_td ltx_align_center">0.0562</td>
<td id="S4.T3.9.14.14.5" class="ltx_td ltx_align_center"><span id="S4.T3.9.14.14.5.1" class="ltx_text ltx_font_bold">0.2387</span></td>
<td id="S4.T3.9.14.14.6" class="ltx_td ltx_align_center">0.1623</td>
<td id="S4.T3.9.14.14.7" class="ltx_td ltx_align_center">0.1349</td>
</tr>
<tr id="S4.T3.9.15.15" class="ltx_tr">
<th id="S4.T3.9.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">text-davinci-003</th>
<th id="S4.T3.9.15.15.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">4k</th>
<td id="S4.T3.9.15.15.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.9.15.15.3.1" class="ltx_text ltx_framed_underline">0.3430</span></td>
<td id="S4.T3.9.15.15.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.9.15.15.4.1" class="ltx_text ltx_framed_underline">0.3304</span></td>
<td id="S4.T3.9.15.15.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.9.15.15.5.1" class="ltx_text ltx_font_bold ltx_framed_underline">0.4503</span></td>
<td id="S4.T3.9.15.15.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.9.15.15.6.1" class="ltx_text ltx_framed_underline">0.3577</span></td>
<td id="S4.T3.9.15.15.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.9.15.15.7.1" class="ltx_text ltx_framed_underline">0.4101</span></td>
</tr>
<tr id="S4.T3.9.16.16" class="ltx_tr">
<th id="S4.T3.9.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">gpt-3.5-turbo</th>
<th id="S4.T3.9.16.16.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">4k</th>
<td id="S4.T3.9.16.16.3" class="ltx_td ltx_align_center"><span id="S4.T3.9.16.16.3.1" class="ltx_text ltx_font_bold">0.3340</span></td>
<td id="S4.T3.9.16.16.4" class="ltx_td ltx_align_center">0.3254</td>
<td id="S4.T3.9.16.16.5" class="ltx_td ltx_align_center">0.3226</td>
<td id="S4.T3.9.16.16.6" class="ltx_td ltx_align_center">0.2762</td>
<td id="S4.T3.9.16.16.7" class="ltx_td ltx_align_center">0.3192</td>
</tr>
<tr id="S4.T3.9.17.17" class="ltx_tr">
<th id="S4.T3.9.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">gpt-3.5-turbo-16k-0613</th>
<th id="S4.T3.9.17.17.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">16k</th>
<td id="S4.T3.9.17.17.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.9.17.17.3.1" class="ltx_text ltx_font_bold">0.3027</span></td>
<td id="S4.T3.9.17.17.4" class="ltx_td ltx_align_center ltx_border_bb">0.2264</td>
<td id="S4.T3.9.17.17.5" class="ltx_td ltx_align_center ltx_border_bb">0.1859</td>
<td id="S4.T3.9.17.17.6" class="ltx_td ltx_align_center ltx_border_bb">0.2113</td>
<td id="S4.T3.9.17.17.7" class="ltx_td ltx_align_center ltx_border_bb">0.2251</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">

<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S4.F5.sf1" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="x8.png" id="S4.F5.sf1.g1" class="ltx_graphics ltx_img_landscape" width="761" height="570" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>text-davinci-003</figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S4.F5.sf2" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="x9.png" id="S4.F5.sf2.g1" class="ltx_graphics ltx_img_landscape" width="761" height="570" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Llama-2-70b</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The reward w.r.t. complexity level in HotPotQA. Each bar represents one LAA.</figcaption>
</figure>
<div id="S4.SS5.p2" class="ltx_para ltx_noindent">
<p id="S4.SS5.p2.1" class="ltx_p"><span id="S4.SS5.p2.1.1" class="ltx_text ltx_font_bold">LAA performance w.r.t. Complexity</span>.
Since we have easy, medium, and high level tasks, we compare the performance of Llama-2-70b and regarding different levels of complexity, as illustrated in Figure <a href="#S4.F6" title="Figure 6 ‣ 4.5 Knowledge Reasoning Simulation ‣ 4 Experiment ‣ BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
We observe degrading performance if increasing the complexity of tasks.
In HotPotQA tasks, the hardness is defined as the question answer hops.
Therefore, hard question requires more context understanding and reasoning ability of LAA.
Though OpenAI text-davinci-003 model consistently outperforms Llama-2-70b on all levels of complexity, their difference is of smaller margin in hard questions.
Since hard questions requires more resoning efforts, we can conclude that Llama-2-70b posses comparable reasoning ability with text-davinci-003.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion and Future Work</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">In this paper, we systematically investigate the performances of various LAA architecture paired with different LLM backbones.
We also provide one novel orchestrating method for multiple agents, <span id="S5.p1.1.1" class="ltx_text ltx_font_italic">i.e.</span> BOLAA.
The benchmarking results provide experimental justification for the LAA investigation and verify the potential benefits of BOLAA architecture.
During the investigation, we also identify the challenge of designing BOLAA architecture for environments with compounding actions.
In the future, we will explore whether we can harness LLMs in the controller such that selection and communication with labor agents is also fully autonomous.
We will continue developing more LAA architectures and include more LLMs and environments for evaluations.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chase (2023)</span>
<span class="ltx_bibblock">
Harrison Chase.

</span>
<span class="ltx_bibblock">Langchain.

</span>
<span class="ltx_bibblock"><a href="https://github.com/hwchase17/langchain" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/hwchase17/langchain</a>, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et al. (2022)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.02311</em>, 2022.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2023)</span>
<span class="ltx_bibblock">
Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan
Sun, and Yu Su.

</span>
<span class="ltx_bibblock">Mind2web: Towards a generalist agent for the web.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.06070</em>, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gravitas (2023)</span>
<span class="ltx_bibblock">
Significant Gravitas.

</span>
<span class="ltx_bibblock">Autogpt.

</span>
<span class="ltx_bibblock"><a href="https://github.com/Significant-Gravitas/Auto-GPT" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Significant-Gravitas/Auto-GPT</a>, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gur et al. (2023)</span>
<span class="ltx_bibblock">
Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo,
Douglas Eck, and Aleksandra Faust.

</span>
<span class="ltx_bibblock">A real-world webagent with planning, long context understanding, and
program synthesis.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.12856</em>, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hao et al. (2023)</span>
<span class="ltx_bibblock">
Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and
Zhiting Hu.

</span>
<span class="ltx_bibblock">Reasoning with language model is planning with world model.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.14992</em>, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsieh et al. (2023)</span>
<span class="ltx_bibblock">
Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner,
Chen-Yu Lee, Ranjay Krishna, and Tomas Pfister.

</span>
<span class="ltx_bibblock">Tool documentation enables zero-shot tool-usage with large language
models.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.00675</em>, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2022)</span>
<span class="ltx_bibblock">
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.

</span>
<span class="ltx_bibblock">Language models as zero-shot planners: Extracting actionable
knowledge for embodied agents.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pp. 9118–9147. PMLR, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2023)</span>
<span class="ltx_bibblock">
Geunwoo Kim, Pierre Baldi, and Stephen McAleer.

</span>
<span class="ltx_bibblock">Language models can solve computer tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.17491</em>, 2023.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2018)</span>
<span class="ltx_bibblock">
Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang.

</span>
<span class="ltx_bibblock">Reinforcement learning on web interfaces using workflow-guided
exploration.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1802.08802</em>, 2018.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu,
Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan
Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan
Sun, Minlie Huang, Yuxiao Dong, and Jie Tang.

</span>
<span class="ltx_bibblock">Agentbench: Evaluating llms as agents, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madaan et al. (2023a)</span>
<span class="ltx_bibblock">
Aman Madaan, Alexander Shypula, Uri Alon, Milad Hashemi, Parthasarathy
Ranganathan, Yiming Yang, Graham Neubig, and Amir Yazdanbakhsh.

</span>
<span class="ltx_bibblock">Learning performance-improving code edits.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.07867</em>, 2023a.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madaan et al. (2023b)</span>
<span class="ltx_bibblock">
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah
Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al.

</span>
<span class="ltx_bibblock">Self-refine: Iterative refinement with self-feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.17651</em>, 2023b.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Murthy et al. (2023)</span>
<span class="ltx_bibblock">
Rithesh Murthy, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Le Xue,
Weiran Yao, Yihao Feng, Zeyuan Chen, Akash Gokul, Devansh Arpit, Ran Xu, Phil
Mui, Huan Wang, Caiming Xiong, and Silvio Savarese.

</span>
<span class="ltx_bibblock">Rex: Rapid exploration and exploitation for ai agents, 2023.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakajima (2023)</span>
<span class="ltx_bibblock">
Yohei Nakajima.

</span>
<span class="ltx_bibblock">Babyagi.

</span>
<span class="ltx_bibblock"><a href="https://github.com/yoheinakajima/babyagi" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/yoheinakajima/babyagi</a>, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakano et al. (2021)</span>
<span class="ltx_bibblock">
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina
Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders,
et al.

</span>
<span class="ltx_bibblock">Webgpt: Browser-assisted question-answering with human feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.09332</em>, 2021.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. (2023)</span>
<span class="ltx_bibblock">
Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy
Liang, and Michael S Bernstein.

</span>
<span class="ltx_bibblock">Generative agents: Interactive simulacra of human behavior.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.03442</em>, 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patil et al. (2023)</span>
<span class="ltx_bibblock">
Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez.

</span>
<span class="ltx_bibblock">Gorilla: Large language model connected with massive apis.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.15334</em>, 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paul et al. (2023)</span>
<span class="ltx_bibblock">
Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine
Bosselut, Robert West, and Boi Faltings.

</span>
<span class="ltx_bibblock">Refiner: Reasoning feedback on intermediate representations.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.01904</em>, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al. (2023)</span>
<span class="ltx_bibblock">
Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin,
Xin Cong, Xiangru Tang, Bill Qian, et al.

</span>
<span class="ltx_bibblock">Toolllm: Facilitating large language models to master 16000+
real-world apis.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.16789</em>, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2023)</span>
<span class="ltx_bibblock">
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting
Zhuang.

</span>
<span class="ltx_bibblock">Hugginggpt: Solving ai tasks with chatgpt and its friends in
huggingface.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.17580</em>, 2023.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2017)</span>
<span class="ltx_bibblock">
Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang.

</span>
<span class="ltx_bibblock">World of bits: An open-domain platform for web-based agents.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pp. 3135–3144. PMLR, 2017.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shinn et al. (2023)</span>
<span class="ltx_bibblock">
Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan,
and Shunyu Yao.

</span>
<span class="ltx_bibblock">Reflexion: Language agents with verbal reinforcement learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.11366</em>, 2023.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori et al. (2023)</span>
<span class="ltx_bibblock">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
Guestrin, Percy Liang, and Tatsunori B. Hashimoto.

</span>
<span class="ltx_bibblock">Stanford alpaca: An instruction-following llama model.

</span>
<span class="ltx_bibblock"><a href="https://github.com/tatsu-lab/stanford_alpaca" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/tatsu-lab/stanford_alpaca</a>, 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team (2023)</span>
<span class="ltx_bibblock">
MosaicML NLP Team.

</span>
<span class="ltx_bibblock">Introducing mpt-7b: A new standard for open-source, commercially
usable llms, 2023.

</span>
<span class="ltx_bibblock">URL <a href="www.mosaicml.com/blog/mpt-7b" title="" class="ltx_ref ltx_url ltx_font_typewriter">www.mosaicml.com/blog/mpt-7b</a>.

</span>
<span class="ltx_bibblock">Accessed: 2023-05-05.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar
Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux,
Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier
Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan
Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang,
Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan
Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien
Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and
Denny Zhou.

</span>
<span class="ltx_bibblock">Chain of thought prompting elicits reasoning in large language
models.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.11903</em>, 2022.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2023)</span>
<span class="ltx_bibblock">
Binfeng Xu, Zhiyuan Peng, Bowen Lei, Subhabrata Mukherjee, Yuchen Liu, and
Dongkuan Xu.

</span>
<span class="ltx_bibblock">Rewoo: Decoupling reasoning from observations for efficient augmented
language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.18323</em>, 2023.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018)</span>
<span class="ltx_bibblock">
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan
Salakhutdinov, and Christopher D. Manning.

</span>
<span class="ltx_bibblock">HotpotQA: A dataset for diverse, explainable multi-hop question
answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Conference on Empirical Methods in Natural Language
Processing (EMNLP)</em>, 2018.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. (2023a)</span>
<span class="ltx_bibblock">
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
and Yuan Cao.

</span>
<span class="ltx_bibblock">ReAct: Synergizing reasoning and acting in language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations
(ICLR)</em>, 2023a.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. (preprint)</span>
<span class="ltx_bibblock">
Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.

</span>
<span class="ltx_bibblock">Webshop: Towards scalable real-world web interaction with grounded
language agents.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, preprint.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. (2023b)</span>
<span class="ltx_bibblock">
Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng,
Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, Ran Xu,
Phil Mui, Huan Wang, Caiming Xiong, and Silvio Savarese.

</span>
<span class="ltx_bibblock">Retroformer: Retrospective large language agents with policy gradient
optimization, 2023b.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Jianguo Zhang, Kun Qian, Zhiwei Liu, Shelby Heinecke, Rui Meng, Ye Liu, Zhou
Yu, Huan Wang, Silvio Savarese, and Caiming Xiong.

</span>
<span class="ltx_bibblock">Dialogstudio: Towards richest and most diverse unified dataset
collection for conversational ai, 2023.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2023)</span>
<span class="ltx_bibblock">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E.
Gonzalez, and Ion Stoica.

</span>
<span class="ltx_bibblock">Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2023)</span>
<span class="ltx_bibblock">
Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar,
Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al.

</span>
<span class="ltx_bibblock">Webarena: A realistic web environment for building autonomous agents.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.13854</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a href="https://webarena.dev" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://webarena.dev</a>.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Aug 14 22:36:03 2023 by <a href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"></a>
</div></footer>
</div>
</body>
</html>
